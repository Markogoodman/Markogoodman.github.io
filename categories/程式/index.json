[{"content":"Bloom filter Bloom filter 是一個概念很簡單又好用的東西，之前看過就想記錄一下\n為何要用 Bloom filter 可以快速確認一個 key 是否存在一個資料集內，且不需花太多空間來儲存這些多餘的資訊。\nHash table 也可以做到快速查詢，但空間使用量就是會多不少\n演算法 一個 Bloom filter 是一個長度為 m 的 bit 陣列，配上 k 個 hash function。每個 hash function 可以隨機的把 key 對應到 m 格陣列上。\n加入資料 當要把一個 key 加入資料集時，就用 k 個 hash function 對這個 key 做計算，把對應的陣列位置設成 1\n查找資料 當要確認一個 key 是否在資料集內時，也是用 k 個 hash function 對這個 key 做計算。\n如果陣列對應的位置所有的值都是 1，這個 key 就有\u0026quot;可能\u0026quot;存在此資料集，就可以進行搜尋，因為之前加進來的 key 們也有機會把該 key 對應到的 k 個陣列位置設為 1。\n如果其中某些位置是 0，則 key 完全不可能存在此資料集。\n應用場景 Cassandra DB 就有使用到 Bloom filter。\nCassandra 用的是類似 log structured merge tree 的結構，內部會有多個樹，資料可能存在任何地方，要找一筆資料時如果每顆樹都要搜尋一定會慢，所以每顆樹都會配上一個 bloom filter 來加速查詢，決定要不要搜尋這顆樹。\n結論 因為 Bloom filter 只會出現 false positive 的情況 (判斷資料存在卻不存在)，不會有 false negative (判斷資料不存在卻存在)，所以不會有正確性的問題。出現 false positive 時只是進去搜尋沒搜尋到而已。\n要達到 1% 的 false positive rate 只需要大約 10 bits/element，真的是超級節省D\n詳情請參考: https://en.wikipedia.org/wiki/Bloom_filter\n","description":"Blooooooom","id":0,"section":"posts","tags":[],"title":"Bloom Filter","uri":"https://markogoodman.github.io/posts/bloom-filter/"},{"content":"How to solve spark large amout of small files problem It\u0026rsquo;s a technical issue I solved when working.\nProblen Description In our data pipeline, an hourly spark job writes data to AWS S3.\nThe data and directory structure looks like this:\n/job_name_start_time/event_timestamp_minute/data_1\r/job_name_start_time/event_timestamp_minute/data_2\r... At time T the job will process all the events generated between T-1hour ~ T.\nFor example, an event generated by job JobX at 2020-03-04 10:30AM will probably be stored in a file at /JobX_2020-03-04-11:00/2020-03-04-10:30/data_n.\nSounds good? But events never arrive on time.\nUsually in one batch of data there are something like\n/JobX_2020-03-04-11:00/2020-03-04-9:58/data_x\nor even\n/JobX_2020-03-04-11:00/2020-03-03-23:58/data_x.\nUpstream data delay is inevitable.\nBut most of the events in batch JobX_2020-03-04-11:00 are generated between 10:00 and 11:00, others are minorities\nAnd we found that file sizes are skewed,\nfor batch JobX_2020-03-04-11:00 the distribution looks like\n/2020-03-04-10:30/data_1 - 200MB\r/2020-03-04-10:30/data_2 - 200MB\r/2020-03-04-10:30/data_3 - 200MB\r... /2020-03-04-9:30/data_1 - 1MB\r/2020-03-04-9:30/data_2 - 1MB\r/2020-03-04-9:30/data_3 - 1MB\r... But ideally each file should be roughly the same size\n/2020-03-04-10:30/data_1 - 201MB\r/2020-03-04-10:30/data_2 - 201MB\r/2020-03-04-10:30/data_3 - 201MB\r... /2020-03-04-9:30/data_1 - 201MB\r... This was the problem we were facing, large amount of small files in 2020-03-04-9:30 directory is probably decreasing reading performance.\nThough tweaking with repartiion to solve this problem might increase the writing time, we still wanted to give it a try.\nBackground information We used to have this piece of code to handle repartitioning and writting\ndata.repartition(config.repartition_num)\rdata.partitionBy(batch_id, minute(event_time)) // batch_id = job_name_start_time\r... Let\u0026rsquo;s first introduce how spark repartition and partitionedBy work.\npartitionBy partitionBy allows you to organize the physical directory structure when writing to it.\nFor example, partitionBy(col1, col2) will make your structure look like\n/col1=A/col2=X/data...\r/col1=A/col2=Y/data...\r/col1=B/col2=X/data... More detail: https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/\nrepartition repartition allows you to regroup data into partitions in memory, data in one partition will be processed together, increase partition number to parallelly process each partition if you have more machines.\ndata.repartition(n) \u0026lt;- repartitions data randomly to create n partitions\rdata.repartition(col) \u0026lt;- repartitions data with col to create 1 partition for each col value Normally when you write, number of partitions is the number of files you have.\nMore detail: https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/\nThe abobe seems good, but if the data in one partition is splitted by partitionBy, it will still generate multiple files in different directories.\nSince we used to partitioned data randomly, each partition contained a lot of T-1 ~ T dat and a little bit of everything else like T-5 ~ T6 or T-31 ~ T-30. When lots of partitions like this are written to disk, it creates lots of seprarte small files for data other than T-1 ~ T(lots of large files for T-1~T)\nSo the target was to repartition data to have data with the same batch_id, minute(event_time) in the same partition before writing to disk, and if a partition is too large it also needs to be splitted (hopefully 1 file is 1GB)\nSolution The solution is quite simple. Let\u0026rsquo;s say batch_id, minute(event_time) = partition_key, now we need to find a way to put rows with the same partition_key in the same partition with each partition size controlled.\nFirst we estimate the size of each row by\nrow_size = length(to_json(struct(col(\u0026quot;*\u0026quot;)))),\nthen we can get how many partitions each partition_key needs by doing\npartition_key_total_size = groupBy(partition_key).agg(sum(col(row_size))),\nand\npartition_number = ceil(col(partition_key_total_size)/ max_size_per_partition_key).\nNow we can join the information back to the data with broadcast since split info is probably quite small.\ndf.join(broadcast(split_info), partition_key)\nNow we have dataframe like this\nif we set the max_size_per_partition_key to 10\n+-------------------------------------------------------\r|partition_key|partition_key_total_size|partition_number\r|-------------------------------------------------------\r| x| 10| 1\r| y| 20| 2\r| y| 20| 2\r| z| 30| 3\r| z| 30| 3\r| z| 30| 3 For partition_key = z, we want there to be 3 partitions (0~2).\nAt first I thought we could just evenly distribute the rows into each partition. For example, go through rows and assigning them partition = row_number % 3, but that requires the data to be processed in the same executor (to label the row_number).\nA better approach is to randomly assign each row to partition 0, 1 or 2. When the number of rows is large, it should be quite evenly distributed.\nThen we add another column to indicate which\n.withColumn(\u0026#34;finalDecision\u0026#34;, concat(\rcol(partition_key), lit(\u0026#34;final_partition\u0026#34;),\r(rand()*col(\u0026#34;partition_number\u0026#34;)).cast(\u0026#34;int\u0026#34;)\r)` Finally we can do\ndf.repartition(col(\u0026quot;finalDecision\u0026quot;))\nto repartition the data and have size controled.\nAnd the output file should be what we expected, no large amount of small files.\nAs the implementation completed, we found that compacting the files reduceed data processing time around 3% (slightly increase the writing time, but decrease the data reading)\n","description":"A problem that I had when working and solved","id":1,"section":"posts","tags":["Spark"],"title":"How to solve spark large amount of small files problem","uri":"https://markogoodman.github.io/posts/spark-small-file-problem/"},{"content":"Parquet 最近在工作上要把一堆 spark app 讀的檔案從 json 換成 parquet\n就小小研究一下這東西到底是什麼\nparquet 其實就是一種資料儲存的檔案格式，且是 binary encoding，人眼看不懂\n像在之前我寫的這篇文章介紹，用純文字的方式去存 json 這種 key value 的 object，在 deserialize 的時候要花很大的力氣一個一個字元去讀，而且處存空間花費也很大，沒有做任何的壓縮\nparquet 用在一個 row group 內用 column-oriented 的方式去存資料，還有多存了一些 metadata，在分析用途且資料量大的時候就會很有效率\nparquet 檔案長相如下\nRef: https://parquet.apache.org/docs/file-format/\n檔案內主要包含 data 和 footer file metadata 兩個部分。\n一筆一筆的紀錄會被分散放在各個 row group 內，接著 group 內會有很多 column chunks，每個 chunk 都包著多筆紀錄的同一個欄位，切著被切分為 pages，每一個 page 基本上來說是不可分割的，是拿來壓縮和 encoding 的單位。\nRef: https://www.linkedin.com/pulse/all-you-need-know-parquet-file-structure-depth-rohan-karanjawala/\n在可以看到 footer 中的 file metadata 包含了 row groups 和 column chunks 的位置，所以這些東西其實是可以分開存在不同地方。\n把 metadata 寫在尾端的好處是可以 one pass 寫入\n在讀資料的時候都是由 footer 開始，先找到需要讀的 column chunks 接著依序去讀\n大量資料分析時通常都是讀所有資料的特定幾個 column 而已，column-oriented 資料編排方式可以跳過不需要的 column，如果是一般 row-oriented 的處存方式例如 Avro，就只能乖乖掃過所有資料\n而因為同一個 column 都是同一種型態，同一個型態的資料都聚集在一起，所以在壓縮上也比 row-oriented 更容易\n舉個簡單的例子，目前儲存了一個 enum 的 column 內容是 AAAAABBBB，那就可以把它壓縮成 5A4B 來儲存\nDDIA 第三章有一個小節專門在講 column-oriented storage\n之前如果有空再來寫一篇紀錄一下\nRow group 與 data page size 都是可以調整的\nRow group size 越大就可以讓 column chunks 更大，可以做更大的 sequential IO，不過在途中也會需要更多的 buffer。官方建議就是把一個 row group 剛好可以放進一個 HDFS block，1GB\ndata page size 的建議則是 8KB，越大的 size 可以減少 header 的數量與處理 header 的時間\n小一點的 size 則是可以更有效率的 query 少量紀錄，因為在撈少量紀錄時不需要去處理很大的 page\nParquet 大概就看到這邊，總結就是它用 column-oriented 的方式去存資料，所以很適合資料分析，而且又存了很多 metadata 來讓各種操作減少很多 overhead (例如 index)\n跟 Parquet 常常被一起提起的還有 ORC，ORC 似乎在某些方面表現稍微優於 parquet，不過 Parquet 在 Spark 上有做一些優化，而 ORC 則是適合跟 Hive 一起使用。但這部分就沒什麼研究了，之後有空再來看看好了。\nRef:\nhttps://parquet.apache.org/docs/file-format/configurations/\nhttps://www.linkedin.com/pulse/all-you-need-know-parquet-file-structure-depth-rohan-karanjawala/\n","description":"parqqqquet","id":2,"section":"posts","tags":["parquet"],"title":"Parquet","uri":"https://markogoodman.github.io/posts/parquet/"},{"content":"看了 DDIA 第九章 記錄一下\n2 phase commit 是用來做 multiple nodes tansaction 時使用的方法\nsingle node transaction 通常都是由 log 與 commit 來達成\npostgresql.org/docs/current/wal-intro.html\n例如 postgres 的做法就是每個操作都會有一個 write-ahead-log (wal)，接著會把資料操作寫進 buffer (記憶體內，速度快)，在 commit (寫入 commit record) 之前必須把所有的 wal 都寫入 persistent storage，每隔一陣子資料庫會一次把所有 buffer 內的更動一次寫進 persistent storage\nwal 是 sequential write，效率會比每次都把資料寫進 persistent storage 高很多\ncommit 之後系統如果 crash，也可以用這些已經寫入的 wal 來做恢復\n決定這個 transaction 是成功或失敗的因素就是 commit record 在系統 crash 之前是否有被寫入 persistent storage\n平時想到 transaction 只會想到是一次更改多筆資料的情況，但其實更改一個物件時也會更改到很多硬碟上的位置（例如有 secondary index），也是有 transaction 的蓋念\n單一節點 tansaction 通常是由 db engine 來處理\nMultiple nodes transaction 如果只是單純的把它當作每個 db node 上都有一個 transaction 來實作，client 可以在完成時一次發送很多 commit 到各個 node 上，但很有可能某些成功某些失敗，造成資料不一致\n如此一來就必須將某些 node 上已經 commit 的 transaction 回朔，但這些 transaction 被 commit 之後就可以被其他 transaction 看見，會有更多 transaction 依賴在這個 commit 上，會出現很多問題\n以下是 2 phase commit 的流程圖\n圖片來自 DDIA figure 9-9\ncoordinator 是在 sindle node transaction 內沒有出現的東西，通常都會實作在發 request 的 client library 內，但也可以額外用一個服務來擔任這個角色\n以下是比較詳細的 2 pc 流程\n首先 coordinator 會在每個 db node 上 init 一個 transaction\n當所有所有資料操作結束後會進入 phase 1\ncoordinator 發送 prepare 給每個 node\n只要有任何一個 node 回覆 no 或是 timeout，則在 phase 2 時 coordinator 會請所有 node abort 各自的 transaction\n如果所有 node 皆回覆 yes，則在 phase 2 時 coordinator 會請所有 node commit\n這邊有兩個 decision points\n在 phase 1 時如果一個 node 回覆 yes，代表這個 node 已經自行先檢查所有意外情況（例如 unique constrain conflict 之類），此 node 無論如何一定可以 commit，放棄了自行 abort 的權利，必須等待 phase 2 coordinator 的通知才能進行下一步動作 (在回覆 yes 前都可以自行 abort)\n當 coordinator 在 phase 1 做了決定（commit or abort）後，會將這個決定寫入 log，成功後這個決定便不會被更改（single node 是把這兩個動作合在一起 -\u0026gt; commit record）\n在 phase 2 時如果有 coordinator 和 node 之間溝通失敗如 timeout 或有 node crash，則會無限次重試直到成功\n而 coordinator 如果 crash 了，回答 yes 的 node 也必須要無限時的等待 coordinator 來通知自己接下來到底是要 abort 還是 commit\nnode 若是在收到通知前擅自 commit，則可能會因為其他參與者在之前回答了 no，其實 coordinator 的決定是 abort\n反之，若 node 在收到通知前擅自 abort，其他參與者很有可能已經收到 coordinator 的 commit 決定了\n可以看到這個 protocol 中有可能因為 coordinator crash 造成所有 node 都被卡住，所以 2 pc 被稱為 blocking protocol\n有人提出了 3pc 要做到 non blocking protocol，而實務上是不太可能做到的，因為它需要一個可以準確得知某個 node 是否 crash 的方法，但網路通訊是 unbounded delay 所以很有可能是 timeout，node 卻活著的情況\n以上就是 2pc 的流程\n重點大概就是在兩個 decision points 那邊是以前網路看文章比較沒看到的部分\n","description":"2 phase commit from DDIA","id":3,"section":"posts","tags":["2 phase commit"],"title":"2 Phase Commit","uri":"https://markogoodman.github.io/posts/2-phase-commit/"},{"content":"DynamoDB index 這篇想講一下用 DynamoDB 遇到的狀況，字很多但內容應該很少，比較像流水帳\n前陣子工作時需要將 DynamoDB 的全部資料(包括已經存在的和未來新來的)載入進 Redshift\n並在途中做一些資料處理，資料處理時需要使用到 DynamoDB 和 PostgreSQL 內的東西\nDynamoDB 內的裝的是聊天的訊息，都有 partition key (chat room id) 和 sort key (unique timestamp in a chat room)\n一開始想了兩種方法\n第一種方法很簡單，就是 Batch processing，類似 cursor based pagination\n就是每過一段時間去 DynamoDB 從 offset (cursor，可以是 partition key + sort key 的組合) 開始抓以後的資料來處理，處理成功就丟進 Redshift 並更新 offset\n實作上看似很簡單，且不太需要處理失敗的情況，因為途中失敗就不要更新 offset 就好\n當時想出來的另一種方法是比較複雜，如下圖，是 stream processing\n要用的 service 多很多，且 service 之間溝通都要處理失敗的問題\n最後還是選了這個來用，不過這不是這篇的重點，下次有機會再來把這裡面的東西寫清楚\n當時其實時間只有一個禮拜左右，且現在資料量不多，Redshift 內的資料是要做分析報表的，需求沒有那麼及時，就打算用第一種方法來解決\n結果剛要開始做的時候就發現 DynamoDB 的 index 好像沒什麼辦法支援這種設計\n簡介一下 DynamoDB\n資料庫內每一筆資料叫做 item，裡面有很多 attribute，就像是 RDB 的 row 和 column，不過每個 item 可以有不同 attribute (NoSQL)\nPrimary key 是在一個 table 中是 unique 的，由 partition key 或是 partition key + sort key 組成\n在同一個 partition key (hash table) 中可以用 sort key (b-tree)讓資料有順序\n到這邊就會發現 DynamoDB 的 query 是兩層，第一層是 hash，第二層是 b-tree，所以想要在我們的 table 內只用一個 cursor 來當作 offset 是不可能的，因為 hash table 沒有順序阿，還是必須掃過每個 entry\n此時又想到一個 tricky 的方法，是建立一個 Global secondary index 然後把所有資料都塞到同一個 partition key 內，這樣就可以用 sort key 來當 cursor，不過查一查又發現 partition key 對 scalability 有超大的影響\nAWS 會把相同 partitio key 下面的資料存在物理上很接近的位置，可以想像成不同 partition key 的資料會存在不同 disk 上，對單一 partition key 的 read 和 write 會有 throughput 的上限，只要把資料平均分散到不同 partition key，幾乎是沒有 scalability 的問題，可以參考下方連結\nhttps://docs.aws.amazon.com/amazonDynamoDB/latest/developerguide/bp-partition-key-design.html\n如果把每個 partition key (chat room id) 都各存一個 cursor (sort key) 呢?\n會有不少 overhead，因為很多其實 partition key 其實都幾乎沒再用，但也不行刪除他，因為我們也無法確定是否會再次啟用。如此一來每次 batch process 啟動時都要去掃過他們造成 overhead\n總之就是想了很多方法，還是找不到一個適合的 index 存法來有效支援第一種方法，或許就是 DynamoDB Partition key、Sort key、index 設計造成的限制，不過這些設計也讓他很容易 scale，算是一種 trade off\n結果最後還是跑去用 DynamoDB stream 了\nNOTE Global Secondary Indexes 可以用另一組 partition key 和 sort key 來建 index\n但 PK + SK 不需要 unique\n只支援 eventual consistency\nLocal Secondary Indexes 依照原本 table 的 partition key，用另一個 sort key 來建 index\n支援 consistent consistency\n但官方不建議用這個東西就是了\n建立多組 index 會讓花費增加，尤其是寫入時會是兩倍 cost (delete, write)\n下面再推薦幾篇 DynamoDB 滿好懂得文章\nDynamoDB 計費\nhttps://medium.com/fcamels-notes/amazon-DynamoDB-%E7%9A%84-consumed-units-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A0%85-e1fbda5687b4\nhttps://medium.com/fcamels-notes/amazon-DynamoDB-%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97-c734d8445ae2\nDynamoDB under the hood\nhttps://www.youtube.com/watch?v=yvBR71D0nAQ\nhttps://www.precisely.com/blog/big-data/big-data-101-batch-stream-processing\n","description":"DynamoDB 流水帳","id":4,"section":"posts","tags":["DynamoDB","index","database"],"title":"DynamoDB Index","uri":"https://markogoodman.github.io/posts/dynamodb-index/"},{"content":"如何把原本使用 AWS console 建立的資源完整搬到 cdk 內 目前公司使用 AWS 上是手動與 cdk 混著用，早期的東西都是用手動建立的，例如 Dynamodb table，裡面的 index 等等。\n但後來很多東西都陸續寫在 cdk 內，不得不說比起手動建立還是方便又安全多了，要修改和架設測試環境時只要改個環境變數或是 region 就直接部署上去，所以還是滿希望把所有東西通通都移到 cdk 上面的\n最近剛好也要使用 Dynamodb 的 stream 功能（原本在 table 上是關閉的），就想說一起搬進去，這幾天就開始了瘋狂看文件的生活，不得不說 cdk 真的是剛出的東西，stack overflow 上面能抄的東西還真少 XD\n載入現有 table from_table 最常見的直接解法就是在 cdk 中用 from_table_attributes, from_table_arn, from_table 直接載入現有的 table\n不過這種方法其實沒辦法完全控制住這個資源\n例如 from_table_attributes 就必須傳進 stream 的 arn 才行，實際上還是要去 console 手動開關那些東西\n讓人強迫症發作\n用 cdk 建立新的 Dynamodb Table 並把舊的資料移進去 也是可行的做法\n不過問題是需要把伺服器關掉一陣子，或著是會讓伺服器需要同時連新舊兩個 db，migration 過程其實滿煩瑣的\n用 cloudformation 搬移 這是研究了一陣子發現最方便可行的做法\n可以讓 cdk code 裡面看起來就像是從這邊寫出來的資源，有完全的控制權力，而且用不用 db downtime\n做法 其實滿簡單的\n先 cdk deploy 一次，確保 cdk code 內容與 aws 上的 stack 內容一致\n去 aws cloudformation 頁面上的 stack 中將 template 複製一份備用 (A)，這份內容應該不包含你要 import 的資源\n接著去 cdk 內加入新的 code，這邊會建議設定的跟原本存在 aws 上的資源完全一樣，如果有一點不同步的話，之後要改可能會出問題\n使用 cdk synth 產生 cloudformation，去 cdk.out 裡面找到該 template (B)\n此時 template B 會多出一些東西，包括 \u0026lsquo;新資源的改動\u0026rsquo; 以及 \u0026lsquo;其他改動\u0026rsquo;(CDKMetadata之類的，應該是 cdk 自己產生的一些版本號)，可以用 vscode diff 之類的來查看，要把 B 內除了新資源以外的變動用 A 的覆蓋掉\n到 aws cloudformation 頁面內選擇該 stack，並在右上角 action 裡面選擇 import resource，上傳第 5 步驟修改後的 template，這時 aws 會自動偵測到 template 內有些資源是目前不在 aws stack 上的，就可以手動將資源填進去，然後就完成拉！\n最後可以在 cloudformation 右上角按 detect drifts，然後 view drifts，會自動檢查 cdk 產生出來的東西是否與原本上面的資源一致。\n我在搬移時就有遇過 cdk 內和 aws 上的值不同，要把 cdk 內的值改過去並且 deploy 時，出現 \u0026lsquo;不可以用同一個 value 更新 resource\u0026rsquo; 這種錯誤，所以建議還是在測試環境多確認幾次 cdk 與 aws 資源一致，沒有 drifts ，才在 production 上做 import 會比較安全\n","description":"如題啊","id":5,"section":"posts","tags":[],"title":"AWS，如何把原本使用 AWS console 建立的資源完整搬到 cdk 內","uri":"https://markogoodman.github.io/posts/aws-migrate-from-existing-resource-to-cdk/"},{"content":"Redis 裡面有一種 data type 叫做 sorted set\n簡單來說就是一堆 key:value，但這些資料會依照 value (score) 來排序\n讓使用者可以快速取得某個 key 的排名，或是一個排名範圍(或是分數範圍)內的 key 們，等等的功能\n詳細可以參考這個～\nhttps://www.tutorialspoint.com/redis/redis_sorted_sets.htm\n會寫這篇是因為看到這東西的實作比較特殊，想找找看這實作背後的原因\nRedis sorted set 內部結構 當資料量小時，sorted set 使用的是 ziplist 超過一定量時則是使用 hash map + skip list ziplist 以後有空再寫 http://zhangtielei.com/posts/blog-redis-ziplist.html\nskip list 怎麼運作的也不特別講，維基百科或 google 看一下很好查到（https://en.wikipedia.org/wiki/Skip_list）\n為什麼 Redis sorted set 使用 skip list 今天主要想看的是第二種實作的方法，一開始我看到這個實作，想說一般資料庫 range query 不都是靠 index 嗎(B+Tree、BTree)？\nRedis 怎麼搞特殊跑來用 skip list XD\n想要了解一下他這個選擇背後的想法\n搜了搜看到這篇文 https://news.ycombinator.com/item?id=1171423\n作者給了三個理由\n以下會拿 BTree 和 B+Tree 來比較一下，對這三個理由研究一下，不然有些乍看之下還滿難懂的\nThey are not very memory intensive. It\u0026rsquo;s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. 第一個理由是關於記憶體用量的，skip list 相比 balance tree 來說記憶體用量是可以比較小的，印象中如果 p 用 1/4，平均是每個 node 會有 4/3 左右個 pointer 而已。\n因為 Redis 是 in-memory 運作，節省記憶體這點可能還滿重要的，畢竟 memory 貴貴。\nBtree 如果是每個 node 只包含一筆資料，那麼就是會有 2 個 pointer。\n如果單一 node 有多筆資料，pointer 數量會變少沒錯，但問題是同一 node 的資料必須緊密排在一起，常常會有預留的空間，會浪費很多記憶體。\n如果用的是 B+Tree 的話，只有 leaf nodes 有存資料，那 internal nodes 記憶體也是花很兇。\n詳情請參考 https://en.wikipedia.org/wiki/B-tree\nA sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. 而這句話翻譯下來就是：使用者大多是為了 range query 來用 sorted set 的，range query 時 skip list 的 cache locality 至少會跟其他平衡術一樣好 (不得不說這一句越看越不懂。\n首先 skip list 就跟 linked list 一樣，在 cache locality 上是比 array 差的，因為資料不連續，cpu 讀資料時常常會把附近的資料一起讀進 cache，而 linked list 資料散佈在記憶體各處，cpu 比較無法把相關資料一次塞到 cache 上。\n如果是一個 node 只有一筆資料的話，BTree 的 cache locality 確實與 linked list 類似。\n不過 BTree 和 B+Tree 如果單一 node 有多筆資料的話 cache locality 應該會比較好才對（？\n有人知道請告訴我 XD\n以我的知識看起來在 cache locality 表現上應該是 BTrees 家族 \u0026gt;= skip list\nBtree 家族會被廣泛用在 database index 上也是因為這個資料聚集的特性，使得在硬碟 io 次數可以減少很多。\n(不過又有在網路上查到一些 cache friendly 的 skip list，搞得我很亂)\nThey are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 這個就比較好理解了\nBTree 在 Treverse，可能會需要 stack 或是 parent pointer 的幫忙，要上上下下的跑，而 B+Tree 的話則是 Traverse 很方便，但跟 BTree 一樣在插入刪除修改時都可能要 split merge 之類的\n而 skip list 的操作就是一個 linked list 操作，幾行就結束了。\nSummary 目前看下來是第一個與第三個理由比較能說服我，第二個理由則是自己想覺得不太合理，網路上也沒什麼可以支持這個說法的相關資訊，作者也沒提供 benchmark，所以暫時保留。\n不過我們可以從這些理由看出作者在選用 skip list 時是怎麼去思考的。\nRedis 和其他 database 最大的差別是它是 in-memory 的，而其他用 BTree 家族的資料庫大多是存在硬碟上，考量的點就會不同。\n如果 skip list 在 range query 時每個 node 都要去硬碟上撈一次應該會慢到天上去，但在記憶體內就差距不會太大。\n如果 BTree 直接用在 memory 內又會消耗太多空間（前面提過的浪費與 pointer 數量），但用在硬碟上就比較沒問題，因為硬碟相對大又便宜。\n總之就是一個 trade off。\n","description":"Sorted set 使用 skip list 的一點看法","id":6,"section":"posts","tags":["Redis","Sorted set"],"title":"Redis Sorted Set","uri":"https://markogoodman.github.io/posts/redis-sorted-set/"},{"content":"4 way handshake 在 tcp 連線要結束時則是需要四次握手\nＡ -\u0026gt; Ｂ FIN\nＢ -\u0026gt; Ａ ACK\nＢ -\u0026gt; Ａ FIN\nＡ -\u0026gt; Ｂ ACK\n當 A 想要結束連線時就傳送 FIN 給 B，B 先確認 FIN 之前的資料都收到之後回傳 ACK 給 A 確認。\n接著反向做一次，整個連線結束\n第一個步驟 A 傳 FIN 給 B 時，要關閉的是 A -\u0026gt; B 的傳輸路線（第三步就是反過來）\nTCP 會跟應用程式說 對方關閉傳輸了（有error eof之類的）\n而什麼時候要傳 FIN 給對方是由自己 application 來決定的，所以如果 application 反應很快其實也可以把 2.3 步驟合在一起變成三次握手，不過通常不會這樣就是了\n","description":"握手手","id":7,"section":"posts","tags":["HTTP"],"title":"Http 4 Way Handshake","uri":"https://markogoodman.github.io/posts/http-4-way-handshake/"},{"content":"protobuf vs json 最近工作新專案要把原有的 http request 和 response 都從 json 改為 protobuf\n預期是可以讓網路傳輸量還有 (de)serialize 加速～\n所以這邊想研究一下 protobuf 到底是做了什麼才比 json 快\njson 基本上 json 就是一串字\n例如我們操作的 object:\n{\r\u0026#34;key\u0026#34;: \u0026#34;value\u0026#34;\r} 轉成字串和bytes就是這樣\n{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;} 7b 22 6b 65 79 22 3a 22 76 61 6c 75 65 22 7d 傳輸的時候就是把這串 byte 傳過去，包括 {} \u0026quot;\u0026quot; 都算，大約是 15 bytes\n而且 deserialize 的時候都是要掃過那個字串的每一個字，判斷每個字是符號還是數字等等，才能轉換成程式語言裡面的 object，很耗時\nprotobuf 使用 protobuf 來溝通前 client server 要先定義好溝通的格式 .proto 檔案\n例如\nsyntax = \u0026#34;proto3\u0026#34;;\rmessage Data { string key = 1;\r} client 和 server 都會使用這個檔案來產生對應的語言的資料結構\n像是 Go 會產生\nxxx.pb.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Data struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields Key string `protobuf:\u0026#34;bytes,1,opt,name=key,proto3\u0026#34;` } func (x *Data) GetKey() string { if x != nil { return x.Key } return \u0026#34;\u0026#34; } 當收到一串 byte 時，使用 proto.Unmarshal 就可以把 byte 轉回 struct 了\nWhy protouf is more efficient than json .proto 檔案每個 field 後面都會有個數字，在傳輸時會替代 field name\n到達目的地時對方在使用先前定義好的 proto 來查詢數字對應的 field name，可以省下很多空間\n除此 field name 的替換外，protobuf 在內容上也會有編排過讓傳輸與 parse 更有效率\n以下舉個例子\n例如傳送 string 時就會加上長度的資訊\n我們用上面的 .proto 為格式傳送了\n{\r\u0026#34;key\u0026#34;: \u0026#34;abc\u0026#34;\r} 那實際上傳出去的 bytes 會是\n09 03 61 62 63 第一個 byte 0x09\n可以拆解成 00001 010\n前 5 個 bits 00001 代表的是 field name number\n後 3 個 bits 010 代表的是 string type\n原先的 \u0026ldquo;key\u0026rdquo; 卻要佔用 5 個 bytes，這邊只要 1 個而已\n第二個 byte 0x03 代表了長度\n後面接著 61 62 62 就是內容\n如此一來在 deserialize 時就可以直接以 data[i:i+3] 這種方式抓出資訊\n不用像 json 一樣每個字元都判斷，當長度很長時差距會很大\n再舉個傳輸 integer(Varint) 的例子\n如果數字是 300 的話，實際上傳送的資料會是\n1010 1100 0000 0010\n每個 byte 的 most significant bit 用來代表下一個 byte 還是不是資料，如果是 1 的話代表下一個 byte 也要一起計算\n將第一個 bit 去掉之後然後兩個 byte reverse (因為儲存方式是 least significant group first)\n_000 0010 _010 1100 就可以得到 300 了\n而 json 在傳送時每個字元都代表了一個 byte，在數字大的時候資料量差距也很大\n總結與優缺點 前面舉了一些例子來說明 protobuf 的設計是如何讓它更有效率\nfield name number 如何省資料量\nstring 如何讓 parse 更快\ninteger 如何省資料量\n還有省掉了符號 {} , : 的資料量\n應該還有其他的但就不一一列舉\n總之使用 protobuf 對我們的程式處理資料都會有效率很多\n而資料內容在處理前是人類不可讀的（有好有壞）\n目前從使用 json 轉換到 protobuf 後感覺多了一些開發成本，畢竟前後端都習慣用 json 來做事情，多了一個東西要管，大家都要熟悉才行\n如果開發的東西規模不大，資料量不大的話還是從 json 開始使用避免額外成本過高\nRef: https://developers.google.com/protocol-buffers/docs/encoding\n","description":"protobuf 怎麼那麼快","id":8,"section":"posts","tags":["protobuf"],"title":"Protobuf and Json","uri":"https://markogoodman.github.io/posts/protobuf-and-json/"},{"content":"How channel works Note of\nhttps://www.youtube.com/watch?v=KBZlN0izeiY\u0026amp;ab_channel=GopherAcademy\nby Kavya Joshi.\nChannel ！！！ goroutine-safe\nstore data、FIFO\npass data between goroutines\nblock, unblock goroutine\nmaking channels https://golang.org/src/runtime/chan.go\nhchan struct contains\nbuf -\u0026gt; circular queue (with send index \u0026amp; receive index)\nsendx\nrecvx\nmutex\nsendq // list of sudo g\nrecvq // list of sudo g\n\u0026hellip; ch := make(chan Task, 3)\nThis comman allocates an hchan on the heap\nch is a pointer to hchan\nsends and receives sender sends\nacquires lock put a copy of data into channel buf releases lock no shared memory except hchan between goroutines\nwhat if a sender trys to send data into a full channel? Sender (G1), Receiver(G2)\nSender G1\nG1 creates a sudog(of itself, this contains goroutine pointer and data) and puts it into the sendq.\nG1 calls the scheduler to detach(gopark) G1 from the M and set G1 to waiting state.\nThen the scheduler pops a runnable G from runq and schedules it to run on the M.\nSender is blockd but not the OS thread.\nHow to resume?\nReceiver\nGet an element from channel buf then pops a sudog from sendq.\nEnqueue data(in poped sudog) into buf.\nCalls into the scheduler to set the waiting sender(G1) to runnable and put it in runq of P(or global queue).\nReturns to Receiver\nwhat if a receiver trys to receive data on an empty channel? Sender (G1), Receiver(G2)\nReceiver\nG2 puts a sudog into recvq then call gopark to detach from M.\nSender\nG1 enqueue data into the buf, and call goready???\nNo,\nG1 writes to memory location of G2 directly\nG1 writes to G2\u0026rsquo;s stack\nG2 does not need to access the channel, and fewer memory copy\ndesign considerations simplicity queue + lock v.s. lock-free\nperformance So threads not blocked\ncross-goroutine stack \u0026amp; reads writes\ngoroutine wake up is lockless\nfewer memory copy\nothers nbufferd channel: read and write directly to each other\u0026rsquo;s stack\nselect: put the current goroutine to sendq and recvq of the channels in cases then call scheduler.\n","description":"How channel works??","id":9,"section":"posts","tags":["Go","Channel"],"title":"How Channel Works","uri":"https://markogoodman.github.io/posts/how-channel-works/"},{"content":"怎麼裝舊版的 homegrew 套件 工作的時候要裝 protobuf 的 3.15.8 版，沒想到 homebrew 不給裝ＲＲＲ\n用 brew info protobuf 查了一下現在最新版已經到 3.17.x 了，3.15.8 也沒列出來\n估狗了一下發現這篇 https://itnext.io/how-to-install-an-older-brew-package-add141e58d32\n沒想到跟著做竟然噴錯，似乎是 homebrew 更新後不讓使用者直接用 url 安裝了\n還好留言裡面有個解答，幫他翻譯一下\n主要就是自己建一個 tap(有點類似套件庫的感覺)，然後把 protobuf 的 3.15.8 版本塞進去，之後 brew install protobuf@3.15.8 就可以在這個 tap 內找到。\n以下為步驟\n在自己的 github 裡面建一個 repo 名字叫做 homebrew-custom (\u0026ldquo;homebrew-\u0026rdquo; 開頭就好) 執行 brew extract \u0026ndash;version=3.15.8 protobuf \u0026lt;github_username\u0026gt;/custom 接著就可以執行 brew install protobuf@3.15.8 了 第二步驟裡面，homebrew 看到後面那個 custom 會自動加一個 homebrew- 的 prefix 變成 repo_name，然後配合 github_username 找到你的 github repo，接著去 homebrew-core repo 裡面找歷史紀錄，把 protobuf 3.15.8 拿出來，塞到 /usr/local/Homebrew/Library/Taps/ 下面的 github_username 裡面，\nbrew install 就可以運作了\n然後可以記得把 /usr/local/Homebrew/Library/Taps//\u0026lt;repo_name\u0026gt; push 上去 github，之後別台電腦 brew tap 一下就可以用了\n","description":"ㄜ 如題","id":10,"section":"posts","tags":[],"title":"怎麼安裝 homebrew 裡面的舊版 package","uri":"https://markogoodman.github.io/posts/brew-install-older-pkg/"},{"content":"Hyperloglog 這也是在工作接觸到的東西，是用來做基數統計的一個方法\n第一次看到真的覺得這是什麼巫術0.0\n目的是要做一個按讚的系統\n就是玩家可以重複對某個影片按讚，每次都要有 log，但統計按讚數時同一個玩家只計算一次\n基數統計 基數 -\u0026gt; unique value count\n第一個想到的應該就是用 hashmap，簡單好用幾乎每個程式語言都有內建\n缺點就是基數大的時候空間消耗大\n再來就是用 bitmap，把每個 key 都對應到某一個 bit，相較 hashmap 空間會少用一些，但長度基本上也是會越來越長\nHyperloglog 是一種基數的估算方法，上面兩種都是精確地知道不重複的數量有多少，也可以知道某個值是否出現過\n而這種估計方法會有誤差，而且也無法知道某個值是否已經出現\n但是相對它的速度很快，而且使用空間非常的小\nHyperloglog 基本概念 是一種機率的概念\n把資料轉換成一串 bit\n因為每個 bit 是 0 或是 1 的機率都是 1/2\n所以從這串 bit 左邊開始看，連續有 n (n\u0026gt;0) 個 0 的機率是 (1/2)^n\n所以當我們有很多筆資料時\n需要做的就是把每筆資料都轉換成一串 bit\n找出從左邊數來第一個 1 的位置 (連續0的數量+1)\n例如以下三筆\n00110110 -\u0026gt; 3\n01010111 -\u0026gt; 2\n00010010 -\u0026gt; 4\n從中取最大值 4， 2^4 = 16 個元素\n推測概念是，我們大約有了 16 筆資料，才能得到一串有連續三個 0 的資料\n大致概念是上面那樣，當然直接使用會有不少誤差\n所以接下來就是一堆誤差處理方法\n舉個簡單的例子，像是下面這種有五筆資料，將前兩個 bit 當作 bucket number，將每個 bucket 各自取最大，再把桶子之間平均\n[00] 110110 -\u0026gt; 1 (這筆資料在桶子[00]中不是max，被忽略)\n[00] 010010 -\u0026gt; 2\n[01] 010111 -\u0026gt; 2\n[10] 100010 -\u0026gt; 1\n[11] 010010 -\u0026gt; 2\n(2+2+1+2) / 4 = 1.75\n估計值 = 4 * 2^(1.75) = 13.45\n因為每次資料進來都是轉換成 bit，然後看看有沒有比目前最多連續 0 的數量還多，有的話就更新一下，所以基本上只要一串 bit 的空間就可以計算基數了\nLoglog 與 HyperLoglog 大概都是用這種概念去估計基數(Hyperloglog比較新)\n但又會用一些方法去修正算式減少誤差(乘上常數什麼，偏差修正之類的，數學家很厲害)\n詳細可以參考下面這些連結，不過每次看了都忘記，只記得概念而已\nhttp://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html\nhttps://en.wikipedia.org/wiki/HyperLogLog\nhttps://blog.csdn.net/u011564172/article/details/86597575\n然後 Redis 內其實就有內建的 Hyperloglog 可以用了\n方法很簡單\nPFADD key value // 把 value 加入基數計算內 PFCOUNT key // 估算出來的基數 不過工作上最後決定還是不用這個方法，因為根據流量來看按讚的人數根本就不會讓空間消耗很大QQ，而且按讚的名單也想存在 db 裡面，用了還要先經過 redis 感覺有點多此一舉(mongodb 沒有 hyperloglog)，直接簡單用 hash map 做就好了\n","description":"基數神器，計算不重複的元素還在用 hash table 嗎？ 來看看最新潮的 hyperloglog","id":11,"section":"posts","tags":["redis","hyperloglog","cardinality"],"title":"Hyperloglog","uri":"https://markogoodman.github.io/posts/hyperloglog/"},{"content":"Redis transaction 工作時有需要做一個\nSet if key exists\n發現 redis 只提供一個 SETNX，Set if key doesn\u0026rsquo;t exist\n順便來研究一下 redis 的 transaction\nMULTI https://redis.io/topics/transactions\n在 redis 裡面做 transaction 基本上就是使用 MULTI\nMULTI SET key0 value0 SET key1 value1\rEXEC Note: redis transaction 跟一般的 db transaction 有點不一樣，可以保證中間不被插入其他指令，照順序執行，但做到一半出錯不會 rollback，且甚至會繼續往下做\n例如\nSET a 1\rSET b ssss\rSET c 1\rMULTI\rINCR a\rINCR b\rINCR c\rEXEC\r1) (integer) 2\r2) (error) ERR value is not an integer or out of range\r3) (integer) 2 Redis 為何沒有 rollback:\nhttps://redislabs.com/blog/you-dont-need-transaction-rollbacks-in-redis/\nRedis persistency:\nhttps://redis.io/topics/persistence?_ga=2.166846334.1799612137.1620217522-2107259978.1617716900\ntransaction 做到一半真的機器停住的話，可能真的只有一半指令進去，AOF的話就可以把多的那部分去掉，RDB的話應該是直接掉資料了（？\n我們 Server 使用的是 https://github.com/go-redis/redis\n裡面提供了 TxPipeline 就是接到 MULTI 上面\ngo-redis 提供的 Pipeline 沒有用 MULTI EXEC 包起來，中間有可能被切開，就是一次傳很多指令去 redis 減少 round trip 時間\n使用方法也滿簡單的\n1 2 3 4 pipe := client.TxPipeline() pipe.Incr(key) pipe.Expire(key, time.Minute*15) _, err = pipe.Exec() 不過還是沒辦法達到 set if key exists\n如下面的範例，因為 exists 要等到 pipe.Exec() 之後才會有正確的值進去\n1 2 3 4 5 6 pipe := client.TxPipeline() pipe.Set(key, 1, 1*time.Second) exists := pipe.Exists(key) fmt.Println(exists.Val()) // 0 pipe.Exec() fmt.Println(exists.Val()) // 1 估狗了一下發現只好用 WATCH 來 workaround\nWATCH 可以監視一個 key\n之後使用 MULTI EXEC 如果發現這個 key 被動過，就會 EXEC 失敗\nWATCH key0\rMULTI\rSET xxx xxxxx\rEXEC // 如果 key0 在 EXEC 之前被改過，就會失敗 所以要實現 set if key exists\n就變成\n// sudo code\rWATCH key0\rexists = EXISTS key0\rif exists == True {\rMULTI\rSET xxx xxxxx\rEXEC // 如果 key0 在 EXEC 之前被改過，就會失敗\r} 但也不全然是只要存在就 set，因為當 key0 被改的時候也會不做\n下面是 Go 裡面的做法\n參考：https://www.tizi365.com/archives/309.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 key := \u0026#34;key\u0026#34; fn := func(tx *redis.Tx) error { // Do things exists := client.Exists(key).Val() if exists != 1 { return nil } // client.HSet(key, \u0026#34;B\u0026#34;, 666) // Modifying key causes transaction failure // call Pupeline if key is not changed _, err := tx.Pipelined(func(pipe redis.Pipeliner) error { pipe.HSet(key, \u0026#34;A\u0026#34;, 2) return nil }) return err } client.HSet(key, \u0026#34;A\u0026#34;, 1) fmt.Println(\u0026#34;Before\u0026#34;, client.HGet(key, \u0026#34;A\u0026#34;).Val()) err := model.userRedis.Watch(fn, key) fmt.Println(err) // 失敗的話會印出 redis: transaction failed fmt.Println(\u0026#34;After\u0026#34;, client.HGet(key, \u0026#34;A\u0026#34;).Val()) return ","description":"Redis transaction 怎麼用Ｒ","id":12,"section":"posts","tags":["Redis","Transaction"],"title":"Redis Transaction 使用","uri":"https://markogoodman.github.io/posts/redis-transaction/"},{"content":"Docker https://www.youtube.com/watch?v=3c-iBn73dDE\n前幾天看了一個簡單介紹 docker 的影片還滿完整的，紀錄一下大概講了什麼，沒有太深入的東西不過應該滿夠用的\n他裡面有做了一個簡單的範例，之後要用應該滿好參考的\nhttps://gitlab.com/nanuchi/techworld-js-docker-demo-app\n為什麼要 Container 可以把一個 application 和他所需要的所有東西(依賴的 lib 或是 自己寫的 config 之類的)包在一起，所以這個 container 不需要依靠其他東西來運作\n因為需要的東西都包在一起了，所以不管拿到哪台機器上都不會因為機器的環境差異造成行為不同\n讓開發到部署都非常方便\n例如：\n開發期間，不同的開發者，要把環境設定到一樣是很困難的，不同機器上說不定會行為不同，造成開發困難\n甚至也可能在一台電腦上需要不同版本的某個應用程式，要一直切換\n或是部署時，開發人員把一堆程式的安裝以及環境設定的步驟交給 operation team 去設定 server，但時常這些複雜的步驟是無法所有細節都交代清楚的\n而且要保持開發和部署環境一直一致也是很困難，以上都很容易出事\n這些事情有容器幫忙就可以方便很多\nDocker vs Virtual Machine 作業系統有兩層，kernel 層和 application 層\n不同版本的 Linux 例如 debian, ubuntu，就是用同一種 kernel，但上面是不同的 application\nDocker 只虛擬化 application layer，使用 host 的 kernel\nVM 則是兩層都有（虛擬化整個os）\n所以 docker 小、快\n但如此一來某些 docker image 就會不適用在某 host 上 因為 os kernel 與 image 不相容，像是舊版的 windows 與某些舊版的 mac\n要用 docker toolbox 處理\nContainer, Image ?? image 就是一個模板，敘述了這個 container 作業環境裡面所有的東西\n執行起來之後就是一個 container\nContainer Repository(Registry) 存放 image 的地方，公司常有私有的 repo 放內部自己 build 出來的 image\n公有的像是 Dockerhub，大家會上傳很多好用的 image 上去\nImage Image 是一個層層疊疊的概念\n例如這個 redis 的 image\nhttps://github.com/docker-library/redis/blob/1511c433cdb80391d897b5d2a04c67c261bf5f4b/6.2/Dockerfile\n就是由一個 base image (debian:buster-slim) 之上，設定一堆環境變數，把 redis 裝上去，而形成這個 image\n要製作自己的 redis image 時，也可以根據自己的需求，把 config 等等的資訊疊在這個通用的 redis image 之上，形成自己的 image\nNote: 通常最底下那層都是 Linux based image (要小，alpine 之類的)\n常用 cmd docker pull xxx\ndocker run xxx\ndocker run -d xxx = 不卡在 terminal\ndocker ps -a 列出所有 container 包括沒在跑的\ndocker rm $(docker ps -a -q) 刪掉所有\ndocker stop start\nport 需要綁定 host port to container port 否則一個 container 跑在那邊也是 unreachable\ndocker run -p6000:6379 redis\n執行這指令可以綁 host port 6000\nDebug docker logs (name or id)\ndocker run \u0026ndash;name xxx 可以指定名稱\n進入terminal\ndocker exec -it 4839d8c690c8 /bin/bash (sh)\nit = interactive terminal\nDocker network container 們會跑在一個隔離的 network 裡面\n要讓不同的 container 方便的溝通就要把他們放在同一個 docker network 內\n可用 docker network ls 看有哪些\nDocker compose 就是把一堆 run docker 的指令結構化寫在一個檔案內\nservice 下面會有一堆 container name (對應到 docker run \u0026ndash;name)\n而且會自動架 docker network\ndocker-compose -f docker-compose.yaml up(down) -d\nNote: docker-compose 是讓一堆 container 跑在同一個 host 的東西，要一次操作一堆 host 上的 container 需要用 docker swarm 或是 k8s 之類的\nhttps://stackoverflow.com/questions/47536536/whats-the-difference-between-docker-compose-and-kubernetes\nDockerfile 用來 build docker image 的藍圖，就是一堆敘述說要用哪個 base image，要裝什麼套件等等等的\n-t 給名字和 tag\ndocker build -t my-app:1.0 .\nDocker volumes Data persistency\n換一個 container 時資料就會不見 (docker-compose down up)\n把 host 上的資料和 container 內的資料綁在一起\n資料就不會掉\ndocker volume ls\n主要有三種\ndocker run -v host_path:container_path，寫出兩邊的路徑 docker run -v container_path，只寫 container path，docker 會在某個地方自動創一個資料夾 volume (anonymous volume) docker run -v volume_name:container_path 最常用的，named volume。先創造 volume 再直接給名字 ","description":"某個 docker 影片的筆記，大概就是簡單介紹 docker 是什麼，沒有太深的理論","id":13,"section":"posts","tags":["docker"],"title":"Docker","uri":"https://markogoodman.github.io/posts/docker/"},{"content":"Websocket https://www.cnblogs.com/nuccch/p/10947256.html#websocket%E5%8D%8F%E8%AE%AE%E6%98%AF%E4%BB%80%E4%B9%88\n是一個應用層的 protocol (跟 http 同層)\n最重要的功用大概就是雙向傳輸，建立連線後就把連線開著，client 和 server 之間都可以互相傳訊息，且 overhead 比 http 少\nhttp 傳送訊息時都是一整個一起傳\nwebsocket 可以切成很多 frame 分開送，建立連線後每個 frame 的 header 也很小。這是應用層的 frame，到達應用層級時 protocol 會幫你組好\n為何使用 原本的一個 http server 必須等待 client 傳 req 才可以回應（像是下面的1、2）\n如果是想要持續更新數據，這樣會造成很多 overhead\n其餘持續更新數據方法 polling\n每一段時間都發 http request 去問，有點浪費頻寬而且不一定會命中\nlong polling\n發一個 http req 給 server，server 先把連線留著不回應，等到有資料再回應。client 收到後再發一個 req。重複以上動作\n資料更新頻繁時 client 也是要一直發 req 才行\nserver sent event\n單向 (server-\u0026gt;client) 資料傳輸，websocket 太方便了所以這個比較少人用\n不過在某些特定時候還是可以出場，但我也沒用過\n詳情請參考：https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource\n建立連線 Client 先發 http request (http2.0 不可升級)\nGET /index HTTP/1.1\rHost: www.example.com\rConnection: upgrade\rUpgrade: websocket\r// other field, version 之類的 成功的話，就會把這個連線升級為 websocket\nHTTP/1.1 101 switching protocol xxxx\rConnection: upgrade\rUpgrade: websocket\r// other field 就變成 websocket 連線了\n和 http2 的關係 目前看到比較合理的解答XD\nhttps://stackoverflow.com/questions/28582935/does-http-2-make-websockets-obsolete\nwebsocket 最重要的功能之一就是雙向溝通，兩方隨時都可以傳送資訊，不必被 request + response 綁住\n而 http2 也有 server push，這樣的話 websocket 會被取代嗎？？\nserver push 大多都被拿來使用在瀏覽器讀網頁的效能增進，例如 client 要一個 html，server 可以把裡面的圖片等等資源一起給，避免多次 request\n所以 http2 常常被認為是 request/response + server push (回應時多回一些東西)，而無法雙向傳輸\nhttp2 其實是可以達到雙向傳輸的(例如 grpc 就可以，grpc 使用 http2 的資料傳輸機制再加上 protobuf)\n但是瀏覽器並沒有 expose 足夠的 API 讓 javascript 去操作這塊，所以目前無法達成\n大家只好繼續在瀏覽器上用 websocket\n","description":"websocket 是啥～ 跟 http2 比較一下","id":14,"section":"posts","tags":["http"],"title":"What Is Websocket","uri":"https://markogoodman.github.io/posts/what-is-websocket/"},{"content":"HTTP 們 HTTP https://notfalse.net/39/http-message-format\nHyperText Transfer Protocol\n一種應用層的通訊協定\n簡單來說就是應用程式之間的一種溝通規範，規定好大家資料格式長什麼樣子，互相該如何來回訊息。如果大家都依照這種統一規範來做自己的東西，那麼在銜接上就會很容易\nhttp 通常使用 tcp 協定 (http3 是 udp)\n建立 tcp 連線 3 way handshake 基於 tcp 的 http 們，一開始要過三次的訊息交換來建立連線\n詳細請參考 https://notfalse.net/7/three-way-handshake\n注意這時傳送的訊息跟應用程式實際溝通時的 request 和 response 不一樣\nnote: ack 回應時不單是seq+1，而是 seq+(整個header+data的長度)\nStateless 每個 request 都不會預設 server 知道自己之前做過什麼\nRequest 要依靠自己提供完整的資訊，或提供線索讓 server 知道去哪裡找到資訊 (在 header 帶著 cookie 之類的)\nRequest, Response Request 長相如下 (Ref: https://documentation.help/DogeTool-HTTP-Requests-vt/http_request.htm)\nResponse\n各代 http 不同代的 http 都有一些問題，下面簡單舉幾個例子\nHTTP/1.0 預設不使用 keep-alive (然後好像大家真的都沒用ＸＤ)\n這樣的話每次request都要建立一個TCP連線，想法是為了避免 client 佔著 server 太久，所以做完一件事情就關掉\n如果開一個網頁要讀取很多東西 -\u0026gt; 建立很多連線(3 way) -\u0026gt; 耗費資源\nHTTP/1.1 預設持久連線，重複使用同一個TCP連線\n要馬上關閉的話要特別在 header 裡面的 Connection 寫\n有 head of line blocking 的問題\nhttp1.1 有 pipeline 功能嘗試解決這個問題\n但似乎規定 server 需要依照收到 request 的順序回給 client，代表每個 req 都要多帶一些資訊，且應用程式都必須特別處理這一部分，複雜度太高，且很多 proxy 根本不是你能控制的，所以都沒人在用。\nHOLB 還是存在\n比起 1.0 還多了一些 header etag 之類的東西\ncache: https://blog.techbridge.cc/2017/06/17/cache-introduction/\n可以對 body 的部分進行編碼順便壓縮，常見的像是 gzip https://crypto.stackexchange.com/questions/40215/encoding-vs-compression-vs-encryption\nHTTP/2.0 也是基於 tcp，http2 許多東西都與 http1.1相容，像是 methods、status code 那些\n然後優化了其他東西\n主要有下面幾個變動\n可以 server push (主動丟東西給 client)，在之前的 http 中，都需要　client 先發 request 後 server 才可回應\nheader 壓縮 (hpack)，可以壓縮到剩下幾 byte，原本的可能數十甚至數百\n上面提過的 holb 問題解決了(解決了應用層的，但 tcp 本身網路層就有 holb，http3 才會試著用 udp 來解決)，一條 tcp 連線裡面可以有多個 stream\n例如一個 request 會有一個 stream id x，request 可以被拆成 frame 們，到目的地再組起來，不同 request 的一堆 frame 們都可以不照順序傳輸。\n其實長得還滿像 tcp packet 的 XD\nBinary protocol\n這條不得不說從字面上看起來真的很看不懂XDD\n後來查到這篇 https://stackoverflow.com/questions/58498116/why-is-it-said-that-http2-is-a-binary-protocol 才比較了解一點\n大意就是，原本基於文字的 http，接收的人看到資料就是一串 character，需要一個個字去讀出來解析，找到 newline character 之類的，速度超慢。\n且在傳輸上，一個 request 或是 response 都必須一次傳送到達目的地，在這之前其他人都不能用這條連線(與第三點相關)。\n而 http2 的每個 frame 都很有結構(https://tools.ietf.org/html/rfc7540#section-4.1)，譬如前面 n 個 bit 代表 frame 長度，接下來 m 個 bit 代表什麼什麼，內容多長等等的，都可以預先知道，在解析上就可以非常容易\nhttp3.0 tcp 接收資料時會確保資料封包是連續的，才會依照順序把資料拿給應用程式\n譬如有 1 2 3 4 5個封包\n如果傳送中 1 3 正確到達但 2 掉了，tcp 就只能等到 2 到達後才把 3 也一起拿來用(即使 1 3 這兩個封包在應用層上都可以直接使用，但 tcp 並不瞭解應用層的事情)\n且在此同時 sliding window 也可能被卡住，傳送端或線路上即使空閒也不會繼續傳送後面的封包，造成 holb 的問題\nhttp3 就用 udp + QUIC 來解決這個問題\nQUIC 也是一種傳輸層的協議，提供可靠傳輸\n至於細節還沒研究 XD\n現在還沒有被很廣泛的使用\n下一篇可能會拿 websocket 和 http2 比較看看，因為現在其實很多瀏覽器都支援 http2 了，http2 又有 server push 的功能，感覺和 websocket 還滿像的 ?? GRPC???\n","description":"HTTP～","id":15,"section":"posts","tags":["http"],"title":"What Is Http","uri":"https://markogoodman.github.io/posts/what-is-http/"},{"content":"這章節是概略介紹一下 Go 的 scheduler 如何運作的～\n有些事之前有看過\n不過 tasks 和 continuation 真的是第一次聽到，滿有趣的概念\nWork Stealing Fair scheduling，直接把一堆任務平均分散到各個 processor 上\n但因為 Go 是使用 fork-join，一個 goroutine 是從一個 goroutine 分出去的，常常會互相依賴(順序問題)，可能造成各個 processor 使用不均（每個task loading不同）。且 cache 也可能有效率差的問題，如果相關 task 分散各個 processor。\nA centralized task queue? 如果引進“一個” task queue 呢？\n負責裝所有任務，有空的 processor 就去拿任務來做\n是可以解決一些 loading 分配不均的問題，但大家都要去存取這個唯一 task queue，進出 critical section 很耗費時間。且 cache 問題也沒解決，不知道哪個 task 會去哪個 processor 上\nMultiple queue? 再進一步，給每個 processor 都給一個 local queue 呢？\n解決的 critical section 的問題了\n但如果 fork 出來的 goroutine 都在同一個 P 上，要怎麼搬去其他 P，還有 context switch 怎麼做？？\nWork stealing fork 時，加入該 thread 的 queue 的尾端 thread 沒事時，去別人的 queue 頭端偷工作 當 join point 到達，要等待其他人回應結果才能繼續時，這個 thread 會先暫停這個工作，並從自己的 queue 的尾端拿工作出來做 thread 的 queue 是空的時候，可能會停在 join point 或是去別人的頭端偷工作來做 可以注意的是 1 和 3，都是從尾端操作\n因為尾端的任務是最後 push 進去的，最有可能是 parent 要 join 所需要的結果，快點 join 效能好\n因為是達到 join point 前最後處理的東西（尾端工作），所以最有可能還在 cache 中\nStealing Tasks or Continuations? 假設目前正在執行的 goroutine 叫做 A，而 A 啟動了一個 goroutine B，那此時目前的 thread 會選擇 A 剩下的內容(continuation)或是 B (task) 繼續做呢？\n答案是通常會把 task 拿來做，而把 A 推進 local queue 的尾端\n原因是\n通常程式都是希望中間 fork 出去的 goroutine 做完一些事情，然後 join 回來原本的 goroutine 原本的 gorutine fork 出別的 gorutine 後過不久通常會卡住等 join 如果 thread 繼續做A，那通常就會是 (A 做到 join point) -\u0026gt; (B 做完) -\u0026gt; (繼續A)\n如果是做 B，那通常會是 (B 做完) -\u0026gt; (繼續A)\n少了一個 context switch\n注意 fork 時選擇 A 或 B 都不算 context switch，只是選擇其中之一丟進 queue 而已\ncontinuation-stealing 指的就是在 fork 時，是把原本的 goroutine 剩下部分都進 queue 內，這種方法被認為是比較好的，蛋需要特別的 compiler 支援，而 Go 有\nGMP model G 代表 goroutine 的狀態，尤其是 program counter\n接下來的一小段其實就是在說 shceduler 怎麼運作，跟之前寫過的一篇文章類似\nM(thread) 至少會有跟 P 一樣的數量，當某個 thread 被 G 卡住(讀IO之類的)，這個 M 就會跟 P 切開，自己去旁邊等，讓 P 可以去跟別的 M 結合，繼續使用 CPU\nGlobal run queue 則是為了他們的實作細節優化，我就不知道詳細了ＸＤ\n最後他寫了目前如果一個 goroutine 沒有斷點(function call、io那些)的話是無法被搶奪的，會造成問題\n這個實際上我在工作時就遇過ＸＤ\n有一個 goroutine 裡面跑了無窮迴圈，好像導致垃圾回收把其他人都停止之後一直在等這個 g 等不完就爆了\nlocal 測試時測不出來，後來發現原來是新版的 Go 已經把問題解掉，好像是1.14(?\n爆掉的那台機器好像是用比較舊的某版才會發生，真4神奇\n","description":"Concurrency in Go 第六章～ Goroutines and the Go runtime ","id":16,"section":"posts","tags":["Go","Goroutine","concurrency"],"title":"Concurrency in Go IV 第六章 Goroutines and the Go runtime","uri":"https://markogoodman.github.io/posts/concurrency-in-go-iv/"},{"content":"筆記都沒啥整理Ｒ\nError Propagation // 好像跟 concurrency 比較無關(?\n寫 concurrent code 的時候，debug 會比平常難很多，如果互相傳遞資訊時都附上 error 會比較好\nError需要包含一些東西:\n發生的事情\n例如硬碟滿了、網路斷線等等。在傳遞途中我們也可以自己包些資訊\n時間與地點\n要記錄發生的時間\n要包含完整的 stack trace\n還有上下文(這個不太有想像)，例如分散式系統，要知道是哪個機器出錯\n顯示用訊息\n給使用者看的，不包含 stack trace 這類太詳細的資訊\n類似前面兩點的精簡版\nerror id\n讓看到顯示用訊息的使用者可以找到詳細資訊\n這些資訊大多都要我們自己包，一般的 error 無\n1 2 3 4 5 6 7 8 9 10 11 12 func High(param string) error { result, err := Low.Do() if err != nil { // 檢查資訊是否有包完整，基本上都要包，逃過檢查的就是沒包好 if _, ok := err.(Low.Error); ok { err = WrapErr(err, \u0026#34;cannot do High id:%v\u0026#34;, param) // 會不會使用 defer 包比較好啊? 就不用在每個 func call 都包一次，dup code } return err } // ... } 通常只在 module boundaries(像是public function/method) WrapErr\n在最上層時，如果發現不是自製的 error，就輸出 default 訊息，如果是的話就直接輸出 err.Message 即可。但兩個都要記得輸出 error id\n以下是課本上 p. 151 舉的例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type MyError struct { Inner error Message string StackTrace string Misc map[string]interface{} } func wrapError(err error, messagef string, msgArgs ...interface{}) MyError { return MyError{ Inner: err, // 完整的底層資訊 Message: fmt.Sprintf(messagef, msgArgs...), // 顯示用，會把下層的 err.Message 藏起來 StackTrace: string(debug.Stack()), Misc: make(map[string]interface{}), // 雜亂資訊放這 } } Timeouts and Cancellation 為何想要 timeout？\n很邊緣的 req 可能會想要讓他 timeout 而不是讓他等超久\n何時適合 timeout？\nto後不會一直重複來（造成negative feedback loop） 沒有資源去儲存此 req，像是 queue 滿了之類 req 會過時，等很久換到它時候，已經處理也沒意義 預防 deadlock，死結也時常會發生在程式執行一段時間後，測試時不一定抓得到。死結時通常只能重啟機器，換成 timeout 雖然有可能也是活結，但至少發現後可修，不是直接爆炸 何時想取消？\n上面那些情況 使用者想取消 parent 如果停止了，該 concurrent op 也要取消 Replicated requests，某些情況下(p. 159)為了效能，可能會收到兩個一樣的 req，一個完成後，其他取消 簡單範例\nG -\u0026gt; A -\u0026gt; B\n當 A 沒效率時可能會開啟 A2\n如此 B 可能收到重複的\n一個 op 取消時，其餘的相關 op 該怎麼辦（正在執行的東西或其他子op等等等）\n一個可能會被取消的 op 底下會呼叫到的子 op 都必須檢查，這些 op 如果執行時間超出取消後可以等待的範圍，那就必須把這些 op 做成 preemptable\n簡單處理 Replicated requests 方式就是當子 goroutine 回報結果後，要把往另一個重複的 req 送取消指令（但這就需要雙向溝通）\n或是不處理，或要求 parent permission(A要送給B時問G同意)\ntimeout和取消是程式完成後很難加入的，要一開始就考慮\nHeartbeat 讓別人知道你活著，比較常見的是一個 server 開一個 endpoint 讓別人知道 server 掛了沒\n這邊的 heartbeat 是一個 goroutine 要回傳一個 heartbeat chan，定時傳東西出去表示自己沒掛\n直接把 p. 162 的例子摳逼上來\ndoWork := func(\rdone \u0026lt;-chan interface{},\rpulseInterval time.Duration,\r) (\u0026lt;-chan interface{}, \u0026lt;-chan time.Time) {\rheartbeat := make(chan interface{})\rresults := make(chan time.Time)\rgo func() {\rdefer close(heartbeat)\rdefer close(results)\rpulse := time.Tick(pulseInterval)\rworkGen := time.Tick(2 * pulseInterval)\rsendPulse := func() {\rselect {\rcase heartbeat \u0026lt;- struct{}{}:\rdefault:\r}\r}\rsendResult := func(r time.Time) {\rfor {\rselect {\rcase \u0026lt;-done:\rreturn\rcase \u0026lt;-pulse:\rsendPulse()\rcase results \u0026lt;- r:\rreturn\r}\r}\r}\rfor {\rselect {\rcase \u0026lt;-done:\rreturn\rcase \u0026lt;-pulse:\rsendPulse()\rcase r := \u0026lt;-workGen:\rsendResult(r)\r}\r}\r}()\rreturn heartbeat, results\r} 要注意的是傳東西去 chan 都可能會被卡住包括 heartbeat，所以都使用 select 預防，且送 result 中間可能有多個心跳，所以要用 for 包住 (常常忘記)\n使用上可以在 for 裡面包 select heartbeat, result, time.After(timeout) 這幾個 case，如果太久沒聽到 heartbeat 或 result 就掛掉\nReplicated Requests 就是同時會有多個處理 request 的單元（機器、process、goroutine 都有可能），丟一樣的 req 給他們，拿最快的回應來使用\nRate limiting https://www.alexedwards.net/blog/how-to-rate-limit-http-requests\n預防 rate 太高造成系統壞掉。\n可能是攻擊，或是系統設計上本來就沒考慮到高 rate 的情況\n或是之前說的 negative feedback loop 等等等\n甚至對不同使用者（例如有無付費）設定不同的限制\n可以先將系統能接受的 request 數量限制在自己已知可掌控的範圍內\n不只服務提供方可以設置 rate limit，如果客戶使用需要付費的服務，那幫自己設 rate limit 也合理\n常用的方法叫做 token bucket，要使用資源就必須持有 token，要去桶子裡拿。桶子裡最多可以有 d 個 token，每秒會補充 r 個 token 回去桶子。\ngolang.org/x/time/rate package 實作了 token bucket\nfunc NewLimiter(r Limit, b int) *Limiter // 還有一些 function 可以幫忙產生r，像是 Every(duration) 可以自動轉成 r\rfunc (lim *Limiter) Wait(ctx context.Context) (err error)// 等於 lim.WaitN(ctx, 1)，\rfunc (lim *Limiter) WaitN(ctx context.Context, n int) // block 直到擁有 n 個 token，會去檢查 ctx 的死線和是否取消了\r使用範例\rlim := rate.NewLimiter(rate.Limit(1), 1)\rfunc XXX(ctx context.Context) error {\rif err != lim.Wait(ctx); err != nil{\rreturn err }\r} 也可以把一堆 limiter 包起來，用來做一個有各種不同限制的 limiter，例如把 disk limiter、network limiter、 second limiter(每秒限制)、minute limiter(每分鐘限制)\n在用 for 迴圈去跑需要啟動的 limiter\n範例，書上還有一些優化，不過大致概念是這樣\n// 讓 MultiLimiter 接收 interface，如此一來不止可以接收原生 Go limiter，也可以接收 multiLimiter，包好幾層\rtype RateLimiter interface{\rWait(context.Context) error\r}\rtype multiLimiter struct { limiters []RateLimiter\r}\rfunc MultiLimiter(limiters ...RateLimiter) *multiLimiter{\rreturn \u0026amp;multiLimiter{limiters: limiters}\r}\rfunc (lim *multiLimiter) Wait(ctx context.Context) error{\rfor _, l := range lim.limiters {\rif err := l.Wait(ctx); err != nil{\rreturn err\r}\r}\rreturn nil\r}\r使用上可以在不同的 func 裡面去戳自己想要用的 limiter\rtype XXX struct {\rALimiter, BLimiter, CLimiter, }\rfunc (x *XXX) DoSomething (ctx context.Context) error {\rerr := MultiLimiter(x.ALimiter, x.BLimiter).Wait(ctx)\r.....\r} 詳細請參考 p.183\n補一些書上沒有的用法\nfunc (lim *Limiter) Allow() bool // 等於 AllowN(time.Now(), 1)\rfunc (lim *Limiter) AllowN(now time.Time, n int) bool // 回傳 now 的時候桶子裡面有沒有 n 個 token，如果有會回傳 true 並消耗掉\rfunc (lim *Limiter) Reserve() *Reservation // ReserveN(time.Now(), 1)\rfunc (lim *Limiter) ReserveN(now time.Time, n int) *Reservation 預約 n 個 token，會先把 limiter 裡面的 token 先卡住\rr.OK() // 有沒有可能等到 n 個token\rr.Delay() // 多久才可以得到 n 個 r.Cancel() // 取消預約\r// package 註解的用法\r// r := lim.ReserveN(time.Now(), 1)\r// if !r.OK() {\r// // Not allowed to act! Did you remember to set lim.burst to be \u0026gt; 0 ?\r// return\r// }\r// time.Sleep(r.Delay())\r// Act()\rfunc (lim *Limiter) SetLimit(newLimit Limit)\rfunc (lim *Limiter) SetBurst(newBurst int) Healing Unhealthy Goroutines 會活很久的 process 中通常也會有一些一直都活著的 goroutine，等待資料來，處理，回應。總之他們很有可能會掛掉（？ 需要一些方法來復活他們\n這章節的 code 也太長了ㄅ\n基本上就是要跑起另一個 goroutine 去偵測目標的 heartbeat，當不正常時，就傳 done 給目標，然後重啟一個\nSummary 這章主要是在說要怎麼讓系統穩定點\n滿多主題的觀念也不只限定在 Go 的 concurrency 上\n","description":"Concurrency in Go 第五章～ Concurrency at Scale ","id":17,"section":"posts","tags":["Go","Goroutine","concurrency"],"title":"Concurrency in Go III  第五章 Concurrency at Scale","uri":"https://markogoodman.github.io/posts/concurrency-in-go-iii/"},{"content":"Concurrency in go. 第四章 Concurrency Patterns in Go 這章節就是講一些常見的 Go concurrency pattern，有哪些方便使用以及如何安全的使用這些 pattern\nconfinement 要做到 concurrent safe 除了之前說的 channel 與 primitives 還有些方法\n像是 Immutable data 是 concurrent safe 的，大家可用但是不能改，要不同的值就自己 create (像是 string)\n而 confinement 有兩種 adhoc 與 lexical\nadhoc 是類似大家約定好的寫程式規範，例如約定好只在某部分的 function 存取這個共同變數（即使在其他部分也可以存取這個變數），以這種方法來達到 concurrent safe\n而開發者變多以後，不一定大家都會知道且遵守這規範，可能造成程式出錯\n而 lexical confinement 是利用 compiler 及語言特性直接限制這種錯誤的發生，就如同只有 channel owner 可以傳資料，consumer 只會收到一個 read only 的 channel 這種方法\n或著是如下面的 code\ndata := []byte(\u0026#34;dataaatata\u0026#34;)\rgo processData(\u0026amp;wg, data[:i])\rgo processData(\u0026amp;wg, data[i:]) 把data切開，直接讓兩個 goroutine 不會互相影響\n也可以盡量減少 goroutine 之間的溝通，大多數要互相溝通的程式寫起來會比較複雜一點\n常見 pattern for select for { // 無窮迴圈或某個可iter的東西例如 []string{\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;}\rselect {\rcase \u0026lt;-done:\r// break or return\rcase result \u0026lt;- data:\rfmt.Println(\u0026#34;put something\u0026#34;)\rdefault:\r// 做其他事\r} } Goroutine leak 指的是我們不希望一直存在的 goroutine 一直存在，造成記憶體問題\n例如\nvar ss chan string\rgo func() {\rfor s := range ss {\rfmt.Println(s)\r}\r}() 如果 ss 是 nil，或是根本沒人去關閉這個 chan\n那這個 goroutine 就會一直存在直到 process 結束\n程式碼中如果重複創造出這種東西到最後記憶體就用完了\n要記得關閉或著是使用 for select + done chan 去提醒 goroutine 該結束了\ngo func(){\r// done is a \u0026lt;-chan inteface{}\rfor {\rselect { case \u0026lt;-done:\rreturn\rcase OOO: }\r}\r}() The or-channel p.94 很長一串，總之要記得把 orDone 傳下去，避免上層已經結束而下面的goroutine 都沒關掉\nError handling p. 111\n在一堆 concurrent processes 中，一個重要的問題是誰要處理 error\n下面這段 code 是在 worker 裡面處理，但他最多也只能印出 err 讓人知道\nworker := func(done \u0026lt;-chan interface{}, params) \u0026lt;-chan int {\rresults := make(chan int)\rgo func() {\rdefer close(results)\rfor {\rres, err := DoSomething(params)\rif err != nil {\rfmt.Println(err)\rcontinue\r}\rselect {\rcase \u0026lt;-done:\rreturn\rcase results \u0026lt;- res:\r}\r}\r}()\rreturn results\r} 如果像下面這樣將 error 傳回，接收這些 result 的 goroutine 也可以依據 error 來決定各個 worker 是否該繼續存活等等\ntype Result struct { Error error\rResult int\r}\rworker := func(done \u0026lt;-chan interface{}, params) \u0026lt;-chan Result {\rresults := make(chan Result)\rgo func() {\rdefer close(results)\rfor {\rres, err := DoSomething(params)\rselect {\rcase \u0026lt;-done:\rreturn\rcase results \u0026lt;- Result{Error: err, Result: res}:\r}\r}\r}()\rreturn results\r} 就像是一般的 go function 會回傳 result + err，在一個對上下文不清楚，單純是負責執行任務的 goroutine 中，把 res + err 一起傳回去常常是很好的選擇 ，讓對上下文更清楚的 goroutine 去處理 error\nPipeline 一連串的 stage，輸入輸出資料，隨意組合\n輸入與輸出同type，每個 stage 獨立易修改\n// Batch processing\rfor _, i := range stage2(stage1(intSlice)) {\rfmt.Println(i)\r} // streaming\rfor _, i := range intSlice {\rfmt.Println(stage2(stage1(i)))\r} Best Practices for Constructing Pipelines channel 很適合\n上面那個 streaming 的例子是一個 element 拿出來，處理完所有 stage 之後才換下一個 element\np. 118 範例滿有趣的\n使用 channel，每個 stage 各自跑起 goroutine，每個 element 經過一個 stage 之後會直接塞到 chan 中，讓下一個 stage 拿，自己也準備開始處理下一個 element\n// example\rfunc multiply(done \u0026lt;-chan interface{}, intStream \u0026lt;-chan int, mu int) \u0026lt;-chan int{\routStream := make(chan int)\rgo func(){\rdefer close(outStream)\rfor i := range intStream{\rselect{\rcase \u0026lt;-done:\rreturn\rcase outStream \u0026lt;- i*mu:\r}\r}\r}()\rreturn outStream\r}\r// add 與 multiply 是 stage\r// generator 是一個會一直有其他人塞資料進來的 chan，可能是去讀檔案或是網路收資料等等，generator 時常也會傳入同一個 done 一起控制開關\rfor v := range multiply(done, add(done, generator, 1), 2){\rfmt.Println(v)\r} Fan-out, Fan-in p. 114\npipeline stage 中可能有某些 stage 要花很多時間來執行\n那便會成為整條pipe的效能瓶頸\n這時可以嘗試用多個 goroutine 來執行這個 stage，同時處理多個資料來加速\n這就叫做 fan out\n而多個 goroutine 處理完資料後，要將資料流到一個管道中，讓下一個 stage 可以從這個管道取出資料，則叫做 fan in\n大概的使用方法如下，例子取自書中\n// p. 116, fan-out\rnumFinders := runtime.NumCPU()\rfinders := make([]\u0026lt;-chan int, numFinders) for i := 0; i \u0026lt; numFinders; i++ {\rfinders[i] = primeFinder(done, randIntStream)\r} primeFinder 就是和之前那些例子一樣的一個 stage，接收一個 input channel 從裡面拉資料，會將結果傳回 return 的 channel，只是這個 stage 要花很多時間去計算\n而 fan-in 的例子在 p. 117，主要就是會接收 n 個 input channel (上面那些 finders)，並跑起 n 個 goroutine 來把這 n 個 input channel 的資料接到一個 output channel 上，還有用 wg 控制當 input chan 全關或 done 時，要把 output 關掉\nThe or-done-channel p. 119\n之前的 pipeline 例子上都是直接用 for 去 range 一個 channel\n但實際上不是在使用 pipeline 時，被讀取的 channel 行為會無法預知，目前的 code 收到 done 時，不代表該 channel 會被關閉\n用下面這種 code 可能導致 leak\nfor v := range channel {\r// do something\r} 所以要像前面的預防 goroutine leak 那邊使用 for select 來處理\n當目前的 code 收到 done 時，channel 就算沒關閉也可以跳出 range\norDone 主要是把展開很醜的東西包在 function 裡面，讓 code 乾淨點\nfor v := range orDone(done, channel) {\r// do something\r} 以下截自 p. 120\norDone := func(done, c \u0026lt;-chan interface{}) \u0026lt;-chan interface{} {\rvalStream := make(chan interface{})\rgo func() {\rdefer close(valStream)\rfor {\rselect {\rcase \u0026lt;-done:\rreturn\rcase v, ok := \u0026lt;-c:\rif ok == false {\rreturn\r}\r// 如果直接 valStream \u0026lt;- v 而不用 select，收到 done 時也可能被 valStream 卡住\rselect { case valStream \u0026lt;- v: case \u0026lt;-done:\r}\r}\r}\r}()\rreturn valStream\r} tee-channel 一個 chan 變兩個\nbridge-channel 把 channel of channels 中每個 chan 拿出來並把東西都讀出\nQueuing https://www.cnblogs.com/276815076/p/8615500.html\np. 124\nGo 裡面的 buffered channel 常常會被拿來當做 queue 使用\n隨便增加 queue 可能會把一些問題藏起來，像是 dead lock, live lock\n在 pipeline 中隨便加入 queue 通常不會增加整體的效能，只會改變一些些行為\n例如下面的例子\na := A(done, inputStream) // 每個 element 花費1秒\rb := B(done, a) // 每個 element 花費3秒 B 要處理一個 element 的時間比較長，所以 A 把資料往 a 送時，可能會被 B 的讀取速度卡住\n但就算在他們中間加了一個 buffer 也只是讓 A 可以快速的把 inputStream 處理完早早關閉，但總體執行時間還是被 B 限制住\n書上舉了下面的例子\np := processRequest(done, acceptConnection(done, httpHandler)) 如果是這種情況，我們就可能會想要在 accpet 到 process 之間加入 buffer，否則 client 的 request 們可能會慢慢出現 timeout, 被拒絕的情況\n這邊我們要減少的是前一個 stage 被 block 的情況，而不是 process 的效能。讓這兩個 stage 的處理互相解耦\n在這邊提到主要有兩個使用 queue 的理由\n批次處理可以得到好處 某個 stage delay 或塞住會造成不好的 feedback loop 第一種滿常見的，譬如寫進 disk 之前我們會先希望在 memory 裡面盡量收集完資料再一次寫入，因為 disk 每次讀寫都很慢。又或著是需要存取遠端的資料庫，可能會想要收集多點 command 一次送去，減少來回消耗 等等等\n第二種\npipeline 與其他系統之間可能會互相影響\n舉個例子 web server 與 client\n如果 web server 在接收 request 時沒有 buffer，必須要等 req 才能繼續接收下一個，可能造成 req timeout，導致 client 繼續送更多 req 而越來越糟。\n先把 req queue 住，client 就以為只是處理 req 久了點而已\n這種情況感覺比較難看出來，還需要對互動的系統有了解才行\n總結 queue 可以放的地方也是兩個\nbatching 有好處的地方 pipeline 開頭 可以應用的情境不多，沒必要時也不必用，不然就多一個東西要管\n書上還介紹了 Little\u0026rsquo;s law 但實在有點看不懂到底是怎麼用ＸＤ\nThe context Package 可以去看他的 interface，滿簡潔的\nctx 主要功能：\n提供 API 去從 parent ctx 的位置去取消那些相關的 call，類似之前所說的 done 裝 request 相關資料到處傳 取消功能：\n上層想取消目前的動作 目前的動作想取消他所產生出的子動作 blocking 的動作需要可以被取消動作搶奪，避免 leak ctx := context.Background()\rctx2, c := context.WithCancel(ctx) // create new instance\rc()\rfmt.Println(ctx.Err()) // nil\rfmt.Println(ctx2.Err()) // context canceled 因為是 create new instance 所以把 ctx 往下傳之後，下層無法呼叫 cancel 來取消上層（保護）\n原本的 done 方法也可以做到，但就要把 done 包來包去\n傳值的部分\nuserID := \u0026#34;qq\u0026#34;\rctx := context.WithValue(context.Background(), \u0026#34;userID\u0026#34;, userID)\rfmt.Println(ctx.Value(\u0026#34;userID\u0026#34;)) key 必須是可比較的 == != 之類的\nvalue 必須是 concurrent safe\nkey如果都使用 string，你又把 ctx 傳去別人的 package，有可能會撞\n推薦的方法是自定義 unexported type 例如:\n// p. 143\rtype ctxKey int\rconst (\rctxUserID ctxKey = iota ctxAuthToken\r) 但要提供 func 去從 ctx 抓資料，完整範例在 p. 143\n不過 ctx Value 好像也被很多人覺得很不安全，畢竟都是 interface{}\n而且官方也建議 ctx 只放 request-scoped 的東西，但這東西也滿難定義的\n感覺還是少用（？\n","description":"Concurrency in Go 第四章～ ","id":18,"section":"posts","tags":["Go","Goroutine","concurrency"],"title":"Concurrency in Go - II 第四章","uri":"https://markogoodman.github.io/posts/concurrency-in-go-ii/"},{"content":"這三章節內容其實滿少的，讀過之後隨便記一記\nch1 Why Is Concurrency Hard?\nRace condition\n應該要依序執行的不同指定們，沒有依照正確的順序造成結果錯誤\n例如下面這段程式就有可能出現很多結果，因為使用者可能以為go裡面的程式碼寫在前面，就會先跑\n1 2 3 4 5 6 7 var a int go func(){ a++ }() if a==0{ fmt.Println(\u0026#34;zero\u0026#34;) } Atomicity\n不可切割，安全，一次要做完\nMemory Access Synchronization\n不同地方可能 concurrently 存取同一個記憶體，需要變成 critical section 用鎖鎖住之類的，但會變慢\nDeadlocks, Livelocks, and Starvation\ndead lock -\u0026gt; 4個條件\nDetermining Concurrency Safety\n• Who is responsible for the concurrency?\n• How is the problem space mapped onto concurrency primitives?\n• Who is responsible for the synchronization?\nch2 Go CSP\n下圖是 Concurrency in Go 33 頁的圖\nprimitives 指的像是 sync package 裡面的 mutex\n四個條件的解釋\nAre you trying to transfer ownership of data?\n盡量維持一次只有一個人有資料的擁有權利，做完一件事後要把output丟給別人這種動作，適合用 channel\nAre you trying to guard internal state of a struct?\n把實作藏起來不讓外面看到 -\u0026gt; 用primitives\nAre you trying to coordinate multiple pieces of logic?\n到處都有 lock 很容易造成問題，而 channel 的組合性很好，出錯也好抓，複雜的邏輯最好用 channel 來做 而不是自己到處鎖來鎖去\nIs it a performance-critical section?\n真的發現使用 channel 造成巨大效能問題才改用 primitives，而不是為了效能一開始就用 primitives\nch3 goroutine 不能被 interrupt?\n以前好像是，後來改成可以，不然垃圾回收之類的功能有可能被卡住\n但大多還是在固定時間才會被暫停，例如 sleep, 等 io 之類的\ngoroutine closure 在同一個 address space 執行\nvar wg sync.WaitGroup\rsalutation := \u0026#34;hello\u0026#34;\rwg.Add(1)\rgo func() {\rdefer wg.Done()\rsalutation = \u0026#34;welcome\u0026#34;\r}()\rwg.Wait()\rfmt.Println(salutation) The sync package WaitGroup var wg sync.WaitGroup // 傳進 func 時要傳 pointer\rwg.Add(2) // +2\rdefer wg.Done() // -1\rwg.Wait() // == 0 時完成 mutex var lock sync.Mutex\rlock.Lock()\rdefer lock.Unlock()\rvar m sync.RWMutex\rm.Lock() // rw lock\rm.RLock() // r lock\rm.RLocker() // r locker Cond 有效率的等待，不用浪費ＣＰＵ跑迴圈\nc := sync.NewCond(\u0026amp;sync.Mutex{}) c.L.Lock()\rfor conditionTrue() == false {\rc.Wait() // suspend, 其他 goroutine 可以用 thread\r}\rc.L.Unlock()\rc.Singal() 去戳正在 Wait 的一個 goroutine (最早開始等的)\rc.Broadcast() 去戳所有在 Wait 的 goroutines signal 可以用 channel 來做出一樣的功能\n但 broadcast 會比較麻煩，也可能需要很多操作\nonce var once sync.Once\ronce.Do(xxx) 即使一堆 goroutine，也保證 once 只會做一件事情\n下面這種也是只會執行aaa\nonce.Do(aaa)\ronce.Do(bbb) 計數是記once 做過的事情而不是 unique function 數\nPool 創造及持有東西很貴 不希望有太多 所以放在一個 pool 重複運用並限制數量\n提前把一些東西塞進 pool 避免 create，像是 db 連線等等\n1 2 3 4 5 6 7 8 9 10 myPool := \u0026amp;sync.Pool{ New: func() interface{} { fmt.Println(\u0026#34;Creating new instance.\u0026#34;) return struct{}{} }, } myPool.Get() instance := myPool.Get() myPool.Put(instance) myPool.Get() 使用方法：\n加入 New func Get 的東西有可能是任何狀態 記得 Put 回去，通常用 defer 裡面的東西基本上要長得一樣 Channels var dataStream chan int\rvar dataStream \u0026lt;-chan int\rvar dataStream chan\u0026lt;- int\rdataStream = make(\u0026lt;-chan int, 1) 很少會 make 單向的 chan，通常是傳參數時會用\rdataStream := make(chan int)\rvar d chan\u0026lt;- int\rd = dataStream 關閉 channel\n之後還是可以無限讀取，因為可能很多下游的 goroutine 都需要讀\ndataStream := make(chan int)\rgo func() {\rclose(dataStream)\r}()\rd, ok := \u0026lt;-dataStream\rfmt.Println(\u0026#34;I GET! \u0026#34;, d, ok) // 0, false 收到 close 時會直接 break\rfor integer := range intStream { fmt.Printf(\u0026#34;%v \u0026#34;, integer)\r} p.75 列出了不同狀態的 chan 上操作會出現什麼情況，包括錯誤們\np. 91 有範例\n要用好 channel 有幾點\n確認 channel 的 owner 是誰（盡量少） owner 負責 channel 的創造、寫、關、轉移 ownership 非 owner 負責讀，確認 chan 何時關、處理 blocking 例如第 2 點，把 owner 限制在很小的範圍，就可以避免寫入 nil chan、close closed chan 之類的 p.75 所提的程式邏輯 bug\nSelect 1 2 3 4 5 6 7 8 9 10 11 12 13 v1 := make(chan int) go func() { time.Sleep(3 * time.Second) close(v1) }() select { case x := \u0026lt;-v1: // v1 有東西拿時 // xxx case v1 \u0026lt;- 1: // v1 有空間時 // ooo } 當多個case都可以時會隨機選一個\ntimeout\ncase \u0026lt;-time.After(1 * time.Second):\ndefault (通常和for一起用) 其他 case 都沒中的時候\ndefault:\n","description":"Concurrency in Go 這本書前三章的讀書筆記～ ","id":19,"section":"posts","tags":["Go","Goroutine","concurrency"],"title":"Concurrency in Go - I 前三章","uri":"https://markogoodman.github.io/posts/concurrency-in-go-i/"},{"content":"一直都聽說 Graphql 很潮(? 工作上其實也有遇到類似的問題，一個玩家的資料有一堆欄位，常常因為各種需求加了一堆參數或 API 來拿裡面的不同欄位，graphql 也有被提到說是一個好選擇，不過現在伺服器都用 restful 寫好好的要搬也是一個大工程，只好自己來研究一下 XD\nhttps://ithelp.ithome.com.tw/articles/10200678 \u0026lt;- 不知道 graphql 是什麼可以參考這個，或是去官網看～\n這東西網路上也很多介紹了就不多講，大致上就是用一個 endpoint 就可以讓 client server 之間很有彈性的選擇想要傳輸的資料，很像我們在下資料庫指令。\n下面只是玩玩看 gqlgen 這個 Go 的 graphql 套件而已，總之又是一篇雜亂的文章\ngqlgen 官方範例在此 https://gqlgen.com/getting-started/\n首先進去會教你安裝\n然後跑 go run github.com/99designs/gqlgen init 會出現他的範例\n還包含一個 playground GUI 讓我們方便下 query\ngraph/generated 裡面的東西基本上就是那些拆解 query 之類的程式碼， gqlgen 都幫我們產生好拉\nschema (與 client 的溝通格式) 就是寫在 graph/schema.graphqls 內，從 gqlgen.yml 裡面可以知道也可以分放在多個檔案內\nschema:\r- graph/*.graphqls graph/model 內是 gqlgen 依照 schema 產生的 go struct\n如果希望strcut與schema不同時，就要在gqlgem.yml的models那邊先宣告(Type Mapping)，這樣他就不會幫你gen出來\n參考gqlgen workshop的\nmodels:\rUser:\rmodel: github.com/99designs/gqlgen-workshop/db.User 或者是直接在autobind那個路徑裡面先自己宣告也行，就不會產生\n注意的是如果struct都import來的gqlgen會沒有產生出任何graph/model裡面的檔案導致 module mode l不存在\ngen的時候會跑出一堆錯誤\nvalidation failed: packages.Load: -: malformed module path \u0026quot;gqlgen-workshop/graph/model\u0026quot;: missing dot in first path element\n猜大概是找不到model這個module\n亂改了yml發現把 autobind 還有 model 這兩個註解掉就ＯＫ了\nImplement the resolvers 註解裡面有寫，其實就是把需要的東西放進去例如db\ntype Resolver struct{\rtodos []*model.Todo\r} 接著要實作 schema.resolvers.go 裡面 CreateTodo 和 Todos，這兩個 func 也是 gqlgen 依照 schema 產生的，應該滿好理解\nCreateTodo 的 input 進來我們就處理 input 存進 db\nTodos 就是回傳 db 內容這樣\n回傳對應的 struct 後 gqlgen 產生出來的 code (generated.go) 會幫我們處理剩下的事情，譬如把 client 不需要的 field 拿掉之類的\n接著就可以 go run server.go 去上面玩玩看了\n再來範例上自己寫了一個 Todo 的 struct 而不是用 gen 出來的\ntype Todo struct {\rID string `json:\u0026#34;id\u0026#34;`\rText string `json:\u0026#34;text\u0026#34;`\rDone bool `json:\u0026#34;done\u0026#34;`\rUserID string `json:\u0026#34;user\u0026#34;`\r} 此時再跑一次 go run github.com/99designs/gqlgen generate 會發現 schem.resolvers.go 多了一個 (r *todoResolver) User\n這是因為修改之後 Todo struct 與 schema 上的 Todo 對不起來 (UserID: User)，所以我們在把 struct Todo 轉換為 schema Todo 時，要給他一個方法，用 struct Todo 裡面的資料組成 schema Todo，詳情請參考，詳情請參考 generated 裡面產生出來的 code\nctx = graphql.WithFieldContext(ctx, fc)\rresTmp, err := ec.ResolverMiddleware(ctx, func(rctx context.Context) (interface{}, error) {\rctx = rctx // use context from middleware stack in children\rreturn ec.resolvers.Todo().User(rctx, obj)\r}) 基本範例到這邊差不多就結束了～\n下面還有很多 reference 可以去看看，下面只寫了我有看的\nFieldCollection https://gqlgen.com/reference/field-collection/\n前面的範例是 client query 來了，程式去 db 撈對應的資料，然後交給 gqlgen 去把不必要的欄位篩掉再傳回給 client，field collection 是要讓你在撈 db 前先知道到底有哪些 field 是要用的，撈 db 時可針對去撈，避免浪費資源\nCollectAllFields 會直接回傳一個 string slice 包含 query 最上層的 field 名稱\nCollectFieldsCtx 會回傳一個 CollectedField 的 slice，CollectedField 是一個 struct 裡面包含該 field 的一些資訊(名稱之類的)，還有該 field 的子 field 資訊\n下面改寫一下func印出相關資訊\nfunc (r *queryResolver) Todos(ctx context.Context) ([]*model.Todo, error) {\rfmt.Printf(\u0026#34;CollectAllFields: %+v \\n\\n\u0026#34;, graphql.CollectAllFields(ctx))\rfmt.Printf(\u0026#34;CollectFieldsCtx: \\n\u0026#34;)\rfor _, field := range graphql.CollectFieldsCtx(ctx, nil) {\rfmt.Printf(\u0026#34; %+v\\n\u0026#34;, field.Name)\rfor _, sel := range field.Selections {\rfmt.Printf(\u0026#34; %+v\\n\u0026#34;, sel)\r}\r}\rreturn r.todos, nil\r} CollectAllFields: [id text done user] CollectFieldsCtx: id\rtext\rdone\ruser\r\u0026amp;{Alias:name Name:name Arguments:[] Directives:[] SelectionSet:[] Position:0xc00011df18 Definition:0xc00011b0a0 ObjectDefinition:0xc000108b40}\r\u0026amp;{Alias:id Name:id Arguments:[] Directives:[] SelectionSet:[] Position:0xc00011df58 Definition:0xc00011b030 ObjectDefinition:0xc000108b40} 發 query 後可以看到 CollectAllFields 回傳了上層的 field name， user 裡面只要至少有一個 field 就會回傳 user\n而 graphql.CollectFieldsCtx 裡面的各個 field 還有 Selections，這就是包含了子 field 的資訊，Selection 是 interface，這邊只試印出來，實際上要用 CollectFields，裡面會去轉型。\n因為 field 無法預測到底有幾層，實際上是要用範例中遞迴的方式去解開\nQuery complexity 因為 filed 底下有可能又有 field 又有 field，例如下面\ntype User {\rfriends: [User!]!\r} 可能 query 出一坨東西然後爆炸\n所以需要某些方法來限制 query 的量\n下面先修改原本的例子 schema 和 model 並 go generete ./...\n---- graph/schema.graphqls\rtype Todo {\rid: ID!\rtext: String!\rdone: Boolean!\ruser: User!\rrelated(count: Int!): [Todo!]! # count 用來限制回傳的 related todo 數量\r}\rinput Relation {\ra: String!\rb: String!\r}\rtype Mutation {\rcreateTodo(input: NewTodo!): Todo!\raddRelated(input: Relation!): Todo!\r}\r---- graph/model/todo.go\rtype Todo struct {\rID string `json:\u0026#34;id\u0026#34;`\rText string `json:\u0026#34;text\u0026#34;`\rDone bool `json:\u0026#34;done\u0026#34;`\rUserID string `json:\u0026#34;user\u0026#34;`\rRelated []string `json:\u0026#34;related\u0026#34;`\r} 然後把 schema.resolvers.go 實作一下\naddRelated 就是把 b 加到 a.related\nfunc (r *mutationResolver) AddRelated(ctx context.Context, input model.Relation) (*model.Todo, error) {\rvar a, b *model.Todo\rfor _, todo := range r.todos {\rif todo.ID == input.A {\ra = todo\r} else if todo.ID == input.B {\rb = todo\r}\r}\rif a == nil || b == nil {\rreturn nil, errors.New(\u0026#34;QQ\u0026#34;)\r}\ra.Related = append(a.Related, b.ID)\rreturn a, nil\r}\rfunc (r *todoResolver) Related(ctx context.Context, obj *model.Todo, count int) ([]*model.Todo, error) {\rtodos := []*model.Todo{}\ri := 0\rfor _, rid := range obj.Related {\rfor _, todo := range r.todos {\rif rid == todo.ID {\rtodos = append(todos, todo)\ri++\rif i \u0026gt;= count {\rbreak\r}\r}\r}\r}\rreturn todos, nil\r}\r---- server.go\r加入 srv.Use(extension.FixedComplexityLimit(5)) 這時如果我們下了 query\nquery getTodos {\rtodos {\rid\rtext\rdone\rrelated (count:1){\rid\rtext\r}\r}\r} 會出現 message\u0026quot;: \u0026ldquo;operation has complexity 7, which exceeds the limit of 5\u0026rdquo;\n這是因為預設一個 field 和一層深度都會 complexity + 1\n這邊共有 7 個 complexity (todos, id, text, done, related, id, text)\n但也可以發現count不管改成多少complexity都不會變\n我們可能會想要限制related的數量\n這時可使用 custom complexity Calculation\n把 server.go 改成\nc := generated.Config{Resolvers: \u0026amp;graph.Resolver{}}\rcountComplexity := func(childComplexity, count int) int {\rfmt.Println(\u0026#34;childComplexity: \u0026#34;, childComplexity, \u0026#34;, count: \u0026#34;, count)\rreturn count * childComplexity\r}\rc.Complexity.Todo.Related = countComplexity\rsrv := handler.NewDefaultServer(generated.NewExecutableSchema(c)) 修改的 struct是graph/generated/generated.go 裡面的 ComplexityRoot struct 計算複雜度的 function\n接著跑 query\nquery getTodos {\rtodos {\rid\rtext\rdone\rrelated (count:2){\rid\rrelated (count:3){\rid\ttext\rdone\r}\r}\r}\r} 會得到\noperation has complexity 24, which exceeds the limit of 5\u0026#34;,\rchildComplexity: 3 , count: 3 childComplexity: 10 , count: 2 ＃上面使用 countComplexity func 計算複雜度 = childComplexity * count\n看起來是用遞迴先跑到最內層的 related (count:3)，他的 childComplexity是3 (filed數量)，算出來等於 9\n再來 related (count:2) 的 childComplexity 是 10 (id + related (count:3)的複雜度)，複雜度20\n最後再加 todos, id, text, done = 24\n總結一下，在 query 來的時候 gqlgen 就會幫我們把複雜度算好了，可以自己替換算 complexity 的 func，如果超過我們設定的複雜度限制就會不給他過，避免有人亂打一堆奇怪的 query，把頻寬或程式弄爆\n這次看下來其實感覺 graphql 優勢還滿明顯的，不過應該也有很多坑還沒研究到，像是 restful 就不會有 query complexity 這種東西要處理，而且要用這種新的東西公司內要用感覺也要一陣子研究才能上手\n這次的範例程式碼放在下面的 repo 記錄一下，不然之後要用的時候八成也忘記怎麼用了\nhttps://github.com/Markogoodman/gqltest\n","description":"gqlgen! 簡單紀錄一下","id":20,"section":"posts","tags":["Go","gqlgen","graphql"],"title":"Gqlgen","uri":"https://markogoodman.github.io/posts/gqlgen/"},{"content":"tags: Golang 垃圾回收～～～ 追蹤heap裡面的mem 刪除沒在用的 保留有在用的 主要有分兩大類\n一. Reference counting\n每個 obj 記住自己有被多少人連結到，歸零的時候順便回收\n例如：\na = Person(\u0026#34;A\u0026#34;) // new 一個 Person 的 instance，這個 instance 的 ref count = 1\rb = a // b 也去指相同的東西 ref count 再 +1\rb = None // ref count -1\ra = None // ref count -1 變成 0，記憶體釋放 問題：\ncircular references\na = Person(\u0026#34;A\u0026#34;)\rb = Person(\u0026#34;B\u0026#34;)\ra.friend = b\rb.friend = a\ra = None\rb = None 雖然已經沒有任何變數直接指到新建的兩個 instance，，但實際上兩個 instance 還是互相 reference，讓 ref count 不會歸零。\n所以只看 ref count 來回收還是有可能 memory leak\nhttps://zhuanlan.zhihu.com/p/101781276\n主要有兩種解決法\n讓寫程式的人處理\n譬如在 a.friend = b 時，指定這個是 weak pointer，讓它不列入 ref count 的計算。\n好處是程式語言除了 reference counting 就不用做其他事了\n壞處是碼農們沒寫好還是會 mem leak\n讓電腦做\n加入一些 tracing GC 來輔助處理這些 circular references 的問題\n下面會再講這是啥～\n好壞處就和上面相反\n總結計數的方法\n優點：好做，計數歸零就釋放，不會像 tracing GC 一樣，要找個時間來做 GC 讓程式暫停一陣子，\n缺點：每次都增加減少 count 也會有效能問題，且是不能出錯的東西，甚至要用鎖之類的方法來確保不會有 race condition\n且要額外處理迴圈引用的問題\n二. Tracing: GC 啟動時從 roots 出發，再來找出沒在用的東西然後回收\n可以想像成一個 graph，裡面的 node 是一塊一塊的記憶體，被程式 allocate 來使用，edge 就是 reference\nroots 就是那些我們可以直接存取到的東西 (像是全域變數或是 goroutine stack)，由這些 roots 開始去做圖的 traverse，所有可以從這些 roots\n連接到的東西都是還在使用的，到達不了的 node 就是垃圾\n在這邊也有一種 weak pointer 但和 reference counting 的用法不同，這種 edge 是可以存取一個物件，但不保護這個物件不被回收\n常見名詞：\nMutator: 改變那些記憶體reference的東西，像是我們的程式就是\nMutator roots: root set, root objects，可以直接被存取的物件，像是靜態或全域變數、參數、Goroutine stack的其他東西\nCollector: 負責回收垃圾的程式\nCopy GC 最原始的垃圾回收\n把全部的記憶體切成兩份，一般時候都是用同一份\n當要收垃圾的時候就是從 roots 出發 traverse，把不是垃圾的東西複製到另一份，然後原本的那一份就可以清空了（當作垃圾，不用真的清）\n文章可以參考這個：https://zhuanlan.zhihu.com/p/28197216\nTraverse 和複製時要注意的就是指標的值也要順便更新\n缺點是空間要用很多\n不過複製的時候可以順便讓記憶體更 compact 是好處\n基本的 Mark and Sweep https://liujiacai.net/blog/2018/07/08/mark-sweep/\n就是簡單的收垃圾XD\n網路上很多 pseudo code 可以看\n先把其他在做的事情都停住 (Stop the world) 從 roots 出發把有用到的東西 mark 起來 掃過 heap 所有的物件，把沒用的清除 恢復運作 簡單可以發現問題點，有記憶體碎片、掃過整個 heap 很慢、要停止所有東西\nhttps://stackoverflow.com/questions/23057531/what-are-the-advantages-and-disadvantages-of-having-mark-bits-together-and-separ\n優化\nBitmap marking： 用 bitmap 來紀錄哪些 object 是被 mark 住的\nLazy sweep：allocate 時才去做清垃圾，而且只清剛好需要的空間就好，平常只做標記\nMark-Compact 先把還有在用的標記起來然後 compact\nhttps://en.wikipedia.org/wiki/Mark-compact_algorithm\n目的就是要把還在使用的記憶體聚集在一起，減少記憶體碎片，碎片多就是母湯\n常見的做法就是掃過 heap，然後把每個 obj 往邊邊移動就好\nwikipedia裡面介紹了兩種方法\nTable-based 是需要掃過記憶體兩次，第一次掃過時要用一個 break table 記住每個 obj 原本的位置還有預計要移動到的位置，然後就將該 obj 移動\n第二個 pass 是要用 break table 調整這些 obj 裡面的 pointer\nBreak table 也是存在同一個 heap 內，搬移的時候可能會卡到，table 打散要重新排序(???)\nLISP2 algorithm 是一個簡單多的演算法\n要 3 個 pass\n算出來每個 obj 預計會到哪 更新每個 obj 指出去的 pointer 移動~\n其中第一步預計要移動去哪的資訊會存在各自 obj 中 Incremental GC https://liujiacai.net/blog/2018/08/04/incremental-gc/#%E5%A2%9E%E9%87%8F%E5%BC%8F-GC-%E6%80%9D%E8%B7%AF\nhttps://segmentfault.com/a/1190000022030353\nhttps://titanwolf.org/Network/Articles/Article?AID=786c12ef-88b2-42da-984a-41ef4b5669d1#gsc.tab=0\n不一次做完整個垃圾回收的動作 (其實 ref counting 就類似，但缺點不少)，每次停頓小(最重要的優點)，但 throughpu 會降低就是了\nmark 的階段會被分成很多次\n三色標記法（tricolor marking） 一樣從 roots 出發去 traverse 整個 obj graph，過程中會將 obj 們標成不同顏色\n一開始 roots 都是灰色，其餘白色\n黑：可以到達，且該節點的子節點都看過了\n灰：可以到達，還沒去檢查該節點的子節點們\n白：還沒到達，如果 mark 結束都還是白色就會被當作垃圾\n標記階段可以被中斷，下次要開始時可以從灰色的子節點開始檢查\n直到沒有灰色節點時就完成 mark 階段\n灰色是用來記錄 mark 中斷後，又被 mutator 改變狀態的節點\n新建的 obj 依照不同的方法會設為不同顏色\n然而在標記階段中斷後再次開始前，我們的程式(mutator)可能會去改變這些 obj 之間的 reference，以下圖為例 (從上面網站抓來的)\n改變後可能造成黑A直接 ref 到白D，下次 mark 再開始後因為 A 是黑色，導致 D 不會被掃到，誤當成垃圾\nTricolor invvariant 為了避免上述的情況發生，三色標記法限制了節點的顏色與 reference 的關係\n下面兩種只要其中一種滿足即可\nStrong Invariant: 黑色不可直接指到白色，上面那種情況我們不會再去檢查黑點的子節點，導致戊收垃圾\nWeak Invariant: 黑可以直接指到白色，但這個白色必須是某個灰色節點的直接或間接後代\n參考圖 https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/\nD 在右圖是 B 的後代，所以允許 A 直接 reference\n接下來就是當 mutator 在運作時，我們如何維持上述其中一種 invariant 了\n在我們的程式 (mutator) 做事情時，需要插入一些 collector 的動作\nread barrier\n顧名思義讀資料的時候插入動作，防止 mutator 去讀白色的 obj\n讀白色 obj 時就把它標成灰色\n例：上面那個 Figure 7 在讀 D 的時候就把 D 塗成灰色\n可以保證上面的 strong invariant\nwrite barrier\nhttps://www.mdeditor.tw/pl/prRu/zh-tw\nhttps://www.mdeditor.tw/pl/pFz3/zh-tw\n寫資料時插入動作，基本的有兩種\nstack太多，如果開write barrier會很慢，通常只開heap，所以以下兩種方法需要再開始或結束時掃一次stack來處理，預防有白色被已掃過的stack指到\na. insert write barrier (dijkstra)\r當 mutator 把黑色指向白色時，會將白色變成灰色，或者把黑色變成灰色 (前者好像比較常見)\r如此一來就符合 strong invariant 了～\rmark結束後要再掃一次stack，因為stack沒開wb，指到的東西可能是白色卻需要存活\rb. delete write barrier (Yuasa)\r若有一個指針指向一個 obj 且該 obj 為白或灰\r當 mutator 此指針拿掉時，該 obj 要被塗成灰色\r一開始需要掃roots，把roots直接指到的都塗灰\r為何需要開始時掃stack-\rhttp://www.yuasa.kuis.kyoto-u.ac.jp/~yuasa/ilc2002/slide.pdf\r因為stack沒開rb，如果有一個root在stack上，若他直接指到某個obj，且這個pointer在這個obj被看到前，就被移除，那此obj可能被另外的黑色指到，就還是白色卻需要存活\r一般來說 write barrier 的效率會比較好，因為 heap 讀比寫多很多\nGenerational GC https://liujiacai.net/blog/2018/08/18/generational-gc/\n分代，大部分的東西剛創建不久就會變成垃圾，所以把年輕和老的物件分開存\n年輕的部分常常做ＧＣ，老的偶爾做\n通常年輕的 obj 多，老的少\nyoung -\u0026gt; old 的 pointer 比 old -\u0026gt; young 多\n不動到 old，獨立回收 young 部分\n考慮到 old -\u0026gt; young 的 pointer\n可以新增 write barrier 做一些操作，當 old 指到 young 時就記住，把被指到的 young 當作 young gc 時的 root 若該 old 是變成垃圾，則 young 會暫時成為實際上是垃圾的 root\n所以下次老 gc 時要清掉\n獨立回收 old\n要考慮到 young -\u0026gt; old 的 pointer\n可以依照上面的方法來做，但通常 young -\u0026gt; old 的 pointer 數量較多\n要記錄的東西會很多，可以直接把所有 young 當作 root set，反正不多 Go GC 一開始適用插入屏障後來改用混合型，結合插入和刪除寫屏障，去除了開頭或結束時的 scan\n介紹怎麼讓 Go gc 更有效率\nhttps://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html\nhttps://golangpiter.com/system/attachments/files/000/001/718/original/GP_2019_-_An_Insight_Into_Go_Garbage_Collection.pdf?1572419303\nhttps://www.mdeditor.tw/pl/prRu/zh-tw\nhttps://www.mdeditor.tw/pl/pFz3/zh-tw\n\u0026quot;\u0026quot; 引用上面文章的段落\n在Go 語言 v1.7 版本之前，使用的是 Dijkstra 插入寫屏障保證強三色不變性，但是執行時並沒有在所有的垃圾收集根物件上開啟插入寫屏障。因為 Go 語言的應用程式可能包含成百上千的 goroutine，而垃圾收集的根物件一般包括全域性變數和棧物件，如果執行時需要在幾百個 goroutine 的棧上都開啟寫屏障，會帶來巨大的額外開銷，所以 Go 團隊在實現上選擇了在標記階段完成時暫停程式、將所有棧物件標記為灰色並重新掃描，在活躍 goroutine 非常多的程式中，重新掃描的過程需要佔用 10 ~ 100ms 的時間\n\u0026quot;\u0026quot;\nhttps://studygolang.com/articles/27243\nHybrid write barrier https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\n可以看 Proposal 和 Reasoning，有介紹為何他們可以消除開頭或結尾的掃描\nwritePointer(slot, ptr):\rshade(*slot)\rif current stack is grey: shade(ptr)\r*slot = ptr 奇怪的例子\rA-\u0026gt;B\rC-\u0026gt;nil\r都在 AC 都在 stack 上，若C黑，AB白\r當變成\rA -\u0026gt; nil\rC -\u0026gt; B\r時，因為是stack 所以wb都不會啟動？\rB會被誤刪嗎 這裡說當混合寫屏障時，在掃描特定一個goroutine上的stack時還是要暫停，一次掃完(所以上面那種情況不會出現)\n但不用像Yuasa一開始把全部stack掃完或是Dijkstra最後重掃\nhttps://liqingqiya.github.io/golang/gc/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%86%99%E5%B1%8F%E9%9A%9C/2020/07/24/gc5.html\n其餘連結\nhttps://mcll.top/2020/04/14/gc-write-barrier/\nhttps://www.bookstack.cn/read/qcrao-Go-Questions/spilt.3.GC-GC.md\n原始碼\nhttps://go.googlesource.com/go/+/go1.7beta2/src/runtime/mbarrier.go\n","description":"讀垃圾回收的筆記，懶的整理RR","id":21,"section":"posts","tags":["Go","Garbage collection","垃圾回收"],"title":"垃圾回收！","uri":"https://markogoodman.github.io/posts/gc/"},{"content":"筆記一下研究 csrf 查到的資料，有些小地方網路上的資料都滿模糊的真的查有夠久 XD\nCSRF (Cross-site request forgery) 基本原理 基本原理就是使用者在某個 A 網站認證(登入帳號之類的)後瀏覽器把 cookie 存下來，攻擊者利用發送 request 會自動帶上 cookie 的這個動作來讓使用者誤發一些奇怪的 request 去某 A 網站。\n舉個簡單的範例\n像是一個網站 XXX 提供 API 可以刪除文章\n/posts/delete?id=12\n但需要使用者用帳號密碼登入後，打這個 API 時在 cookie 中帶著上 session id 才可以刪除\n那壞壞的網站可能會有這樣的東西\n\u0026lt;a href='http://XXX/posts/delete?id=12'\u0026gt;hi\u0026lt;/a\u0026gt;\n如果你之前登入過XXX網站，而且該網站又沒有做好防護，點下去文章就會被砍掉拉\n注意的是攻擊者看不到 cookie 的內容，只是讓受害者無意間使用了。\n網路上有更詳細的介紹這邊就不多講，總之攻擊的方式五花八門，不管是換成POST，或是後端只收 json 之類的方法都是有辦法破解 (但我不會\n這篇主要想記錄一些比較常用的防禦方法的運作\n圖形驗證碼或簡訊密碼 很多比較敏感的操作都會要求這些，因為這類的攻擊無法得到驗證碼資訊所以防禦成功\n但對使用者來說這是一個麻煩\nSamesite cookie 是一種設定 http cookie 可以設定的屬性\n長得像這樣\nSet-Cookie: session_id=123; SameSite=Strict\n基本上可以去設定瀏覽器在跨站的 request 是否會自動帶 cookie。\n設定成不會自動帶的時候就防住了 csrf\n詳細請參考下面連結，裡面有說明了怎樣算是跨站以及不同 SameSite 設定的影響\nhttps://medium.com/@azure820529/chrome-80-%E5%BE%8C%E9%87%9D%E5%B0%8D%E7%AC%AC%E4%B8%89%E6%96%B9-cookie-%E7%9A%84%E8%A6%8F%E5%89%87%E8%AA%BF%E6%95%B4-default-samesite-lax-aaba0bc785a3\n目前很多東西都是前後端分離架在不同地方，這些都是跨站的 request。\n因為 samesite 和 same-origin policy 名字太像了我就順便看了一下同源政策，結果根本是不同的東東。\n同源政策是瀏覽器上的功能，主要做的是 cross-origin 發出請求後，收到回應時瀏覽器會看看同源政策的設定是否 OK，不 OK 的話會把回應擋掉。\n不過實際上 request 還是有發出去，所以要是像前面那種攻擊例子也擋不住\n詳細請參考：https://blog.techbridge.cc/2017/05/20/api-ajax-cors-and-jsonp/\n還有J個 youtube.com/watch?v=KaEj_qZgiKY\u0026amp;ab_channel=LiveOverflow\nSynchronizer Token Pattern 因為 Samesite cookie 不是每個瀏覽器都支援\n我們還是需要別的方法來判別送 req 的到底是不是使用者\nSynchronizer Token Pattern 就是當瀏覽器跟 server 要網頁的時候，server 偷偷在表單中塞一個隨機產生且隱藏的 csrf token，並自己存一份。server 收到 request 時會比對自己存的與收到的是否相同。\n詳細請參考：https://medium.com/cross-site-request-forgery-csrf/synchronizer-token-pattern-63871b4c83ad\n提一下如果沒有同源政策弄好，受害者在惡意網站發了 request，惡意網站可以看到回來的 response，csrf token 就被看到嚕\n前後端分離的狀況下可能要把同源政策設好，只讓合法前端看到 token，前端再送 req 時順便帶上\ndouble submit cookie 這個東西是我查比較久才搞清楚的\n運作流程是\nserver 把 csrf token 設到瀏覽器的 cookie 內，瀏覽器在送 request 時可以把這個 token 從 cookie 拿出來，放到 header (或是 form 裡面都可以) 裡面一起送給 server。\n這時進行了 csrf 攻擊，這個攻擊 request 其實還是會帶著 cookie 的 (如果沒設 samesite 的話拉)，但 csrf 攻擊者是看不到 cookie 的，自然也就不知道要放什麼 token 到 header 裡面，認證失敗。\n詳細：https://medium.com/cross-site-request-forgery-csrf/double-submit-cookie-pattern-65bb71d80d9f\n還有其他的變形像是讓前端自己產生 token 放到 cookie 裡面也行 (送req時放到 header 一起帶去 server)\n總之只要攻擊者無法讀到 cookie，他就無法在 header 放入一樣的東西\n這種方法 server 不用存 token\n前後端分離且在完全不同網域的情況下，把 token 從 cookie 拿出來這段可能會有困難，或許可以在第一次收到 token 時就存在 local storage 之類的地方，但也感覺很容易被盜\n差不多寫完了，我也不知道為啥我沒在寫網頁還研究這個\n","description":"筆記一下 csrf","id":22,"section":"posts","tags":["csrf"],"title":"Csrf","uri":"https://markogoodman.github.io/posts/csrf/"},{"content":"Part III: Examples Goroutine 的運作原理其實在前面兩篇就有很好的解釋拉\n這個部分主要是給一些實際的例子，會介紹一下平行與並行適合的是哪些情況，Goroutine 到底什麼時候開多才可以讓處理事情的效率增加\n接下來簡短回憶一下 Part I 寫過的這兩個重點\n1. Parallelism vs Concurrency Parallelism 平行\n把不同的任務放到不同的 core 上執行，同一個時間點同時執行不同的任務，需要有多於一個的 core 才可辦到，基本上效能會比只使用一個 core 好，除非有什麼互相卡資源的問題，等等的範例會介紹一些例外 -\u0026gt; 如下圖的 G1、G2 就是平行執行\nConcurrency 並行\n同一時間點執行一個任務，不同的任務必須輪流(順序不確定)，且結果與依序(sequential)執行所有任務相同，然後希望效能會比依序執行來得好 -\u0026gt; 如下圖的 G1 G4 G6 與 G2 G3 G5\n2. CPU Bound 與 I/O Bound 的任務類型 CPU Bound:\nI/O 設備效能相對好。需要 core 的計算能力更強，或著更多 core 才可以有效加速這一些任務們的執行\n此種類型大部分 CPU 使用率都接近滿載\nI/O Bound:\n任務們被 CPU 執行到一半常常需要等待一些東西 (讀檔案、網路傳輸資料)，才能再回來讓 CPU 繼續計算，如果網路傳輸速度或是硬碟讀取速度加快，可以大幅加速這些任務\nCPU 的 Loading 大部分不高\n在 Part I 講解了 context switch 在 I/O Bound 的任務上是好的，需要等待 I/O 時先把 CPU 讓給其他人使用，等到 I/O 結果時再回到佇列中等待被 CPU 計算，避免 CPU 空閒沒事做。\n在 Part II 時我們講解了 Goroutine 相比普通的 OS thread 優勢是在它的記憶體用量與 context switch 的速度。\n所以我們知道 Goroutine 應該會適合用在 I/O Bound 的任務上，\n文章內也舉了幾個實際的例子，我就不把全部都 copy 過來了，下面簡單的介紹一下\nAdding numbers 程式碼在 Listing 1 內，目的是要把一個陣列裡的數字全部加起來，任務都是需要做計算，沒有等待 I/O\n分為兩種做法\n一種是 sequential 的方法，用一個 Gorutine 把全部加起來 第二種是 concurrent，有八個 Goroutine 各自把八分之一的陣列加總，最後再加在一起 Listing 4 列出來使用一個 core 的結果，顯然 sequential 方法較快，\n在只有一個 core 的情況下，這種 CPU Bound 的任務，多了一些 Goroutine 只是會發生多餘的 context switch 導致總時間拉長而已。\nListing 5 列出了使用八個 core 的結果，每個 core 上各有一個 Goroutine，計算能力大幅增加，速度應該會提升\n實際上速度快了 40% 左右，為什麼不是八倍呢?\n大概是程式一開始只有一個 thread，要去開啟剩下七個 os thread 跑在不同的 core 上造成的延遲\n可以看到 BenchmarkConcurrent-8 3362643 ns/op，每加總一次也才花了 3ms 左右，thread 的創建搞不好就佔了很大一部分，如果把數字陣列的長度拉大，計算時間更久，效能應該會更接近八倍才對\nReading Files 文章中在這部分給的範例我覺得滿奇怪的\n在一個 Core 上使用多個 Goroutine (thread) 的優勢是發現 G 被 block 住時可以讓出 CPU，盡可能使用 CPU，使 I/O 等待與 CPU 運算可以同時進行\n但範例中使用 time.Sleep 來模擬讀檔案\n他這樣的做法應該可以使 CPU 運算部分 (strings.Contains 與其他 overhead) 和 sleep 部分同時進行沒錯，可能有達到一些加速的效果\n但正常來說讀檔案是會互相干擾的，例如在只有一個硬碟的狀況下，Goroutine 再多，硬碟的效能也是相同，大家應該要排隊讀，但 Sleep 的狀況卻是這些 Goroutine 可以一起在睡著的狀態(模擬了同時讀檔案)\nListing 14 使用了 1 個 core 出來的結果，大部分其實就是 8 個 Goroutine 可以同時睡眠的加速而已\nListing 15 發現 8 core + 8 G 出來發現沒有加速(相比 1C 8G)，我推測應該是原本運算的地方就佔比不高了，一個 core 就夠處理運算部分，大部分都在等 sleep，所以多了幾個 core，只用了 8 個 Goroutine，能同時睡著的 G 也只有 8 個，等的時間還是差不多\n下面用一張圖來解釋在何種情況下開多個 Goroutine 或者使用更多 Core 能夠增加效能，縮短任務完成時間。\n下圖假設\n四種情況都有三組任務需要執行。每組都需要 core 計算 10 秒，接著 I/O 5 秒，兩者順序不能調換 不計算 context switch 所花費的時間 不同 Core 可以同時執行計算 (CPU - 10 那個部分) I/O 是大家共用，同時只能有一組 I/O 執行，在做完一組 I/O 之前不會切到另一組 第 (1) 組是序列執行，沒有 context switch ，I/O 時完全無法利用 core 來做事，很慢\n第 (2) 組讓一個 core 上跑了 3 個 G，G1 被 I/O block 住時，G2的計算可以先開始，比 (1) 的效率好點，但 I/O 還是有空窗期\n第 (3) 組就是多了一個 core，可以看到在 10 秒之後 I/O 都被排滿了，都有有效利用\n第 (4) 組再多了一個 core 執行時間並無縮短，因為 10 之後 I/O 都排滿了，而且多幾個 core 也無法縮短一開始的計算時間 10 秒 (除非可以把計算拆小但此處不考慮)\n上圖其實比較像是在解釋平行與並行在這種任務類型下的表現，因為其實把 Goroutine 看成 thread 也說得通。在 Part II 時最後有說過，其實他們切換的流程看起來是類似的，只是 Goroutine context switch 的速度快多了，資源消耗也少點。\n這系列的文章的內容差不多就是這樣了\n總結一下第一章介紹了 Go scheduler 需要的基本 OS 知識\n第二章再透過簡單的圖片來看看 Go scheduler 運作的方式\n最後以例子來看看不同的任務類型在 Concurrency 與 Parallelism 上面運作情況\n了解了這些之後，如果遇到自己的程式有效能上的問題，或許就能知道從哪裡著手分析，看是要多開幾個 Goroutine 還是多租幾個 CPU 來用啦～\n","description":"系列文之三， 一些實際的例子，還有並行與平行~","id":23,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Scheduling in Go Part III","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-iii/"},{"content":"https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html\n以下截圖都是從這個網站來的\nPart II: Go Scheduler 這部分要開始介紹 Go 的 Scheduler 是怎麼運作的，以及它的優勢在哪裡\n在 Go 裡面執行 runtime.NumCPU() 可以知道目前的電腦有幾個 virtual core (看有幾個cpu、各有幾個核心、核心上有幾個hardware thread)，這也是你的電腦最多可以平行執行的任務數量 (Part I 提到過)。\n用 runtime.GOMAXPROCS(n) 可以設定 Go 給予 n 個 \u0026lsquo;P\u0026rsquo;, logical processor，設定 Go 可以\u0026rsquo;邏輯上\u0026rsquo;平行執行的任務數量 (也就是要有幾個 active thread)。\n當 P 的數量大於前面 runtime.NumCPU() 數量，這些 P 就可能要常常交換使用 core 造成 context switch 負擔。\n如Part I所說，如果執行中的 thread 太少就無法利用全部的 core，太多的話這些 thread 會需要做 context switch 減慢速度。所以 Go 就把預設值設為 runtime.NumCPU()的數量。\n但在很少數的情況下 GOMAXPROCS 設高反而可以增加一點效能，參考 https://colobu.com/2017/10/11/interesting-things-about-GOMAXPROCS/\nGo Scheduler 是使用一個叫做 GMP model 的東西包含以下三個元件\nG: Goroutine, 一些要做的任務\nM: Machine thread, 就是 OS 層級的 thread\nP: Logical processor, 可以把他想成任務執行器，管理自己的 queue (Local run queue)，裡面有很多 G 待做\n除此之外還有一個 Global run queue 存放孤兒 Goroutine\n要執行任務計算(G)時，P 必須與一個 M 接在一起，並使用一個 core 來執行如下圖 (夾在 M 和 P 中間的就是執行中的 G)\nGoroutine states, context switch 與 thread 相同，也有 等待(例如等IO)、可執行、執行中三個狀態\n執行中代表上圖中 G 在 M 與 P 之間\n可執行的 G 就會在 local run queue 或 global run queue 排隊等著被執行\n等待狀態的 G 需要等待某些資源才能繼續被執行，可能會在 Network poller 中等待非同步的動作，或著與 M 一起被 block 住後兩個一起去旁邊等待， 下面的 3. 4點就會出現這個狀態的 G\nGoroutine 的 context switch 將 G 從執行中換下來, 把其他 G 換上來執行，但這只發生在必要的時刻，經常做只會多花時間\n可能發生的情況有幾種\n產生新的 goroutine\ngo func()\u0026hellip; 有可能會c ontext switch，但不一定\nGarbage collection\nGC 有自己的 Goroutine 要做也需要 M, 所以要把其他G換下來來執行收垃圾動作\nSynchronization and Orchestration\n例如等待 channel 的資料被 block 的時候, 或者是 mutext 會把 goroutine 卡住的動作都可能會有 context switch\nsystem call\n分為 asynchronous 與 synchronous system call\n非同步的狀況，例如大部分 OS 中 Networking-based system calls，這種情況 G 可以從 M 和 P 上拔下來，去 Network poller 中等待結果 (依靠 OS 的非同步 IO 達成，例如 epoll)，過程可以參考網頁內 Figure 3,4,5 ，G 得到結果之後再把 G 塞回 queue 中等待執行。期間 M 與 P 可以繼續消化其他 G。\n同步的 system call 例如某些 File-based system calls，會把整個 M block 住 (OS 無法非同步的處理這類型的 system call)，這時做這個呼叫的 G 會和 M 一起被放去旁邊等待結果 (thread M 會被從 core 上 context switch 下來，效率比前一種差)，不佔用 cpu 資源，流程參考網頁內 Figure 6,7,8，而 P 會取得一條新的 M (找 idle 的或新創一條) 來執行其他 G。當旁邊等待的 GM 得到結果後 G 會被塞回原本的queue中， M 則是變成idel等其他人用，需要時就不必新建 (正確來說是變成 spinning thread，主動去找孤兒P配對，也會使用到 cpu 資源，但通常比起刪掉重創來的划算，參考 https://zhuanlan.zhihu.com/p/42057783)。\nWork Stealing 簡單來說就是 P 管理的 queue 中的 G 可以在不同 P 的 queue 之間移動\n當一個 P 發現自己沒有 G 可以做了，有可能會去 Global run queue 撿工作，也可能去別的 P 搶工作來做。\n當然這也要花時間，太過頻繁的搬動 G 也不好\n最後作者給了一個範例\n總體來說 goroutine 切換和 thread 之間的切換很類似，但 goroutine 的 size 小，所以可以創造很多個，context switch 速度也快很多，相比以往每個任務都開啟一條 thread，也省去了許多 thread 的產生銷毀耗費，所以速度與資源使用才會比較好\n下面這張圖是一個韓國人的投影片其中的一頁\nhttps://www.slideshare.net/Hyejong/golang-restful\n總結了 Goroutine 與 thread 資源的差距\nPS. context switch 時間約是 200ns 與 1000ns，5倍的差距\n","description":"系列文之二， Goroutine 的是怎麼調度的與 GMP model","id":24,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Scheduling in Go Part II","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-ii/"},{"content":"一開始用 Go 的 Map 時常常搞不清楚到底要不要傳指標，好像大部分的時候都不用，但用到 unmarshal 之類的 function 就又要傳指標進去了。\n這篇就來研究一下 Go 的 Map 到底是什麼生物\n先看看下面這段程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // mv = map value func ModifyMap(mv map[int]int) { mv[1] = 1 } // mp = map pointer func ModifyMapByPointer(mp *map[int]int) { (*mp)[1] = 1 } func main() { m1 := make(map[int]int) ModifyMap(m1) fmt.Println(m1) // map[1:1] m2 := make(map[int]int) ModifyMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] } 兩個 function 都改到 map 裡面的值了，所以用不用指標到底有沒有差呢?\n再看看下面這段程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func MakeMap(mv map[int]int) { mv = make(map[int]int) mv[1] = 1 } func MakeMapByPointer(mp *map[int]int) { (*mp) = make(map[int]int) (*mp)[1] = 1 } func main() { var m1 map[int]int MakeMap(m1) fmt.Println(m1) // map[] var m2 map[int]int MakeMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] } 這次的 MakeMap 的操作就沒有反應到 main 傳進去的 m1 上，這是怎麼回事呢？\n首先當我們使用 make 產生 map 時，它就是回傳一個 pointer 給我們。也就是說當我們呼叫了 m := make(map[int]int)，m 這個變數內存的其實是一個地址，指到真正的 map struct。\n來看看第一組程式碼發生了什麼事\n簡單來說就是 m1 把 map 的地址 x 傳進去 ModifyMap，mv 內就存著相同的值 x，所以可以改到真正的 map struct。\n1 2 3 4 5 6 7 func ModifyMap(mv map[int]int) { mv[1] = 1 } m1 := make(map[int]int) ModifyMap(m1) fmt.Println(m1) // map[1:1] 下面這部分，是把 m2 本身的位置 y 傳進 ModifyMapByPointer， mp 就存著 m2 的位置 y。\n接著透過指標改到了 map struct\n1 2 3 4 5 6 7 func ModifyMapByPointer(mp *map[int]int) { (*mp)[1] = 1 } m2 := make(map[int]int) ModifyMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] 再來看看在 function 裡面 make map 會怎麼樣\n這裡 m1 一開始是 nil，傳進 MakeMap 的東西也是 nil，自然 mv 內也會存著 nil。\n接著呼叫了 make，mv = make(map[int]int)把真正 map struct 的地址存到了 mv 內，mv[1] = 1也是真的有改到 map 裡面的值，只是外面的 m1 內存的值還是 nil，所以我們 print 出來的也就是空 map 了。\n1 2 3 4 5 6 7 8 func MakeMap(mv map[int]int) { mv = make(map[int]int) mv[1] = 1 } var m1 map[int]int MakeMap(m1) fmt.Println(m1) // map[] 最後一部分的範例，m2 的地址 y 傳進了 MakeMapByPointer，mp 內存著的是 m2 的地址，當呼叫(*mp) = make(map[int]int)時所代表的是產生一個 map struct，並把該地址 x 存進 mp 所指到的變數內，也就是 m2 的值中，所以後續的更改我們也可以從 m2 看到拉\n1 2 3 4 5 6 7 8 func MakeMapByPointer(mp *map[int]int) { (*mp) = make(map[int]int) (*mp)[1] = 1 } var m2 map[int]int MakeMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] 至於真正的 map struct 是一個叫做 hmap 的東西，這邊就不講那麼細，剩下的上網估狗 Go makemap hmap 之類的關鍵字都可以找到嚕\n","description":"一開始用 Go 的 Map 時常常搞不清楚到底要不要傳指標，好像大部分的時候都不用，但用到 unmarshal 之類的 function 就又要傳指標進去了??","id":25,"section":"posts","tags":["Go","map","pointer"],"title":"Go 的 MAP 要不要用指標","uri":"https://markogoodman.github.io/posts/go-map/"},{"content":"關於這系列文 之前看了 Ardan labs 寫的下面這個系列文還滿不錯的\nhttps://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html\n主要是在講 Goroutine 和一般我們看到的 thread 比起來到底厲害在哪裡，也舉了一些範例讓讀者知道 Goroutine 用在哪裡才是正確的\n這系列文章就是拿來記個重點XD\n他的系列文分為三篇，我也就分三篇記\nPart I: OS Scheduler 這個章節不講 Goroutine 講一些 OS 要知道的預備知識，也不會涉及太多細節。\nProcess、Thread、Coroutine 下圖是 Learning Concurrency in Kotlin 這本書裡面的一張圖，解釋了 Process、Thread、Coroutine 的關係。\nProcess Process 是 OS 分配資源的基本單位 (記憶體之類的)\n簡單來說把一個程式 (program) 跑起來就是一個 process，一開始 process 內會有一條初始的 thread 來跑主要程式，這條 thread 可以建立出更多 thread，每個 process 裡面可以裝一堆 thread。\nThread Thread (kernel thread) 是 OS 分配 CPU 執行時間的單位，只能存在於 process 內，常被叫做 light-weight process。\n在大多我們使用的作業系統中，thread 的排程都是 preemptive (可搶佔)\u0008，代表著一條 thread 就算現在在 CPU 上執行，隨時也可能被各種原因換下來，基本上都是由 OS 來管理的。\nCPU 從正在執行 thread A 的任務鍾，換到執行 thread B 的任務，這個行為叫做 context switch。 Thread 之間有哪些共用與不共用的東西這邊就不多講網路上很多文章可以查到。\n提供兩篇文章參考\nhttps://medium.com/@yovan/os-process-thread-user-kernel-%E7%AD%86%E8%A8%98-aa6e04d35002\nhttps://www.itread01.com/content/1546525452.html\n大學上的 OS 課程大概就是介紹了上面那些東西而已。\nCoroutine 再來介紹的 coroutine (goroutine 就是一種)，基本上作業系統不知道有這個東西的存在，OS 只負責執行 thread 上面的一堆任務而已。coroutine 都是由我們使用的程式語言來處理，而每個程式語言提供的 coroutine 運作方式都有些不同。\ncoroutine 也叫做 light-weight thread 或者是 user-level thread，是跑在 kernel thread 上的一堆東西。\n基本上 coroutine 做的事情就是可以讓 function 執行到一半中斷，把 CPU 時間讓給別的 coroutine 來執行他們的工作\u0008\n舉個簡單的應用例子，當 Coroutine A 做事情做到一半，發現要等待讀取檔案或是等一些網路傳輸的資料才可以執行下一步，而這些工作又是不需要 CPU 的，那 coroutine A 就可以把 CPU 的時間讓給下一個 coroutine B，等待需要的東西回來之後再從中斷點繼續工作。\n與 thread 相比， coroutine 的 size 更小，像是在 Go 創建一個新的 goroutine 就只花費幾 KB，一條 java thread 卻要到 1~2 MB。\n而上面那種切換 coroutine 的行為其實是種 coroutine context switch。如果 coroutine A 與 coroutine B 是在同一個 thread，切換時時 OS 並不會發現，也不會出現 thread 的 context switch。\nCoroutine 的 context switch 與 thread 的 比起來速度也快很多，要儲存與載入的資料量相差很大。\n這部分之後講 goroutine 時會再提到。\n越多 Thread 真的會讓程式跑越快嗎 ? 答案是不會\n這部分先不討論 coroutine，單純就 multi-thread 的程式來說明\n進入正題前必須先理解，一個 thread 會做的工作主要分為這兩種\nCPU-bound\n指的是 CPU 大部分時間都很忙的工作類型。譬如計算一個超大 int 陣列的總和，CPU 就是一直做加法，超忙。\nIO-bound\nCPU 常常會沒事做，可能都在等著硬碟讀完檔案的通知，或者網路傳輸結束的通知等等。\n簡單的例子像是把一堆檔案一個一個讀進來，做一點點修改再寫回去，就會花很多時間在讀寫檔案，CPU 沒什麼事情好做。\n基本上 context switch 在 IO-bound 的工作上是好的，當一個任務需要等待 IO 時，便換下來讓需要 CPU 的人用，避免讓 CPU idle。\n在 CPU-bound 的工作上 context switch 需要多花時間，只會拖慢完成全部任務的時間而已，但有時為了讓使用者覺得每個任務都有在進行，context switch 是必須的。\n回來講 thread 數量的問題\n當機器有 n 個 core，代表同時最多也只能執行 n 個 thread，若 thread 數量少於 n 的話，必然會有 core 沒事情做，顯然是不好。\n如果 thread 數量大於 n 太多，那就會常常在執行 context switch 而浪費時間，且 thread 會佔用不少記憶體。\n要取得平衡必須知道自己的系統是在做 CPU-bound 還是 IO-bound 的任務多，前者 thread 少點 (但還是要 \u0026gt;= n)，後者 thread 多點。\n當寫一個有 database 的 web service，最常使用的 thread 數量是 n * 3，就是一個大家的經驗法則而已。\n總結，我們希望每個 CPU 無時無刻都在工作，才能有最大的產出\n當一個 thread 等待 IO 時就把他換下來，讓 CPU 繼續工作\n當一個 thread 正在被 CPU 執行時，我們就盡量不要打擾他\n接著 Part II 會來介紹 Go 的 scheduler 是怎麼運作的，他為什麼讓 Goroutine 比 thread 還有效率\n","description":"系列文之一，Ardan labs 的 Scheduling in Go 系列文說明了 Goroutine 到底哪裡厲害，以及適合的使用情境，這篇文章就是要記錄一下重點之後忘記就可以直接看了","id":26,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Goroutine 為啥那麼快 (Scheduling in Go) Part I","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-i/"},{"content":"前幾天幫原本已經有資料的 collection 新建 unique index 的時候讓 Server 啟動爆掉了記錄一下\n1 2 3 db.coll.insert({\u0026#34;a\u0026#34;:1}) db.coll.insert({\u0026#34;a\u0026#34;:2}) db.coll.createIndex({\u0026#34;b\u0026#34;:1}, {unique: true}) // duplicate key error 在一個之前資料都沒有的 field 上建 unique index，結果噴出了下面的錯誤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;operationTime\u0026#34; : Timestamp(1609329527, 19822385), \u0026#34;ok\u0026#34; : 0, \u0026#34;errmsg\u0026#34; : \u0026#34;E11000 duplicate key error collection: test.coll index: b_1 dup key: { b: null }\u0026#34;, \u0026#34;code\u0026#34; : 11000, \u0026#34;codeName\u0026#34; : \u0026#34;DuplicateKey\u0026#34;, \u0026#34;keyPattern\u0026#34; : { \u0026#34;b\u0026#34; : 1 }, \u0026#34;keyValue\u0026#34; : { \u0026#34;b\u0026#34; : null }, \u0026#34;$clusterTime\u0026#34; : { \u0026#34;clusterTime\u0026#34; : Timestamp(1609329527, 19822385), \u0026#34;signature\u0026#34; : { \u0026#34;hash\u0026#34; : BinData(0,\u0026#34;AAAAAAAAAAAAAAAAAAAAAAAAAAA=\u0026#34;), \u0026#34;keyId\u0026#34; : NumberLong(0) } } } errmsg內大致是在說在 b_1 這個 index (幫 b 建立 index 時候 MongoDB 幫取的名字)上，發生了 b = null 的 duplicate key。因為前面塞進去的兩筆都沒有 b ，建立 index 時 MongoDB 自動把他們視為 null 值，所以兩筆資料就撞在一起了。\n此時比較正常的解法是跑程式做資料庫的 schema migration，把之前的資料都補上該 field 的值。\n但如果新加的 key 本來就不想與以前資料相容該怎辦哩?\n有下面兩個解決方法\n第一個是已經過時的用法，官方建議 mongodb 3.2版之後就建議使用第二種方式取代。\nsparse: true 可以讓 MongoDB 建 index 時直接忽略沒有這個 field 的資料(不像上面把 field 的值當作 null )，所以就不會有 duplicate key error了。 1 db.coll.createIndex({\u0026#34;b\u0026#34;:1}, {unique: true, sparse: true}) 第二種是使用 partial index\nPartial index 指的是用某些條件去決定是否要在該筆資料上建 index，下面這個例子就使用了 $exists: true 來過濾，只有資料內存在 b 這個 field 的資料，我們才在 b 上面建立 unique index。 1 2 3 4 5 6 7 8 9 db.coll.createIndex( {\u0026#34;b\u0026#34;:1}, { unique: true, partialFilterExpression:{ \u0026#34;b\u0026#34;: {$exists: true} } } ) PartialFilterExpression 後面也可以接很多不同的條件\n例如 \u0026ldquo;b\u0026rdquo;: {$gt: 1} ，就可以過濾出那些 a field 大於 1 的資料並在這些資料上建 index。\n差不多就阿捏\n","description":"前幾天幫原本已經有資料的 collection 新建 unique index 的時候讓 Server 啟動爆掉了記錄一下","id":27,"section":"posts","tags":["mongodb","index","database"],"title":"Mongodb Partial and Sparse Index","uri":"https://markogoodman.github.io/posts/mongodb-partial-sparse-index/"},{"content":"誒\u0026hellip;\n","description":"","id":28,"section":"","tags":null,"title":"About","uri":"https://markogoodman.github.io/about/"},{"content":"汪\n","description":"","id":29,"section":"posts","tags":null,"title":"My First Post","uri":"https://markogoodman.github.io/posts/my-first-post/"}]