[{"content":"一直都聽說 Graphql 很潮(? 工作上其實也有遇到類似的問題，一個玩家的資料有一堆欄位，常常因為各種需求加了一堆參數或 API 來拿裡面的不同欄位，graphql 也有被提到說是一個好選擇，不過現在伺服器都用 restful 寫好好的要搬也是一個大工程，只好自己來研究一下 XD\nhttps://ithelp.ithome.com.tw/articles/10200678 \u0026lt;- 不知道 graphql 是什麼可以參考這個，或是去官網看～\n這東西網路上也很多介紹了就不多講，大致上就是用一個 endpoint 就可以讓 client server 之間很有彈性的選擇想要傳輸的資料，很像我們在下資料庫指令。\n下面只是玩玩看 gqlgen 這個 Go 的 graphql 套件而已，總之又是一篇雜亂的文章\ngqlgen 官方範例在此 https://gqlgen.com/getting-started/\n首先進去會教你安裝\n然後跑 go run github.com/99designs/gqlgen init 會出現他的範例\n還包含一個 playground GUI 讓我們方便下 query\ngraph/generated 裡面的東西基本上就是那些拆解 query 之類的程式碼， gqlgen 都幫我們產生好拉\nschema (與 client 的溝通格式) 就是寫在 graph/schema.graphqls 內，從 gqlgen.yml 裡面可以知道也可以分放在多個檔案內\nschema: - graph/*.graphqls graph/model 內是 gqlgen 依照 schema 產生的 go struct\n如果希望strcut與schema不同時，就要在gqlgem.yml的models那邊先宣告(Type Mapping)，這樣他就不會幫你gen出來\n參考gqlgen workshop的\nmodels: User: model: github.com/99designs/gqlgen-workshop/db.User 或者是直接在autobind那個路徑裡面先自己宣告也行，就不會產生\n注意的是如果struct都import來的gqlgen會沒有產生出任何graph/model裡面的檔案導致 module mode l不存在\ngen的時候會跑出一堆錯誤\nvalidation failed: packages.Load: -: malformed module path \u0026quot;gqlgen-workshop/graph/model\u0026quot;: missing dot in first path element\n猜大概是找不到model這個module\n亂改了yml發現把 autobind 還有 model 這兩個註解掉就ＯＫ了\nImplement the resolvers 註解裡面有寫，其實就是把需要的東西放進去例如db\ntype Resolver struct{ todos []*model.Todo } 接著要實作 schema.resolvers.go 裡面 CreateTodo 和 Todos，這兩個 func 也是 gqlgen 依照 schema 產生的，應該滿好理解\nCreateTodo 的 input 進來我們就處理 input 存進 db\nTodos 就是回傳 db 內容這樣\n回傳對應的 struct 後 gqlgen 產生出來的 code (generated.go) 會幫我們處理剩下的事情，譬如把 client 不需要的 field 拿掉之類的\n接著就可以 go run server.go 去上面玩玩看了\n再來範例上自己寫了一個 Todo 的 struct 而不是用 gen 出來的\ntype Todo struct { ID string `json:\u0026quot;id\u0026quot;` Text string `json:\u0026quot;text\u0026quot;` Done bool `json:\u0026quot;done\u0026quot;` UserID string `json:\u0026quot;user\u0026quot;` } 此時再跑一次 go run github.com/99designs/gqlgen generate 會發現 schem.resolvers.go 多了一個 (r *todoResolver) User\n這是因為修改之後 Todo struct 與 schema 上的 Todo 對不起來 (UserID: User)，所以我們在把 struct Todo 轉換為 schema Todo 時，要給他一個方法，用 struct Todo 裡面的資料組成 schema Todo，詳情請參考，詳情請參考 generated 裡面產生出來的 code\n\tctx = graphql.WithFieldContext(ctx, fc) resTmp, err := ec.ResolverMiddleware(ctx, func(rctx context.Context) (interface{}, error) { ctx = rctx // use context from middleware stack in children return ec.resolvers.Todo().User(rctx, obj) }) 基本範例到這邊差不多就結束了～\n下面還有很多 reference 可以去看看，下面只寫了我有看的\nFieldCollection https://gqlgen.com/reference/field-collection/\n前面的範例是 client query 來了，程式去 db 撈對應的資料，然後交給 gqlgen 去把不必要的欄位篩掉再傳回給 client，field collection 是要讓你在撈 db 前先知道到底有哪些 field 是要用的，撈 db 時可針對去撈，避免浪費資源\nCollectAllFields 會直接回傳一個 string slice 包含 query 最上層的 field 名稱\nCollectFieldsCtx 會回傳一個 CollectedField 的 slice，CollectedField 是一個 struct 裡面包含該 field 的一些資訊(名稱之類的)，還有該 field 的子 field 資訊\n下面改寫一下func印出相關資訊\nfunc (r *queryResolver) Todos(ctx context.Context) ([]*model.Todo, error) { fmt.Printf(\u0026quot;CollectAllFields: %+v \\n\\n\u0026quot;, graphql.CollectAllFields(ctx)) fmt.Printf(\u0026quot;CollectFieldsCtx: \\n\u0026quot;) for _, field := range graphql.CollectFieldsCtx(ctx, nil) { fmt.Printf(\u0026quot; %+v\\n\u0026quot;, field.Name) for _, sel := range field.Selections { fmt.Printf(\u0026quot; %+v\\n\u0026quot;, sel) } } return r.todos, nil } CollectAllFields: [id text done user] CollectFieldsCtx: id text done user \u0026amp;{Alias:name Name:name Arguments:[] Directives:[] SelectionSet:[] Position:0xc00011df18 Definition:0xc00011b0a0 ObjectDefinition:0xc000108b40} \u0026amp;{Alias:id Name:id Arguments:[] Directives:[] SelectionSet:[] Position:0xc00011df58 Definition:0xc00011b030 ObjectDefinition:0xc000108b40} 發 query 後可以看到 CollectAllFields 回傳了上層的 field name， user 裡面只要至少有一個 field 就會回傳 user\n而 graphql.CollectFieldsCtx 裡面的各個 field 還有 Selections，這就是包含了子 field 的資訊，Selection 是 interface，這邊只試印出來，實際上要用 CollectFields，裡面會去轉型。\n因為 field 無法預測到底有幾層，實際上是要用範例中遞迴的方式去解開\nQuery complexity 因為 filed 底下有可能又有 field 又有 field，例如下面\ntype User { friends: [User!]! } 可能 query 出一坨東西然後爆炸\n所以需要某些方法來限制 query 的量\n下面先修改原本的例子 schema 和 model 並 go generete ./...\n---- graph/schema.graphqls type Todo { id: ID! text: String! done: Boolean! user: User! related(count: Int!): [Todo!]! # count 用來限制回傳的 related todo 數量 } input Relation { a: String! b: String! } type Mutation { createTodo(input: NewTodo!): Todo! addRelated(input: Relation!): Todo! } ---- graph/model/todo.go type Todo struct { ID string `json:\u0026quot;id\u0026quot;` Text string `json:\u0026quot;text\u0026quot;` Done bool `json:\u0026quot;done\u0026quot;` UserID string `json:\u0026quot;user\u0026quot;` Related []string `json:\u0026quot;related\u0026quot;` } 然後把 schema.resolvers.go 實作一下\naddRelated 就是把 b 加到 a.related\nfunc (r *mutationResolver) AddRelated(ctx context.Context, input model.Relation) (*model.Todo, error) { var a, b *model.Todo for _, todo := range r.todos { if todo.ID == input.A { a = todo } else if todo.ID == input.B { b = todo } } if a == nil || b == nil { return nil, errors.New(\u0026quot;QQ\u0026quot;) } a.Related = append(a.Related, b.ID) return a, nil } func (r *todoResolver) Related(ctx context.Context, obj *model.Todo, count int) ([]*model.Todo, error) { todos := []*model.Todo{} i := 0 for _, rid := range obj.Related { for _, todo := range r.todos { if rid == todo.ID { todos = append(todos, todo) i++ if i \u0026gt;= count { break } } } } return todos, nil } ---- server.go 加入 srv.Use(extension.FixedComplexityLimit(5)) 這時如果我們下了 query\nquery getTodos { todos { id text done related (count:1){ id text } } } 會出現 message\u0026rdquo;: \u0026ldquo;operation has complexity 7, which exceeds the limit of 5\u0026rdquo;\n這是因為預設一個 field 和一層深度都會 complexity + 1\n這邊共有 7 個 complexity (todos, id, text, done, related, id, text)\n但也可以發現count不管改成多少complexity都不會變\n我們可能會想要限制related的數量\n這時可使用 custom complexity Calculation\n把 server.go 改成\nc := generated.Config{Resolvers: \u0026amp;graph.Resolver{}} countComplexity := func(childComplexity, count int) int { fmt.Println(\u0026quot;childComplexity: \u0026quot;, childComplexity, \u0026quot;, count: \u0026quot;, count) return count * childComplexity } c.Complexity.Todo.Related = countComplexity srv := handler.NewDefaultServer(generated.NewExecutableSchema(c)) 修改的 struct是graph/generated/generated.go 裡面的 ComplexityRoot struct 計算複雜度的 function\n接著跑 query\nquery getTodos { todos { id text done related (count:2){ id related (count:3){ id\ttext done } } } } 會得到\noperation has complexity 24, which exceeds the limit of 5\u0026quot;, childComplexity: 3 , count: 3 childComplexity: 10 , count: 2 ＃上面使用 countComplexity func 計算複雜度 = childComplexity * count\n看起來是用遞迴先跑到最內層的 related (count:3)，他的 childComplexity是3 (filed數量)，算出來等於 9\n再來 related (count:2) 的 childComplexity 是 10 (id + related (count:3)的複雜度)，複雜度20\n最後再加 todos, id, text, done = 24\n總結一下，在 query 來的時候 gqlgen 就會幫我們把複雜度算好了，可以自己替換算 complexity 的 func，如果超過我們設定的複雜度限制就會不給他過，避免有人亂打一堆奇怪的 query，把頻寬或程式弄爆\n這次看下來其實感覺 graphql 優勢還滿明顯的，不過應該也有很多坑還沒研究到，像是 restful 就不會有 query complexity 這種東西要處理，而且要用這種新的東西公司內要用感覺也要一陣子研究才能上手\n這次的範例程式碼放在下面的 repo 記錄一下，不然之後要用的時候八成也忘記怎麼用了\nhttps://github.com/Markogoodman/gqltest\n","description":"gqlgen! 簡單紀錄一下","id":0,"section":"posts","tags":["Go","gqlgen","graphql"],"title":"Gqlgen","uri":"https://markogoodman.github.io/posts/gqlgen/"},{"content":"tags: Golang 垃圾回收～～～  追蹤heap裡面的mem 刪除沒在用的 保留有在用的  主要有分兩大類\n一. Reference counting\n每個 obj 記住自己有被多少人連結到，歸零的時候順便回收\n例如：\na = Person(\u0026quot;A\u0026quot;) // new 一個 Person 的 instance，這個 instance 的 ref count = 1 b = a // b 也去指相同的東西 ref count 再 +1 b = None // ref count -1 a = None // ref count -1 變成 0，記憶體釋放 \n問題：\ncircular references\na = Person(\u0026quot;A\u0026quot;) b = Person(\u0026quot;B\u0026quot;) a.friend = b b.friend = a a = None b = None 雖然已經沒有任何變數直接指到新建的兩個 instance，，但實際上兩個 instance 還是互相 reference，讓 ref count 不會歸零。\n所以只看 ref count 來回收還是有可能 memory leak\nhttps://zhuanlan.zhihu.com/p/101781276\n主要有兩種解決法\n  讓寫程式的人處理\n譬如在 a.friend = b 時，指定這個是 weak pointer，讓它不列入 ref count 的計算。\n好處是程式語言除了 reference counting 就不用做其他事了\n壞處是碼農們沒寫好還是會 mem leak\n  讓電腦做\n加入一些 tracing GC 來輔助處理這些 circular references 的問題\n下面會再講這是啥～\n好壞處就和上面相反\n  總結計數的方法\n優點：好做，計數歸零就釋放，不會像 tracing GC 一樣，要找個時間來做 GC 讓程式暫停一陣子，\n缺點：每次都增加減少 count 也會有效能問題，且是不能出錯的東西，甚至要用鎖之類的方法來確保不會有 race condition\n且要額外處理迴圈引用的問題\n二. Tracing: GC 啟動時從 roots 出發，再來找出沒在用的東西然後回收\n可以想像成一個 graph，裡面的 node 是一塊一塊的記憶體，被程式 allocate 來使用，edge 就是 reference\nroots 就是那些我們可以直接存取到的東西 (像是全域變數或是 goroutine stack)，由這些 roots 開始去做圖的 traverse，所有可以從這些 roots\n連接到的東西都是還在使用的，到達不了的 node 就是垃圾\n在這邊也有一種 weak pointer 但和 reference counting 的用法不同，這種 edge 是可以存取一個物件，但不保護這個物件不被回收\n常見名詞：\nMutator: 改變那些記憶體reference的東西，像是我們的程式就是\nMutator roots: root set, root objects，可以直接被存取的物件，像是靜態或全域變數、參數、Goroutine stack的其他東西\nCollector: 負責回收垃圾的程式\nCopy GC 最原始的垃圾回收\n把全部的記憶體切成兩份，一般時候都是用同一份\n當要收垃圾的時候就是從 roots 出發 traverse，把不是垃圾的東西複製到另一份，然後原本的那一份就可以清空了（當作垃圾，不用真的清）\n文章可以參考這個：https://zhuanlan.zhihu.com/p/28197216\nTraverse 和複製時要注意的就是指標的值也要順便更新\n缺點是空間要用很多\n不過複製的時候可以順便讓記憶體更 compact 是好處\n基本的 Mark and Sweep https://liujiacai.net/blog/2018/07/08/mark-sweep/\n就是簡單的收垃圾XD\n網路上很多 pseudo code 可以看\n 先把其他在做的事情都停住 (Stop the world) 從 roots 出發把有用到的東西 mark 起來 掃過 heap 所有的物件，把沒用的清除 恢復運作  簡單可以發現問題點，有記憶體碎片、掃過整個 heap 很慢、要停止所有東西\nhttps://stackoverflow.com/questions/23057531/what-are-the-advantages-and-disadvantages-of-having-mark-bits-together-and-separ\n優化\nBitmap marking： 用 bitmap 來紀錄哪些 object 是被 mark 住的\nLazy sweep：allocate 時才去做清垃圾，而且只清剛好需要的空間就好，平常只做標記\nMark-Compact 先把還有在用的標記起來然後 compact\nhttps://en.wikipedia.org/wiki/Mark-compact_algorithm\n目的就是要把還在使用的記憶體聚集在一起，減少記憶體碎片，碎片多就是母湯\n常見的做法就是掃過 heap，然後把每個 obj 往邊邊移動就好\nwikipedia裡面介紹了兩種方法\nTable-based 是需要掃過記憶體兩次，第一次掃過時要用一個 break table 記住每個 obj 原本的位置還有預計要移動到的位置，然後就將該 obj 移動\n第二個 pass 是要用 break table 調整這些 obj 裡面的 pointer\nBreak table 也是存在同一個 heap 內，搬移的時候可能會卡到，table 打散要重新排序(???)\nLISP2 algorithm 是一個簡單多的演算法\n要 3 個 pass\n 算出來每個 obj 預計會到哪 更新每個 obj 指出去的 pointer 移動~\n其中第一步預計要移動去哪的資訊會存在各自 obj 中  Incremental GC https://liujiacai.net/blog/2018/08/04/incremental-gc/#%E5%A2%9E%E9%87%8F%E5%BC%8F-GC-%E6%80%9D%E8%B7%AF\nhttps://segmentfault.com/a/1190000022030353\nhttps://titanwolf.org/Network/Articles/Article?AID=786c12ef-88b2-42da-984a-41ef4b5669d1#gsc.tab=0\n不一次做完整個垃圾回收的動作 (其實 ref counting 就類似，但缺點不少)，每次停頓小(最重要的優點)，但 throughpu 會降低就是了\nmark 的階段會被分成很多次\n三色標記法（tricolor marking） 一樣從 roots 出發去 traverse 整個 obj graph，過程中會將 obj 們標成不同顏色\n一開始 roots 都是灰色，其餘白色\n黑：可以到達，且該節點的子節點都看過了\n灰：可以到達，還沒去檢查該節點的子節點們\n白：還沒到達，如果 mark 結束都還是白色就會被當作垃圾\n標記階段可以被中斷，下次要開始時可以從灰色的子節點開始檢查\n直到沒有灰色節點時就完成 mark 階段\n灰色是用來記錄 mark 中斷後，又被 mutator 改變狀態的節點\n新建的 obj 依照不同的方法會設為不同顏色\n然而在標記階段中斷後再次開始前，我們的程式(mutator)可能會去改變這些 obj 之間的 reference，以下圖為例 (從上面網站抓來的)\n改變後可能造成黑A直接 ref 到白D，下次 mark 再開始後因為 A 是黑色，導致 D 不會被掃到，誤當成垃圾\nTricolor invvariant 為了避免上述的情況發生，三色標記法限制了節點的顏色與 reference 的關係\n下面兩種只要其中一種滿足即可\nStrong Invariant: 黑色不可直接指到白色，上面那種情況我們不會再去檢查黑點的子節點，導致戊收垃圾\nWeak Invariant: 黑可以直接指到白色，但這個白色必須是某個灰色節點的直接或間接後代\n參考圖 https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/\nD 在右圖是 B 的後代，所以允許 A 直接 reference\n接下來就是當 mutator 在運作時，我們如何維持上述其中一種 invariant 了\n在我們的程式 (mutator) 做事情時，需要插入一些 collector 的動作\n  read barrier\n顧名思義讀資料的時候插入動作，防止 mutator 去讀白色的 obj\n讀白色 obj 時就把它標成灰色\n例：上面那個 Figure 7 在讀 D 的時候就把 D 塗成灰色\n可以保證上面的 strong invariant\n  write barrier\n  https://www.mdeditor.tw/pl/prRu/zh-tw\nhttps://www.mdeditor.tw/pl/pFz3/zh-tw\n寫資料時插入動作，基本的有兩種\nstack太多，如果開write barrier會很慢，通常只開heap，所以以下兩種方法需要再開始或結束時掃一次stack來處理，預防有白色被已掃過的stack指到\na. insert write barrier (dijkstra) 當 mutator 把黑色指向白色時，會將白色變成灰色，或者把黑色變成灰色 (前者好像比較常見) 如此一來就符合 strong invariant 了～ mark結束後要再掃一次stack，因為stack沒開wb，指到的東西可能是白色卻需要存活 b. delete write barrier (Yuasa) 若有一個指針指向一個 obj 且該 obj 為白或灰 當 mutator 此指針拿掉時，該 obj 要被塗成灰色 一開始需要掃roots，把roots直接指到的都塗灰 為何需要開始時掃stack- http://www.yuasa.kuis.kyoto-u.ac.jp/~yuasa/ilc2002/slide.pdf 因為stack沒開rb，如果有一個root在stack上，若他直接指到某個obj，且這個pointer在這個obj被看到前，就被移除，那此obj可能被另外的黑色指到，就還是白色卻需要存活  一般來說 write barrier 的效率會比較好，因為 heap 讀比寫多很多\nGenerational GC https://liujiacai.net/blog/2018/08/18/generational-gc/\n分代，大部分的東西剛創建不久就會變成垃圾，所以把年輕和老的物件分開存\n年輕的部分常常做ＧＣ，老的偶爾做\n通常年輕的 obj 多，老的少\nyoung -\u0026gt; old 的 pointer 比 old -\u0026gt; young 多\n 不動到 old，獨立回收 young 部分\n考慮到 old -\u0026gt; young 的 pointer\n可以新增 write barrier 做一些操作，當 old 指到 young 時就記住，把被指到的 young 當作 young gc 時的 root  若該 old 是變成垃圾，則 young 會暫時成為實際上是垃圾的 root\n所以下次老 gc 時要清掉\n 獨立回收 old\n要考慮到 young -\u0026gt; old 的 pointer\n可以依照上面的方法來做，但通常 young -\u0026gt; old 的 pointer 數量較多\n要記錄的東西會很多，可以直接把所有 young 當作 root set，反正不多  Go GC 一開始適用插入屏障後來改用混合型，結合插入和刪除寫屏障，去除了開頭或結束時的 scan\n介紹怎麼讓 Go gc 更有效率\nhttps://www.ardanlabs.com/blog/2018/12/garbage-collection-in-go-part1-semantics.html\nhttps://golangpiter.com/system/attachments/files/000/001/718/original/GP_2019_-_An_Insight_Into_Go_Garbage_Collection.pdf?1572419303\nhttps://www.mdeditor.tw/pl/prRu/zh-tw\nhttps://www.mdeditor.tw/pl/pFz3/zh-tw\n\u0026quot;\u0026rdquo; 引用上面文章的段落\n在Go 語言 v1.7 版本之前，使用的是 Dijkstra 插入寫屏障保證強三色不變性，但是執行時並沒有在所有的垃圾收集根物件上開啟插入寫屏障。因為 Go 語言的應用程式可能包含成百上千的 goroutine，而垃圾收集的根物件一般包括全域性變數和棧物件，如果執行時需要在幾百個 goroutine 的棧上都開啟寫屏障，會帶來巨大的額外開銷，所以 Go 團隊在實現上選擇了在標記階段完成時暫停程式、將所有棧物件標記為灰色並重新掃描，在活躍 goroutine 非常多的程式中，重新掃描的過程需要佔用 10 ~ 100ms 的時間\n\u0026quot;\u0026rdquo;\nhttps://studygolang.com/articles/27243\nHybrid write barrier https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\n可以看 Proposal 和 Reasoning，有介紹為何他們可以消除開頭或結尾的掃描\nwritePointer(slot, ptr): shade(*slot) if current stack is grey: shade(ptr) *slot = ptr 奇怪的例子 A-\u0026gt;B C-\u0026gt;nil 都在 AC 都在 stack 上，若C黑，AB白 當變成 A -\u0026gt; nil C -\u0026gt; B 時，因為是stack 所以wb都不會啟動？ B會被誤刪嗎 這裡說當混合寫屏障時，在掃描特定一個goroutine上的stack時還是要暫停，一次掃完(所以上面那種情況不會出現)\n但不用像Yuasa一開始把全部stack掃完或是Dijkstra最後重掃\nhttps://liqingqiya.github.io/golang/gc/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%86%99%E5%B1%8F%E9%9A%9C/2020/07/24/gc5.html\n其餘連結\nhttps://mcll.top/2020/04/14/gc-write-barrier/\nhttps://www.bookstack.cn/read/qcrao-Go-Questions/spilt.3.GC-GC.md\n原始碼\nhttps://go.googlesource.com/go/+/go1.7beta2/src/runtime/mbarrier.go\n","description":"讀垃圾回收的筆記，懶的整理RR","id":1,"section":"posts","tags":["Go","Garbage collection","垃圾回收"],"title":"垃圾回收！","uri":"https://markogoodman.github.io/posts/gc/"},{"content":"筆記一下研究 csrf 查到的資料，有些小地方網路上的資料都滿模糊的真的查有夠久 XD\nCSRF (Cross-site request forgery) 基本原理 基本原理就是使用者在某個 A 網站認證(登入帳號之類的)後瀏覽器把 cookie 存下來，攻擊者利用發送 request 會自動帶上 cookie 的這個動作來讓使用者誤發一些奇怪的 request 去某 A 網站。\n舉個簡單的範例\n像是一個網站 XXX 提供 API 可以刪除文章\n/posts/delete?id=12\n但需要使用者用帳號密碼登入後，打這個 API 時在 cookie 中帶著上 session id 才可以刪除\n那壞壞的網站可能會有這樣的東西\n\u0026lt;a href='http://XXX/posts/delete?id=12'\u0026gt;hi\u0026lt;/a\u0026gt;\n如果你之前登入過XXX網站，而且該網站又沒有做好防護，點下去文章就會被砍掉拉\n注意的是攻擊者看不到 cookie 的內容，只是讓受害者無意間使用了。\n網路上有更詳細的介紹這邊就不多講，總之攻擊的方式五花八門，不管是換成POST，或是後端只收 json 之類的方法都是有辦法破解 (但我不會\n這篇主要想記錄一些比較常用的防禦方法的運作\n圖形驗證碼或簡訊密碼 很多比較敏感的操作都會要求這些，因為這類的攻擊無法得到驗證碼資訊所以防禦成功\n但對使用者來說這是一個麻煩\nSamesite cookie 是一種設定 http cookie 可以設定的屬性\n長得像這樣\nSet-Cookie: session_id=123; SameSite=Strict\n基本上可以去設定瀏覽器在跨站的 request 是否會自動帶 cookie。\n設定成不會自動帶的時候就防住了 csrf\n詳細請參考下面連結，裡面有說明了怎樣算是跨站以及不同 SameSite 設定的影響\nhttps://medium.com/@azure820529/chrome-80-%E5%BE%8C%E9%87%9D%E5%B0%8D%E7%AC%AC%E4%B8%89%E6%96%B9-cookie-%E7%9A%84%E8%A6%8F%E5%89%87%E8%AA%BF%E6%95%B4-default-samesite-lax-aaba0bc785a3\n目前很多東西都是前後端分離架在不同地方，這些都是跨站的 request。\n因為 samesite 和 same-origin policy 名字太像了我就順便看了一下同源政策，結果根本是不同的東東。\n同源政策是瀏覽器上的功能，主要做的是 cross-origin 發出請求後，收到回應時瀏覽器會看看同源政策的設定是否 OK，不 OK 的話會把回應擋掉。\n不過實際上 request 還是有發出去，所以要是像前面那種攻擊例子也擋不住\n詳細請參考：https://blog.techbridge.cc/2017/05/20/api-ajax-cors-and-jsonp/\n還有J個 youtube.com/watch?v=KaEj_qZgiKY\u0026amp;ab_channel=LiveOverflow\nSynchronizer Token Pattern 因為 Samesite cookie 不是每個瀏覽器都支援\n我們還是需要別的方法來判別送 req 的到底是不是使用者\nSynchronizer Token Pattern 就是當瀏覽器跟 server 要網頁的時候，server 偷偷在表單中塞一個隨機產生且隱藏的 csrf token，並自己存一份。server 收到 request 時會比對自己存的與收到的是否相同。\n詳細請參考：https://medium.com/cross-site-request-forgery-csrf/synchronizer-token-pattern-63871b4c83ad\n提一下如果沒有同源政策弄好，受害者在惡意網站發了 request，惡意網站可以看到回來的 response，csrf token 就被看到嚕\n前後端分離的狀況下可能要把同源政策設好，只讓合法前端看到 token，前端再送 req 時順便帶上\ndouble submit cookie 這個東西是我查比較久才搞清楚的\n運作流程是\nserver 把 csrf token 設到瀏覽器的 cookie 內，瀏覽器在送 request 時可以把這個 token 從 cookie 拿出來，放到 header (或是 form 裡面都可以) 裡面一起送給 server。\n這時進行了 csrf 攻擊，這個攻擊 request 其實還是會帶著 cookie 的 (如果沒設 samesite 的話拉)，但 csrf 攻擊者是看不到 cookie 的，自然也就不知道要放什麼 token 到 header 裡面，認證失敗。\n詳細：https://medium.com/cross-site-request-forgery-csrf/double-submit-cookie-pattern-65bb71d80d9f\n還有其他的變形像是讓前端自己產生 token 放到 cookie 裡面也行 (送req時放到 header 一起帶去 server)\n總之只要攻擊者無法讀到 cookie，他就無法在 header 放入一樣的東西\n這種方法 server 不用存 token\n前後端分離且在完全不同網域的情況下，把 token 從 cookie 拿出來這段可能會有困難，或許可以在第一次收到 token 時就存在 local storage 之類的地方，但也感覺很容易被盜\n差不多寫完了，我也不知道為啥我沒在寫網頁還研究這個\n","description":"筆記一下 csrf","id":2,"section":"posts","tags":["csrf"],"title":"Csrf","uri":"https://markogoodman.github.io/posts/csrf/"},{"content":"Part III: Examples Goroutine 的運作原理其實在前面兩篇就有很好的解釋拉\n這個部分主要是給一些實際的例子，會介紹一下平行與並行適合的是哪些情況，Goroutine 到底什麼時候開多才可以讓處理事情的效率增加\n接下來簡短回憶一下 Part I 寫過的這兩個重點\n1. Parallelism vs Concurrency   Parallelism 平行\n把不同的任務放到不同的 core 上執行，同一個時間點同時執行不同的任務，需要有多於一個的 core 才可辦到，基本上效能會比只使用一個 core 好，除非有什麼互相卡資源的問題，等等的範例會介紹一些例外 -\u0026gt; 如下圖的 G1、G2 就是平行執行\n  Concurrency 並行\n同一時間點執行一個任務，不同的任務必須輪流(順序不確定)，且結果與依序(sequential)執行所有任務相同，然後希望效能會比依序執行來得好 -\u0026gt; 如下圖的 G1 G4 G6 與 G2 G3 G5\n  2. CPU Bound 與 I/O Bound 的任務類型   CPU Bound:\nI/O 設備效能相對好。需要 core 的計算能力更強，或著更多 core 才可以有效加速這一些任務們的執行\n此種類型大部分 CPU 使用率都接近滿載\n  I/O Bound:\n任務們被 CPU 執行到一半常常需要等待一些東西 (讀檔案、網路傳輸資料)，才能再回來讓 CPU 繼續計算，如果網路傳輸速度或是硬碟讀取速度加快，可以大幅加速這些任務\nCPU 的 Loading 大部分不高\n  在 Part I 講解了 context switch 在 I/O Bound 的任務上是好的，需要等待 I/O 時先把 CPU 讓給其他人使用，等到 I/O 結果時再回到佇列中等待被 CPU 計算，避免 CPU 空閒沒事做。\n在 Part II 時我們講解了 Goroutine 相比普通的 OS thread 優勢是在它的記憶體用量與 context switch 的速度。\n所以我們知道 Goroutine 應該會適合用在 I/O Bound 的任務上，\n文章內也舉了幾個實際的例子，我就不把全部都 copy 過來了，下面簡單的介紹一下\nAdding numbers 程式碼在 Listing 1 內，目的是要把一個陣列裡的數字全部加起來，任務都是需要做計算，沒有等待 I/O\n分為兩種做法\n 一種是 sequential 的方法，用一個 Gorutine 把全部加起來 第二種是 concurrent，有八個 Goroutine 各自把八分之一的陣列加總，最後再加在一起  Listing 4 列出來使用一個 core 的結果，顯然 sequential 方法較快，\n在只有一個 core 的情況下，這種 CPU Bound 的任務，多了一些 Goroutine 只是會發生多餘的 context switch 導致總時間拉長而已。\nListing 5 列出了使用八個 core 的結果，每個 core 上各有一個 Goroutine，計算能力大幅增加，速度應該會提升\n實際上速度快了 40% 左右，為什麼不是八倍呢?\n大概是程式一開始只有一個 thread，要去開啟剩下七個 os thread 跑在不同的 core 上造成的延遲\n可以看到 BenchmarkConcurrent-8 3362643 ns/op，每加總一次也才花了 3ms 左右，thread 的創建搞不好就佔了很大一部分，如果把數字陣列的長度拉大，計算時間更久，效能應該會更接近八倍才對\nReading Files 文章中在這部分給的範例我覺得滿奇怪的\n在一個 Core 上使用多個 Goroutine (thread) 的優勢是發現 G 被 block 住時可以讓出 CPU，盡可能使用 CPU，使 I/O 等待與 CPU 運算可以同時進行\n但範例中使用 time.Sleep 來模擬讀檔案\n他這樣的做法應該可以使 CPU 運算部分 (strings.Contains 與其他 overhead) 和 sleep 部分同時進行沒錯，可能有達到一些加速的效果\n但正常來說讀檔案是會互相干擾的，例如在只有一個硬碟的狀況下，Goroutine 再多，硬碟的效能也是相同，大家應該要排隊讀，但 Sleep 的狀況卻是這些 Goroutine 可以一起在睡著的狀態(模擬了同時讀檔案)\nListing 14 使用了 1 個 core 出來的結果，大部分其實就是 8 個 Goroutine 可以同時睡眠的加速而已\nListing 15 發現 8 core + 8 G 出來發現沒有加速(相比 1C 8G)，我推測應該是原本運算的地方就佔比不高了，一個 core 就夠處理運算部分，大部分都在等 sleep，所以多了幾個 core，只用了 8 個 Goroutine，能同時睡著的 G 也只有 8 個，等的時間還是差不多\n下面用一張圖來解釋在何種情況下開多個 Goroutine 或者使用更多 Core 能夠增加效能，縮短任務完成時間。\n下圖假設\n 四種情況都有三組任務需要執行。每組都需要 core 計算 10 秒，接著 I/O 5 秒，兩者順序不能調換 不計算 context switch 所花費的時間 不同 Core 可以同時執行計算 (CPU - 10 那個部分) I/O 是大家共用，同時只能有一組 I/O 執行，在做完一組 I/O 之前不會切到另一組  第 (1) 組是序列執行，沒有 context switch ，I/O 時完全無法利用 core 來做事，很慢\n第 (2) 組讓一個 core 上跑了 3 個 G，G1 被 I/O block 住時，G2的計算可以先開始，比 (1) 的效率好點，但 I/O 還是有空窗期\n第 (3) 組就是多了一個 core，可以看到在 10 秒之後 I/O 都被排滿了，都有有效利用\n第 (4) 組再多了一個 core 執行時間並無縮短，因為 10 之後 I/O 都排滿了，而且多幾個 core 也無法縮短一開始的計算時間 10 秒 (除非可以把計算拆小但此處不考慮)\n上圖其實比較像是在解釋平行與並行在這種任務類型下的表現，因為其實把 Goroutine 看成 thread 也說得通。在 Part II 時最後有說過，其實他們切換的流程看起來是類似的，只是 Goroutine context switch 的速度快多了，資源消耗也少點。\n這系列的文章的內容差不多就是這樣了\n總結一下第一章介紹了 Go scheduler 需要的基本 OS 知識\n第二章再透過簡單的圖片來看看 Go scheduler 運作的方式\n最後以例子來看看不同的任務類型在 Concurrency 與 Parallelism 上面運作情況\n了解了這些之後，如果遇到自己的程式有效能上的問題，或許就能知道從哪裡著手分析，看是要多開幾個 Goroutine 還是多租幾個 CPU 來用啦～\n","description":"系列文之三， 一些實際的例子，還有並行與平行~","id":3,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Scheduling in Go Part III","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-iii/"},{"content":"https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html\n以下截圖都是從這個網站來的\nPart II: Go Scheduler 這部分要開始介紹 Go 的 Scheduler 是怎麼運作的，以及它的優勢在哪裡\n在 Go 裡面執行 runtime.NumCPU() 可以知道目前的電腦有幾個 virtual core (看有幾個cpu、各有幾個核心、核心上有幾個hardware thread)，這也是你的電腦最多可以平行執行的任務數量 (Part I 提到過)。\n用 runtime.GOMAXPROCS(n) 可以設定 Go 給予 n 個 \u0026lsquo;P\u0026rsquo;, logical processor，設定 Go 可以'邏輯上'平行執行的任務數量 (也就是要有幾個 active thread)。\n當 P 的數量大於前面 runtime.NumCPU() 數量，這些 P 就可能要常常交換使用 core 造成 context switch 負擔。\n如Part I所說，如果執行中的 thread 太少就無法利用全部的 core，太多的話這些 thread 會需要做 context switch 減慢速度。所以 Go 就把預設值設為 runtime.NumCPU()的數量。\n但在很少數的情況下 GOMAXPROCS 設高反而可以增加一點效能，參考 https://colobu.com/2017/10/11/interesting-things-about-GOMAXPROCS/\nGo Scheduler 是使用一個叫做 GMP model 的東西包含以下三個元件\nG: Goroutine, 一些要做的任務\nM: Machine thread, 就是 OS 層級的 thread\nP: Logical processor, 可以把他想成任務執行器，管理自己的 queue (Local run queue)，裡面有很多 G 待做\n除此之外還有一個 Global run queue 存放孤兒 Goroutine\n要執行任務計算(G)時，P 必須與一個 M 接在一起，並使用一個 core 來執行如下圖 (夾在 M 和 P 中間的就是執行中的 G)\nGoroutine states, context switch 與 thread 相同，也有 等待(例如等IO)、可執行、執行中三個狀態\n執行中代表上圖中 G 在 M 與 P 之間\n可執行的 G 就會在 local run queue 或 global run queue 排隊等著被執行\n等待狀態的 G 需要等待某些資源才能繼續被執行，可能會在 Network poller 中等待非同步的動作，或著與 M 一起被 block 住後兩個一起去旁邊等待， 下面的 3. 4點就會出現這個狀態的 G\nGoroutine 的 context switch 將 G 從執行中換下來, 把其他 G 換上來執行，但這只發生在必要的時刻，經常做只會多花時間\n可能發生的情況有幾種\n  產生新的 goroutine\ngo func()\u0026hellip; 有可能會c ontext switch，但不一定\n  Garbage collection\nGC 有自己的 Goroutine 要做也需要 M, 所以要把其他G換下來來執行收垃圾動作\n  Synchronization and Orchestration\n例如等待 channel 的資料被 block 的時候, 或者是 mutext 會把 goroutine 卡住的動作都可能會有 context switch\n  system call\n分為 asynchronous 與 synchronous system call\n非同步的狀況，例如大部分 OS 中 Networking-based system calls，這種情況 G 可以從 M 和 P 上拔下來，去 Network poller 中等待結果 (依靠 OS 的非同步 IO 達成，例如 epoll)，過程可以參考網頁內 Figure 3,4,5 ，G 得到結果之後再把 G 塞回 queue 中等待執行。期間 M 與 P 可以繼續消化其他 G。\n同步的 system call 例如某些 File-based system calls，會把整個 M block 住 (OS 無法非同步的處理這類型的 system call)，這時做這個呼叫的 G 會和 M 一起被放去旁邊等待結果 (thread M 會被從 core 上 context switch 下來，效率比前一種差)，不佔用 cpu 資源，流程參考網頁內 Figure 6,7,8，而 P 會取得一條新的 M (找 idle 的或新創一條) 來執行其他 G。當旁邊等待的 GM 得到結果後 G 會被塞回原本的queue中， M 則是變成idel等其他人用，需要時就不必新建 (正確來說是變成 spinning thread，主動去找孤兒P配對，也會使用到 cpu 資源，但通常比起刪掉重創來的划算，參考 https://zhuanlan.zhihu.com/p/42057783)。\n  Work Stealing 簡單來說就是 P 管理的 queue 中的 G 可以在不同 P 的 queue 之間移動\n當一個 P 發現自己沒有 G 可以做了，有可能會去 Global run queue 撿工作，也可能去別的 P 搶工作來做。\n當然這也要花時間，太過頻繁的搬動 G 也不好\n最後作者給了一個範例\n總體來說 goroutine 切換和 thread 之間的切換很類似，但 goroutine 的 size 小，所以可以創造很多個，context switch 速度也快很多，相比以往每個任務都開啟一條 thread，也省去了許多 thread 的產生銷毀耗費，所以速度與資源使用才會比較好\n下面這張圖是一個韓國人的投影片其中的一頁\nhttps://www.slideshare.net/Hyejong/golang-restful\n總結了 Goroutine 與 thread 資源的差距\nPS. context switch 時間約是 200ns 與 1000ns，5倍的差距\n","description":"系列文之二， Goroutine 的是怎麼調度的與 GMP model","id":4,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Scheduling in Go Part II","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-ii/"},{"content":"一開始用 Go 的 Map 時常常搞不清楚到底要不要傳指標，好像大部分的時候都不用，但用到 unmarshal 之類的 function 就又要傳指標進去了。\n這篇就來研究一下 Go 的 Map 到底是什麼生物\n先看看下面這段程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // mv = map value func ModifyMap(mv map[int]int) { mv[1] = 1 } // mp = map pointer func ModifyMapByPointer(mp *map[int]int) { (*mp)[1] = 1 } func main() { m1 := make(map[int]int) ModifyMap(m1) fmt.Println(m1) // map[1:1]  m2 := make(map[int]int) ModifyMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] }   兩個 function 都改到 map 裡面的值了，所以用不用指標到底有沒有差呢?\n再看看下面這段程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  func MakeMap(mv map[int]int) { mv = make(map[int]int) mv[1] = 1 } func MakeMapByPointer(mp *map[int]int) { (*mp) = make(map[int]int) (*mp)[1] = 1 } func main() { var m1 map[int]int MakeMap(m1) fmt.Println(m1) // map[]  var m2 map[int]int MakeMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] }   這次的 MakeMap 的操作就沒有反應到 main 傳進去的 m1 上，這是怎麼回事呢？\n首先當我們使用 make 產生 map 時，它就是回傳一個 pointer 給我們。也就是說當我們呼叫了 m := make(map[int]int)，m 這個變數內存的其實是一個地址，指到真正的 map struct。\n來看看第一組程式碼發生了什麼事\n簡單來說就是 m1 把 map 的地址 x 傳進去 ModifyMap，mv 內就存著相同的值 x，所以可以改到真正的 map struct。\n1 2 3 4 5 6 7  func ModifyMap(mv map[int]int) { mv[1] = 1 } m1 := make(map[int]int) ModifyMap(m1) fmt.Println(m1) // map[1:1]   下面這部分，是把 m2 本身的位置 y 傳進 ModifyMapByPointer， mp 就存著 m2 的位置 y。\n接著透過指標改到了 map struct\n1 2 3 4 5 6 7  func ModifyMapByPointer(mp *map[int]int) { (*mp)[1] = 1 } m2 := make(map[int]int) ModifyMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1]   再來看看在 function 裡面 make map 會怎麼樣\n這裡 m1 一開始是 nil，傳進 MakeMap 的東西也是 nil，自然 mv 內也會存著 nil。\n接著呼叫了 make，mv = make(map[int]int)把真正 map struct 的地址存到了 mv 內，mv[1] = 1也是真的有改到 map 裡面的值，只是外面的 m1 內存的值還是 nil，所以我們 print 出來的也就是空 map 了。\n1 2 3 4 5 6 7 8  func MakeMap(mv map[int]int) { mv = make(map[int]int) mv[1] = 1 } var m1 map[int]int MakeMap(m1) fmt.Println(m1) // map[]   最後一部分的範例，m2 的地址 y 傳進了 MakeMapByPointer，mp 內存著的是 m2 的地址，當呼叫(*mp) = make(map[int]int)時所代表的是產生一個 map struct，並把該地址 x 存進 mp 所指到的變數內，也就是 m2 的值中，所以後續的更改我們也可以從 m2 看到拉\n1 2 3 4 5 6 7 8  func MakeMapByPointer(mp *map[int]int) { (*mp) = make(map[int]int) (*mp)[1] = 1 } var m2 map[int]int MakeMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1]   至於真正的 map struct 是一個叫做 hmap 的東西，這邊就不講那麼細，剩下的上網估狗 Go makemap hmap 之類的關鍵字都可以找到嚕\n","description":"一開始用 Go 的 Map 時常常搞不清楚到底要不要傳指標，好像大部分的時候都不用，但用到 unmarshal 之類的 function 就又要傳指標進去了??","id":5,"section":"posts","tags":["Go","map","pointer"],"title":"Go 的 MAP 要不要用指標","uri":"https://markogoodman.github.io/posts/go-map/"},{"content":"關於這系列文 之前看了 Ardan labs 寫的下面這個系列文還滿不錯的\nhttps://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html\n主要是在講 Goroutine 和一般我們看到的 thread 比起來到底厲害在哪裡，也舉了一些範例讓讀者知道 Goroutine 用在哪裡才是正確的\n這系列文章就是拿來記個重點XD\n他的系列文分為三篇，我也就分三篇記\nPart I: OS Scheduler 這個章節不講 Goroutine 講一些 OS 要知道的預備知識，也不會涉及太多細節。\nProcess、Thread、Coroutine 下圖是 Learning Concurrency in Kotlin 這本書裡面的一張圖，解釋了 Process、Thread、Coroutine 的關係。\nProcess Process 是 OS 分配資源的基本單位 (記憶體之類的)\n簡單來說把一個程式 (program) 跑起來就是一個 process，一開始 process 內會有一條初始的 thread 來跑主要程式，這條 thread 可以建立出更多 thread，每個 process 裡面可以裝一堆 thread。\nThread Thread (kernel thread) 是 OS 分配 CPU 執行時間的單位，只能存在於 process 內，常被叫做 light-weight process。\n在大多我們使用的作業系統中，thread 的排程都是 preemptive (可搶佔)\u0008，代表著一條 thread 就算現在在 CPU 上執行，隨時也可能被各種原因換下來，基本上都是由 OS 來管理的。\n CPU 從正在執行 thread A 的任務鍾，換到執行 thread B 的任務，這個行為叫做 context switch。  Thread 之間有哪些共用與不共用的東西這邊就不多講網路上很多文章可以查到。\n提供兩篇文章參考\nhttps://medium.com/@yovan/os-process-thread-user-kernel-%E7%AD%86%E8%A8%98-aa6e04d35002\nhttps://www.itread01.com/content/1546525452.html\n大學上的 OS 課程大概就是介紹了上面那些東西而已。\nCoroutine 再來介紹的 coroutine (goroutine 就是一種)，基本上作業系統不知道有這個東西的存在，OS 只負責執行 thread 上面的一堆任務而已。coroutine 都是由我們使用的程式語言來處理，而每個程式語言提供的 coroutine 運作方式都有些不同。\ncoroutine 也叫做 light-weight thread 或者是 user-level thread，是跑在 kernel thread 上的一堆東西。\n基本上 coroutine 做的事情就是可以讓 function 執行到一半中斷，把 CPU 時間讓給別的 coroutine 來執行他們的工作\u0008\n舉個簡單的應用例子，當 Coroutine A 做事情做到一半，發現要等待讀取檔案或是等一些網路傳輸的資料才可以執行下一步，而這些工作又是不需要 CPU 的，那 coroutine A 就可以把 CPU 的時間讓給下一個 coroutine B，等待需要的東西回來之後再從中斷點繼續工作。\n與 thread 相比， coroutine 的 size 更小，像是在 Go 創建一個新的 goroutine 就只花費幾 KB，一條 java thread 卻要到 1~2 MB。\n而上面那種切換 coroutine 的行為其實是種 coroutine context switch。如果 coroutine A 與 coroutine B 是在同一個 thread，切換時時 OS 並不會發現，也不會出現 thread 的 context switch。\nCoroutine 的 context switch 與 thread 的 比起來速度也快很多，要儲存與載入的資料量相差很大。\n這部分之後講 goroutine 時會再提到。\n越多 Thread 真的會讓程式跑越快嗎 ? 答案是不會\n這部分先不討論 coroutine，單純就 multi-thread 的程式來說明\n進入正題前必須先理解，一個 thread 會做的工作主要分為這兩種\n  CPU-bound\n指的是 CPU 大部分時間都很忙的工作類型。譬如計算一個超大 int 陣列的總和，CPU 就是一直做加法，超忙。\n  IO-bound\nCPU 常常會沒事做，可能都在等著硬碟讀完檔案的通知，或者網路傳輸結束的通知等等。\n簡單的例子像是把一堆檔案一個一個讀進來，做一點點修改再寫回去，就會花很多時間在讀寫檔案，CPU 沒什麼事情好做。\n  基本上 context switch 在 IO-bound 的工作上是好的，當一個任務需要等待 IO 時，便換下來讓需要 CPU 的人用，避免讓 CPU idle。\n在 CPU-bound 的工作上 context switch 需要多花時間，只會拖慢完成全部任務的時間而已，但有時為了讓使用者覺得每個任務都有在進行，context switch 是必須的。\n回來講 thread 數量的問題\n當機器有 n 個 core，代表同時最多也只能執行 n 個 thread，若 thread 數量少於 n 的話，必然會有 core 沒事情做，顯然是不好。\n如果 thread 數量大於 n 太多，那就會常常在執行 context switch 而浪費時間，且 thread 會佔用不少記憶體。\n要取得平衡必須知道自己的系統是在做 CPU-bound 還是 IO-bound 的任務多，前者 thread 少點 (但還是要 \u0026gt;= n)，後者 thread 多點。\n當寫一個有 database 的 web service，最常使用的 thread 數量是 n * 3，就是一個大家的經驗法則而已。\n總結，我們希望每個 CPU 無時無刻都在工作，才能有最大的產出\n當一個 thread 等待 IO 時就把他換下來，讓 CPU 繼續工作\n當一個 thread 正在被 CPU 執行時，我們就盡量不要打擾他\n接著 Part II 會來介紹 Go 的 scheduler 是怎麼運作的，他為什麼讓 Goroutine 比 thread 還有效率\n","description":"系列文之一，Ardan labs 的 Scheduling in Go 系列文說明了 Goroutine 到底哪裡厲害，以及適合的使用情境，這篇文章就是要記錄一下重點之後忘記就可以直接看了","id":6,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Goroutine 為啥那麼快 (Scheduling in Go) Part I","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-i/"},{"content":"前幾天幫原本已經有資料的 collection 新建 unique index 的時候讓 Server 啟動爆掉了記錄一下\n1 2 3  db.coll.insert({\u0026#34;a\u0026#34;:1}) db.coll.insert({\u0026#34;a\u0026#34;:2}) db.coll.createIndex({\u0026#34;b\u0026#34;:1}, {unique: true}) // duplicate key error   在一個之前資料都沒有的 field 上建 unique index，結果噴出了下面的錯誤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  { \u0026#34;operationTime\u0026#34; : Timestamp(1609329527, 19822385), \u0026#34;ok\u0026#34; : 0, \u0026#34;errmsg\u0026#34; : \u0026#34;E11000 duplicate key error collection: test.coll index: b_1 dup key: { b: null }\u0026#34;, \u0026#34;code\u0026#34; : 11000, \u0026#34;codeName\u0026#34; : \u0026#34;DuplicateKey\u0026#34;, \u0026#34;keyPattern\u0026#34; : { \u0026#34;b\u0026#34; : 1 }, \u0026#34;keyValue\u0026#34; : { \u0026#34;b\u0026#34; : null }, \u0026#34;$clusterTime\u0026#34; : { \u0026#34;clusterTime\u0026#34; : Timestamp(1609329527, 19822385), \u0026#34;signature\u0026#34; : { \u0026#34;hash\u0026#34; : BinData(0,\u0026#34;AAAAAAAAAAAAAAAAAAAAAAAAAAA=\u0026#34;), \u0026#34;keyId\u0026#34; : NumberLong(0) } } }   errmsg內大致是在說在 b_1 這個 index (幫 b 建立 index 時候 MongoDB 幫取的名字)上，發生了 b = null 的 duplicate key。因為前面塞進去的兩筆都沒有 b ，建立 index 時 MongoDB 自動把他們視為 null 值，所以兩筆資料就撞在一起了。\n此時比較正常的解法是跑程式做資料庫的 schema migration，把之前的資料都補上該 field 的值。\n但如果新加的 key 本來就不想與以前資料相容該怎辦哩?\n有下面兩個解決方法\n 第一個是已經過時的用法，官方建議 mongodb 3.2版之後就建議使用第二種方式取代。\nsparse: true 可以讓 MongoDB 建 index 時直接忽略沒有這個 field 的資料(不像上面把 field 的值當作 null )，所以就不會有 duplicate key error了。  1  db.coll.createIndex({\u0026#34;b\u0026#34;:1}, {unique: true, sparse: true})   第二種是使用 partial index\nPartial index 指的是用某些條件去決定是否要在該筆資料上建 index，下面這個例子就使用了 $exists: true 來過濾，只有資料內存在 b 這個 field 的資料，我們才在 b 上面建立 unique index。  1 2 3 4 5 6 7 8 9  db.coll.createIndex( {\u0026#34;b\u0026#34;:1}, { unique: true, partialFilterExpression:{ \u0026#34;b\u0026#34;: {$exists: true} } } )   PartialFilterExpression 後面也可以接很多不同的條件\n例如 \u0026ldquo;b\u0026rdquo;: {$gt: 1} ，就可以過濾出那些 a field 大於 1 的資料並在這些資料上建 index。\n差不多就阿捏\n","description":"前幾天幫原本已經有資料的 collection 新建 unique index 的時候讓 Server 啟動爆掉了記錄一下","id":7,"section":"posts","tags":["mongodb","index","database"],"title":"Mongodb Partial and Sparse Index","uri":"https://markogoodman.github.io/posts/mongodb-partial-sparse-index/"},{"content":"誒\u0026hellip;\n","description":"","id":8,"section":"","tags":null,"title":"About","uri":"https://markogoodman.github.io/about/"},{"content":"汪\n","description":"","id":9,"section":"posts","tags":null,"title":"My First Post","uri":"https://markogoodman.github.io/posts/my-first-post/"}]