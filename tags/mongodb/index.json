[{"content":"Part III: Examples Goroutine 的運作原理其實在前面兩篇就有很好的解釋拉\n這個部分主要是給一些實際的例子，會介紹一下平行與並行適合的是哪些情況，Goroutine 到底什麼時候開多才可以讓處理事情的效率增加\n接下來簡短回憶一下 Part I 寫過的這兩個重點\n1. Parallelism vs Concurrency   Parallelism 平行\n把不同的任務放到不同的 core 上執行，同一個時間點同時執行不同的任務，需要有多於一個的 core 才可辦到，基本上效能會比只使用一個 core 好，除非有什麼互相卡資源的問題，等等的範例會介紹一些例外 -\u0026gt; 如下圖的 G1、G2 就是平行執行\n  Concurrency 並行\n同一時間點執行一個任務，不同的任務必須輪流(順序不確定)，且結果與依序(sequential)執行所有任務相同，然後希望效能會比依序執行來得好 -\u0026gt; 如下圖的 G1 G4 G6 與 G2 G3 G5\n  2. CPU Bound 與 I/O Bound 的任務類型   CPU Bound:\nI/O 設備效能相對好。需要 core 的計算能力更強，或著更多 core 才可以有效加速這一些任務們的執行\n此種類型大部分 CPU 使用率都接近滿載\n  I/O Bound:\n任務們被 CPU 執行到一半常常需要等待一些東西 (讀檔案、網路傳輸資料)，才能再回來讓 CPU 繼續計算，如果網路傳輸速度或是硬碟讀取速度加快，可以大幅加速這些任務\nCPU 的 Loading 大部分不高\n  在 Part I 講解了 context switch 在 I/O Bound 的任務上是好的，需要等待 I/O 時先把 CPU 讓給其他人使用，等到 I/O 結果時再回到佇列中等待被 CPU 計算，避免 CPU 空閒沒事做。\n在 Part II 時我們講解了 Goroutine 相比普通的 OS thread 優勢是在它的記憶體用量與 context switch 的速度。\n所以我們知道 Goroutine 應該會適合用在 I/O Bound 的任務上，\n文章內也舉了幾個實際的例子，我就不把全部都 copy 過來了，下面簡單的介紹一下\nAdding numbers 程式碼在 Listing 1 內，目的是要把一個陣列裡的數字全部加起來，任務都是需要做計算，沒有等待 I/O\n分為兩種做法\n 一種是 sequential 的方法，用一個 Gorutine 把全部加起來 第二種是 concurrent，有八個 Goroutine 各自把八分之一的陣列加總，最後再加在一起  Listing 4 列出來使用一個 core 的結果，顯然 sequential 方法較快，\n在只有一個 core 的情況下，這種 CPU Bound 的任務，多了一些 Goroutine 只是會發生多餘的 context switch 導致總時間拉長而已。\nListing 5 列出了使用八個 core 的結果，每個 core 上各有一個 Goroutine，計算能力大幅增加，速度應該會提升\n實際上速度快了 40% 左右，為什麼不是八倍呢?\n大概是程式一開始只有一個 thread，要去開啟剩下七個 os thread 跑在不同的 core 上造成的延遲\n可以看到 BenchmarkConcurrent-8 3362643 ns/op，每加總一次也才花了 3ms 左右，thread 的創建搞不好就佔了很大一部分，如果把數字陣列的長度拉大，計算時間更久，效能應該會更接近八倍才對\nReading Files 文章中在這部分給的範例我覺得滿奇怪的\n在一個 Core 上使用多個 Goroutine (thread) 的優勢是發現 G 被 block 住時可以讓出 CPU，盡可能使用 CPU，使 I/O 等待與 CPU 運算可以同時進行\n但範例中使用 time.Sleep 來模擬讀檔案\n他這樣的做法應該可以使 CPU 運算部分 (strings.Contains 與其他 overhead) 和 sleep 部分同時進行沒錯，可能有達到一些加速的效果\n但正常來說讀檔案是會互相干擾的，例如在只有一個硬碟的狀況下，Goroutine 再多，硬碟的效能也是相同，大家應該要排隊讀，但 Sleep 的狀況卻是這些 Goroutine 可以一起在睡著的狀態(模擬了同時讀檔案)\nListing 14 使用了 1 個 core 出來的結果，大部分其實就是 8 個 Goroutine 可以同時睡眠的加速而已\nListing 15 發現 8 core + 8 G 出來發現沒有加速(相比 1C 8G)，我推測應該是原本運算的地方就佔比不高了，一個 core 就夠處理運算部分，大部分都在等 sleep，所以多了幾個 core，只用了 8 個 Goroutine，能同時睡著的 G 也只有 8 個，等的時間還是差不多\n下面用一張圖來解釋在何種情況下開多個 Goroutine 或者使用更多 Core 能夠增加效能，縮短任務完成時間。\n下圖\n 四種情況都有三組任務需要執行。每組都需要 core 計算 10 秒，接著 I/O 5 秒，兩者順序不能調換 不計算 context switch 所花費的時間 不同 Core 可以同時執行計算 (CPU - 10 那個部分) I/O 是大家共用，同時只能有一組 I/O 執行  第 (1) 組是序列執行，沒有 context switch ，I/O 時完全無法利用 core 來做事，很慢\n第 (2) 組讓一個 core 上跑了 3 個 G，G1 被 I/O block 住時，G2的計算可以先開始，比 (1) 的效率好點，但 I/O 還是有空窗期\n第 (3) 組就是多了一個 core，可以看到在 10 秒之後 I/O 都被排滿了，都有有效利用\n第 (4) 組再多了一個 core 執行時間並無縮短，因為 10 之後 I/O 都排滿了，而且多幾個 core 也無法縮短一開始的計算時間 10 秒 (除非可以把計算拆小但此處不考慮)\n上圖其實比較像是在解釋平行與並行在這種任務類型下的表現，因為其實把 Goroutine 看成 thread 也說得通。在 Part II 時最後有說過，其實他們切換的流程看起來是類似的，只是 Goroutine context switch 的速度快多了，資源消耗也少點。\n這系列的文章的內容差不多就是這樣了\n總結一下第一章介紹了 Go scheduler 需要的基本 OS 知識\n第二章再透過簡單的圖片來看看 Go scheduler 運作的方式\n最後以例子來看看不同的任務類型在 Concurrency 與 Parallelism 上面運作情況\n了解了這些之後，如果遇到自己的程式有效能上的問題，或許就能知道從哪裡著手分析，看是要多開幾個 Goroutine 還是多租幾個 CPU 來用啦～\n","description":"系列文之三， 一些實際的例子，還有並行與平行~","id":0,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Scheduling in Go Part III","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-iii/"},{"content":"https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html\n以下截圖都是從這個網站來的\nPart II: Go Scheduler 這部分要開始介紹 Go 的 Scheduler 是怎麼運作的，以及它的優勢在哪裡\n在 Go 裡面執行 runtime.NumCPU() 可以知道目前的電腦有幾個 virtual core (看有幾個cpu、各有幾個核心、核心上有幾個hardware thread)，這也是你的電腦最多可以平行執行的任務數量 (Part I 提到過)。\n用 runtime.GOMAXPROCS(n) 可以設定 Go 給予 n 個 \u0026lsquo;P\u0026rsquo;, logical processor，設定 Go 可以\u0026rsquo;邏輯上\u0026rsquo;平行執行的任務數量 (也就是要有幾個 active thread)。\n當 P 的數量大於前面 runtime.NumCPU() 數量，這些 P 就可能要常常交換使用 core 造成 context switch 負擔。\n如Part I所說，如果執行中的 thread 太少就無法利用全部的 core，太多的話這些 thread 會需要做 context switch 減慢速度。所以 Go 就把預設值設為 runtime.NumCPU()的數量。\n但在很少數的情況下 GOMAXPROCS 設高反而可以增加一點效能，參考 https://colobu.com/2017/10/11/interesting-things-about-GOMAXPROCS/\nGo Scheduler 是使用一個叫做 GMP model 的東西包含以下三個元件\nG: Goroutine, 一些要做的任務\nM: Machine thread, 就是 OS 層級的 thread\nP: Logical processor, 可以把他想成任務執行器，管理自己的 queue (Local run queue)，裡面有很多 G 待做\n除此之外還有一個 Global run queue 存放孤兒 Goroutine\n要執行任務計算(G)時，P 必須與一個 M 接在一起，並使用一個 core 來執行如下圖 (夾在 M 和 P 中間的就是執行中的 G)\nGoroutine states, context switch 與 thread 相同，也有 等待(例如等IO)、可執行、執行中三個狀態\n執行中代表上圖中 G 在 M 與 P 之間\n可執行的 G 就會在 local run queue 或 global run queue 排隊等著被執行\n等待狀態的 G 需要等待某些資源才能繼續被執行，可能會在 Network poller 中等待非同步的動作，或著與 M 一起被 block 住後兩個一起去旁邊等待， 下面的 3. 4點就會出現這個狀態的 G\nGoroutine 的 context switch 將 G 從執行中換下來, 把其他 G 換上來執行，但這只發生在必要的時刻，經常做只會多花時間\n可能發生的情況有幾種\n  產生新的 goroutine\ngo func()\u0026hellip; 有可能會c ontext switch，但不一定\n  Garbage collection\nGC 有自己的 Goroutine 要做也需要 M, 所以要把其他G換下來來執行收垃圾動作\n  Synchronization and Orchestration\n例如等待 channel 的資料被 block 的時候, 或者是 mutext 會把 goroutine 卡住的動作都可能會有 context switch\n  system call\n分為 asynchronous 與 synchronous system call\n非同步的狀況，例如大部分 OS 中 Networking-based system calls，這種情況 G 可以從 M 和 P 上拔下來，去 Network poller 中等待結果 (依靠 OS 的非同步 IO 達成，例如 epoll)，過程可以參考網頁內 Figure 3,4,5 ，G 得到結果之後再把 G 塞回 queue 中等待執行。期間 M 與 P 可以繼續消化其他 G。\n同步的 system call 例如某些 File-based system calls，會把整個 M block 住 (OS 無法非同步的處理這類型的 system call)，這時做這個呼叫的 G 會和 M 一起被放去旁邊等待結果 (thread M 會被從 core 上 context switch 下來，效率比前一種差)，不佔用 cpu 資源，流程參考網頁內 Figure 6,7,8，而 P 會取得一條新的 M (找 idle 的或新創一條) 來執行其他 G。當旁邊等待的 GM 得到結果後 G 會被塞回原本的queue中， M 則是變成idel等其他人用，需要時就不必新建 (正確來說是變成 spinning thread，主動去找孤兒P配對，也會使用到 cpu 資源，但通常比起刪掉重創來的划算，參考 https://zhuanlan.zhihu.com/p/42057783)。\n  Work Stealing 簡單來說就是 P 管理的 queue 中的 G 可以在不同 P 的 queue 之間移動\n當一個 P 發現自己沒有 G 可以做了，有可能會去 Global run queue 撿工作，也可能去別的 P 搶工作來做。\n當然這也要花時間，太過頻繁的搬動 G 也不好\n最後作者給了一個範例\n總體來說 goroutine 切換和 thread 之間的切換很類似，但 goroutine 的 size 小，所以可以創造很多個，context switch 速度也快很多，相比以往每個任務都開啟一條 thread，也省去了許多 thread 的產生銷毀耗費，所以速度與資源使用才會比較好\n下面這張圖是一個韓國人的投影片其中的一頁\nhttps://www.slideshare.net/Hyejong/golang-restful\n總結了 Goroutine 與 thread 資源的差距\nPS. context switch 時間約是 200ns 與 1000ns 5倍的差距 ","description":"系列文之二， Goroutine 的是怎麼調度的與 GMP model","id":1,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Scheduling in Go Part II","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-ii/"},{"content":"一開始用 Go 的 Map 時常常搞不清楚到底要不要傳指標，好像大部分的時候都不用，但用到 unmarshal 之類的 function 就又要傳指標進去了。\n這篇就來研究一下 Go 的 Map 到底是什麼生物\n先看看下面這段程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // mv = map value func ModifyMap(mv map[int]int) { mv[1] = 1 } // mp = map pointer func ModifyMapByPointer(mp *map[int]int) { (*mp)[1] = 1 } func main() { m1 := make(map[int]int) ModifyMap(m1) fmt.Println(m1) // map[1:1]  m2 := make(map[int]int) ModifyMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] }   兩個 function 都改到 map 裡面的值了，所以用不用指標到底有沒有差呢?\n再看看下面這段程式碼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  func MakeMap(mv map[int]int) { mv = make(map[int]int) mv[1] = 1 } func MakeMapByPointer(mp *map[int]int) { (*mp) = make(map[int]int) (*mp)[1] = 1 } func main() { var m1 map[int]int MakeMap(m1) fmt.Println(m1) // map[]  var m2 map[int]int MakeMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1] }   這次的 MakeMap 的操作就沒有反應到 main 傳進去的 m1 上，這是怎麼回事呢？\n首先當我們使用 make 產生 map 時，它就是回傳一個 pointer 給我們。也就是說當我們呼叫了 m := make(map[int]int)，m 這個變數內存的其實是一個地址，指到真正的 map struct。\n來看看第一組程式碼發生了什麼事\n簡單來說就是 m1 把 map 的地址 x 傳進去 ModifyMap，mv 內就存著相同的值 x，所以可以改到真正的 map struct。\n1 2 3 4 5 6 7  func ModifyMap(mv map[int]int) { mv[1] = 1 } m1 := make(map[int]int) ModifyMap(m1) fmt.Println(m1) // map[1:1]   下面這部分，是把 m2 本身的位置 y 傳進 ModifyMapByPointer， mp 就存著 m2 的位置 y。\n接著透過指標改到了 map struct\n1 2 3 4 5 6 7  func ModifyMapByPointer(mp *map[int]int) { (*mp)[1] = 1 } m2 := make(map[int]int) ModifyMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1]   再來看看在 function 裡面 make map 會怎麼樣\n這裡 m1 一開始是 nil，傳進 MakeMap 的東西也是 nil，自然 mv 內也會存著 nil。\n接著呼叫了 make，mv = make(map[int]int)把真正 map struct 的地址存到了 mv 內，mv[1] = 1也是真的有改到 map 裡面的值，只是外面的 m1 內存的值還是 nil，所以我們 print 出來的也就是空 map 了。\n1 2 3 4 5 6 7 8  func MakeMap(mv map[int]int) { mv = make(map[int]int) mv[1] = 1 } var m1 map[int]int MakeMap(m1) fmt.Println(m1) // map[]   最後一部分的範例，m2 的地址 y 傳進了 MakeMapByPointer，mp 內存著的是 m2 的地址，當呼叫(*mp) = make(map[int]int)時所代表的是產生一個 map struct，並把該地址 x 存進 mp 所指到的變數內，也就是 m2 的值中，所以後續的更改我們也可以從 m2 看到拉\n1 2 3 4 5 6 7 8  func MakeMapByPointer(mp *map[int]int) { (*mp) = make(map[int]int) (*mp)[1] = 1 } var m2 map[int]int MakeMapByPointer(\u0026amp;m2) fmt.Println(m2) // map[1:1]   至於真正的 map struct 是一個叫做 hmap 的東西，這邊就不講那麼細，剩下的上網估狗 Go makemap hmap 之類的關鍵字都可以找到嚕\n","description":"一開始用 Go 的 Map 時常常搞不清楚到底要不要傳指標，好像大部分的時候都不用，但用到 unmarshal 之類的 function 就又要傳指標進去了??","id":2,"section":"posts","tags":["Go","map","pointer"],"title":"Go 的 MAP 要不要用指標","uri":"https://markogoodman.github.io/posts/go-map/"},{"content":"關於這系列文 之前看了 Ardan labs 寫的下面這個系列文還滿不錯的\nhttps://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html\n主要是在講 Goroutine 和一般我們看到的 thread 比起來到底厲害在哪裡，也舉了一些範例讓讀者知道 Goroutine 用在哪裡才是正確的\n這系列文章就是拿來記個重點XD\n他的系列文分為三篇，我也就分三篇記\nPart I: OS Scheduler 這個章節不講 Goroutine 講一些 OS 要知道的預備知識，也不會涉及太多細節。\nProcess、Thread、Coroutine 下圖是 Learning Concurrency in Kotlin 這本書裡面的一張圖，解釋了 Process、Thread、Coroutine 的關係。\nProcess Process 是 OS 分配資源的基本單位 (記憶體之類的)\n簡單來說把一個程式 (program) 跑起來就是一個 process，一開始 process 內會有一條初始的 thread 來跑主要程式，這條 thread 可以建立出更多 thread，每個 process 裡面可以裝一堆 thread。\nThread Thread (kernel thread) 是 OS 分配 CPU 執行時間的單位，只能存在於 process 內，常被叫做 light-weight process。\n在大多我們使用的作業系統中，thread 的排程都是 preemptive (可搶佔)\u0008，代表著一條 thread 就算現在在 CPU 上執行，隨時也可能被各種原因換下來，基本上都是由 OS 來管理的。\n CPU 從正在執行 thread A 的任務鍾，換到執行 thread B 的任務，這個行為叫做 context switch。  Thread 之間有哪些共用與不共用的東西這邊就不多講網路上很多文章可以查到。\n提供兩篇文章參考\nhttps://medium.com/@yovan/os-process-thread-user-kernel-%E7%AD%86%E8%A8%98-aa6e04d35002\nhttps://www.itread01.com/content/1546525452.html\n大學上的 OS 課程大概就是介紹了上面那些東西而已。\nCoroutine 再來介紹的 coroutine (goroutine 就是一種)，基本上作業系統不知道有這個東西的存在，OS 只負責執行 thread 上面的一堆任務而已。coroutine 都是由我們使用的程式語言來處理，而每個程式語言提供的 coroutine 運作方式都有些不同。\ncoroutine 也叫做 light-weight thread 或者是 user-level thread，是跑在 kernel thread 上的一堆東西。\n基本上 coroutine 做的事情就是可以讓 function 執行到一半中斷，把 CPU 時間讓給別的 coroutine 來執行他們的工作\u0008\n舉個簡單的應用例子，當 Coroutine A 做事情做到一半，發現要等待讀取檔案或是等一些網路傳輸的資料才可以執行下一步，而這些工作又是不需要 CPU 的，那 coroutine A 就可以把 CPU 的時間讓給下一個 coroutine B，等待需要的東西回來之後再從中斷點繼續工作。\n與 thread 相比， coroutine 的 size 更小，像是在 Go 創建一個新的 goroutine 就只花費幾 KB，一條 java thread 卻要到 1~2 MB。\n而上面那種切換 coroutine 的行為其實是種 coroutine context switch。如果 coroutine A 與 coroutine B 是在同一個 thread，切換時時 OS 並不會發現，也不會出現 thread 的 context switch。\nCoroutine 的 context switch 與 thread 的 比起來速度也快很多，要儲存與載入的資料量相差很大。\n這部分之後講 goroutine 時會再提到。\n越多 Thread 真的會讓程式跑越快嗎 ? 答案是不會\n這部分先不討論 coroutine，單純就 multi-thread 的程式來說明\n進入正題前必須先理解，一個 thread 會做的工作主要分為這兩種\n  CPU-bound\n指的是 CPU 大部分時間都很忙的工作類型。譬如計算一個超大 int 陣列的總和，CPU 就是一直做加法，超忙。\n  IO-bound\nCPU 常常會沒事做，可能都在等著硬碟讀完檔案的通知，或者網路傳輸結束的通知等等。\n簡單的例子像是把一堆檔案一個一個讀進來，做一點點修改再寫回去，就會花很多時間在讀寫檔案，CPU 沒什麼事情好做。\n  基本上 context switch 在 IO-bound 的工作上是好的，當一個任務需要等待 IO 時，便換下來讓需要 CPU 的人用，避免讓 CPU idle。\n在 CPU-bound 的工作上 context switch 需要多花時間，只會拖慢完成全部任務的時間而已，但有時為了讓使用者覺得每個任務都有在進行，context switch 是必須的。\n回來講 thread 數量的問題\n當機器有 n 個 core，代表同時最多也只能執行 n 個 thread，若 thread 數量少於 n 的話，必然會有 core 沒事情做，顯然是不好。\n如果 thread 數量大於 n 太多，那就會常常在執行 context switch 而浪費時間，且 thread 會佔用不少記憶體。\n要取得平衡必須知道自己的系統是在做 CPU-bound 還是 IO-bound 的任務多，前者 thread 少點 (但還是要 \u0026gt;= n)，後者 thread 多點。\n當寫一個有 database 的 web service，最常使用的 thread 數量是 n * 3，就是一個大家的經驗法則而已。\n總結，我們希望每個 CPU 無時無刻都在工作，才能有最大的產出\n當一個 thread 等待 IO 時就把他換下來，讓 CPU 繼續工作\n當一個 thread 正在被 CPU 執行時，我們就盡量不要打擾他\n接著 Part II 會來介紹 Go 的 scheduler 是怎麼運作的，他為什麼讓 Goroutine 比 thread 還有效率\n","description":"系列文之一，Ardan labs 的 Scheduling in Go 系列文說明了 Goroutine 到底哪裡厲害，以及適合的使用情境，這篇文章就是要記錄一下重點之後忘記就可以直接看了","id":3,"section":"posts","tags":["Go","Goroutine","scheduler"],"title":"Goroutine 為啥那麼快 (Scheduling in Go) Part I","uri":"https://markogoodman.github.io/posts/scheduling-in-go-part-i/"},{"content":"前幾天幫原本已經有資料的 collection 新建 unique index 的時候讓 Server 啟動爆掉了記錄一下\n1 2 3  db.coll.insert({\u0026#34;a\u0026#34;:1}) db.coll.insert({\u0026#34;a\u0026#34;:2}) db.coll.createIndex({\u0026#34;b\u0026#34;:1}, {unique: true}) // duplicate key error   在一個之前資料都沒有的 field 上建 unique index，結果噴出了下面的錯誤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  { \u0026#34;operationTime\u0026#34; : Timestamp(1609329527, 19822385), \u0026#34;ok\u0026#34; : 0, \u0026#34;errmsg\u0026#34; : \u0026#34;E11000 duplicate key error collection: test.coll index: b_1 dup key: { b: null }\u0026#34;, \u0026#34;code\u0026#34; : 11000, \u0026#34;codeName\u0026#34; : \u0026#34;DuplicateKey\u0026#34;, \u0026#34;keyPattern\u0026#34; : { \u0026#34;b\u0026#34; : 1 }, \u0026#34;keyValue\u0026#34; : { \u0026#34;b\u0026#34; : null }, \u0026#34;$clusterTime\u0026#34; : { \u0026#34;clusterTime\u0026#34; : Timestamp(1609329527, 19822385), \u0026#34;signature\u0026#34; : { \u0026#34;hash\u0026#34; : BinData(0,\u0026#34;AAAAAAAAAAAAAAAAAAAAAAAAAAA=\u0026#34;), \u0026#34;keyId\u0026#34; : NumberLong(0) } } }   errmsg內大致是在說在 b_1 這個 index (幫 b 建立 index 時候 MongoDB 幫取的名字)上，發生了 b = null 的 duplicate key。因為前面塞進去的兩筆都沒有 b ，建立 index 時 MongoDB 自動把他們視為 null 值，所以兩筆資料就撞在一起了。\n此時比較正常的解法是跑程式做資料庫的 schema migration，把之前的資料都補上該 field 的值。\n但如果新加的 key 本來就不想與以前資料相容該怎辦哩?\n有下面兩個解決方法\n 第一個是已經過時的用法，官方建議 mongodb 3.2版之後就建議使用第二種方式取代。\nsparse: true 可以讓 MongoDB 建 index 時直接忽略沒有這個 field 的資料(不像上面把 field 的值當作 null )，所以就不會有 duplicate key error了。  1  db.coll.createIndex({\u0026#34;b\u0026#34;:1}, {unique: true, sparse: true})   第二種是使用 partial index\nPartial index 指的是用某些條件去決定是否要在該筆資料上建 index，下面這個例子就使用了 $exists: true 來過濾，只有資料內存在 b 這個 field 的資料，我們才在 b 上面建立 unique index。  1 2 3 4 5 6 7 8 9  db.coll.createIndex( {\u0026#34;b\u0026#34;:1}, { unique: true, partialFilterExpression:{ \u0026#34;b\u0026#34;: {$exists: true} } } )   PartialFilterExpression 後面也可以接很多不同的條件\n例如 \u0026ldquo;b\u0026rdquo;: {$gt: 1} ，就可以過濾出那些 a field 大於 1 的資料並在這些資料上建 index。\n差不多就阿捏\n","description":"前幾天幫原本已經有資料的 collection 新建 unique index 的時候讓 Server 啟動爆掉了記錄一下","id":4,"section":"posts","tags":["mongodb","index","database"],"title":"Mongodb Partial and Sparse Index","uri":"https://markogoodman.github.io/posts/mongodb-partial-sparse-index/"},{"content":"誒\u0026hellip;\n","description":"","id":5,"section":"","tags":null,"title":"About","uri":"https://markogoodman.github.io/about/"},{"content":"汪\n","description":"","id":6,"section":"posts","tags":null,"title":"My First Post","uri":"https://markogoodman.github.io/posts/my-first-post/"}]